
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Potential Risks from Advanced AI + Innovation Policy</title>
    <style>
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>
    <h1>Potential Risks from Advanced AI</h1>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Grant Title</th>
      <th>Organization Name</th>
      <th>Focus Area</th>
      <th>Amount</th>
      <th>Date</th>
      <th>Abstract</th>
      <th>Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Practical AI Alignment and Interpretability Research Group — Interpretability Work</td>
      <td>Practical AI Alignment and Interpretability Research Group</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$737,000</td>
      <td>September 2024</td>
      <td>Open Philanthropy recommended a grant of $737,000 over two years to the Practical AI Alignment and Interpretability Research Group to support work led by Atticus Geiger to conduct interpretability research, create open-source course materials on mechanistic interpretability, and run a mentorship program.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/practical-ai-alignment-and-interpretability-research-group-interpretability-work/">https://www.openphilanthropy.org/grants/practical-ai-alignment-and-interpretability-research-group-interpretability-work/</a></td>
    </tr>
    <tr>
      <td>University of California, Berkeley — Software Engineering Benchmark</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$739,866</td>
      <td>August 2024</td>
      <td>Open Philanthropy recommended a gift of $739,866 to UC Berkeley to support Professor Koushik Sen and his team in developing a software engineering benchmark, using their Repository2Environment framework.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-california-berkeley-software-engineering-benchmark/">https://www.openphilanthropy.org/grants/university-of-california-berkeley-software-engineering-benchmark/</a></td>
    </tr>
    <tr>
      <td>Cambridge in America — Data Science Benchmark</td>
      <td>Cambridge in America</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$517,693</td>
      <td>July 2024</td>
      <td>Image courtesy of Cambridge in America\nOpen Philanthropy recommended a gift of $517,693 over two years to Cambridge in America to support the development of a benchmark for evaluating the performance of Large Language Model (LLM) agents in data science tasks. The project will be led by Professors Lorenzo Pacchiardi, José Hernández-Orallo, and Lucy Cheke.\nThis gift was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/cambridge-in-america-data-science-benchmark/">https://www.openphilanthropy.org/grants/cambridge-in-america-data-science-benchmark/</a></td>
    </tr>
    <tr>
      <td>Ali Merali — Economics Research on AI Model Scaling Effects</td>
      <td>Ali Merali</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$54,650</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $54,650 to Ali Merali to support a series of randomized controlled trials (RCTs) estimating how much model scale affects human performance on economically relevant tasks.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ali-merali-economics-research-on-ai-model-scaling-effects/">https://www.openphilanthropy.org/grants/ali-merali-economics-research-on-ai-model-scaling-effects/</a></td>
    </tr>
    <tr>
      <td>Talos Network — General Support</td>
      <td>Talos Network</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$662,378</td>
      <td>July 2024</td>
      <td>Image courtesy of the Talos Network\nOpen Philanthropy recommended a grant of $662,378 to the Talos Network for general support. The Talos Network is a talent development organization for careers in European AI policy.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/talos-network-general-support/">https://www.openphilanthropy.org/grants/talos-network-general-support/</a></td>
    </tr>
    <tr>
      <td>FAR AI — AI Alignment Research Projects (2024)</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,400,000</td>
      <td>July 2024</td>
      <td>Image courtesy of FAR AI\nOpen Philanthropy recommended a grant of $2,400,000 to FAR AI to support research expenses for several AI alignment research projects. Ethan Perez will mentor the independent researchers undertaking these projects.\nThis follows our January 2024 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-ai-alignment-research-projects-2024/">https://www.openphilanthropy.org/grants/far-ai-ai-alignment-research-projects-2024/</a></td>
    </tr>
    <tr>
      <td>Jennifer Lin — LLM Model-Based Planning Report</td>
      <td>Jennifer Lin</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $70,000 to Jennifer Lin to support the production of a publicly available report investigating whether large language models (LLMs) have the cognitive ability of model-based planning.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/jennifer-lin-llm-model-based-planning-report/">https://www.openphilanthropy.org/grants/jennifer-lin-llm-model-based-planning-report/</a></td>
    </tr>
    <tr>
      <td>New York University — LLM Cybersecurity Benchmark</td>
      <td>New York University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,077,350</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $2,077,350 to New York University to support a project to develop a benchmark for large language model (LLM) agent cyber operations capabilities, in partnership with New Foundry. The project will be led by Alex Leader, Dan Zhao, and He He. \nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/new-york-university-llm-cybersecurity-benchmark/">https://www.openphilanthropy.org/grants/new-york-university-llm-cybersecurity-benchmark/</a></td>
    </tr>
    <tr>
      <td>Stanford University — LLM Cybersecurity Benchmark</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,937,000</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $2,937,000 to Stanford University to support research to develop a benchmark for the cybersecurity capabilities of large language model (LLM) agents, led by Dr. Percy Liang.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-llm-cyberoffense-benchmark/">https://www.openphilanthropy.org/grants/stanford-university-llm-cyberoffense-benchmark/</a></td>
    </tr>
    <tr>
      <td>Trustees of Boston University — LLM Research Benchmark</td>
      <td>Boston University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$756,396</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $756,396 over two years to Boston University to support a project evaluating the abilities of large language models (LLMs) at performing academic machine learning research, led by Professor Najoung Kim and Professor Sebastian Schuster. \nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/trustees-of-boston-university-llm-research-benchmark/">https://www.openphilanthropy.org/grants/trustees-of-boston-university-llm-research-benchmark/</a></td>
    </tr>
    <tr>
      <td>Centre for International Governance Innovation — Global AI Risks Initiative</td>
      <td>Centre for International Governance Innovation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$300,000</td>
      <td>July 2024</td>
      <td>Open Philanthropy recommended a grant of $300,000 to the Centre for International Governance Innovation (CIGI) to support the Global AI Risks Initiative, a project within CIGI that fosters international cooperation to address global-scale risks from artificial intelligence. Duncan Cass-Beggs is Executive Director of the Initiative.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-international-governance-innovation-global-ai-risks-initiative/">https://www.openphilanthropy.org/grants/centre-for-international-governance-innovation-global-ai-risks-initiative/</a></td>
    </tr>
    <tr>
      <td>University of Minnesota — Legal Automation Benchmark</td>
      <td>University of Minnesota, Twin Cities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$74,132</td>
      <td>June 2024</td>
      <td>Open Philanthropy recommended a grant of $74,132 to the University of Minnesota to support a project to develop benchmarks for assessing large language models’ capabilities in automating legal tasks, led by Assistant Professor Dongyeop Kang.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-minnesota-legal-automation-benchmark/">https://www.openphilanthropy.org/grants/university-of-minnesota-legal-automation-benchmark/</a></td>
    </tr>
    <tr>
      <td>Yale University — LLM Persuasiveness Evaluation</td>
      <td>Yale University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$596,200</td>
      <td>June 2024</td>
      <td>Photo courtesy of Yale’s Institution for Social and Policy Studies\nOpen Philanthropy recommended a grant of $596,200 over two years to Yale University to support a project evaluating the persuasiveness of large language models (LLMs), led by Associate Professor Josh Kalla.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/yale-university-llm-persuasiveness-evaluation/">https://www.openphilanthropy.org/grants/yale-university-llm-persuasiveness-evaluation/</a></td>
    </tr>
    <tr>
      <td>Palisade Research — General Support</td>
      <td>Palisade Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,680,000</td>
      <td>June 2024</td>
      <td>Open Philanthropy recommended a grant of $1,680,000 to Palisade Research for general support. Palisade Research studies AI capabilities to better understand misuse risks from current systems, and how advances in hacking, deception, and persuasion will affect the risk of catastrophic AI outcomes.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in August 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/palisade-research-general-support/">https://www.openphilanthropy.org/grants/palisade-research-general-support/</a></td>
    </tr>
    <tr>
      <td>UC Santa Barbara — LLM Use Case Database</td>
      <td>Santa Barbara</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$133,402</td>
      <td>June 2024</td>
      <td>Open Philanthropy recommended a grant of $133,402 to the University of California, Santa Barbara to support a project led by Professor William Wang to build a database and taxonomy of large language model (LLM) use cases. This is one of two grants we recently made to support this work.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-santa-barbara-llm-use-case-database/">https://www.openphilanthropy.org/grants/uc-santa-barbara-llm-use-case-database/</a></td>
    </tr>
    <tr>
      <td>Princeton University — Software Engineering LLM Benchmark</td>
      <td>Princeton University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,045,620</td>
      <td>May 2024</td>
      <td>Open Philanthropy recommended a grant of $1,045,620 to Princeton University to support a project to develop a benchmark for evaluating the performance of Large Language Model (LLM) agents in software engineering tasks, led by Assistant Professor Karthik Narasimhan.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/princeton-university-software-engineering-llm-benchmark/">https://www.openphilanthropy.org/grants/princeton-university-software-engineering-llm-benchmark/</a></td>
    </tr>
    <tr>
      <td>Effective Altruism Israel — Information Security Talent Development</td>
      <td>Effective Altruism Israel</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$139,228</td>
      <td>May 2024</td>
      <td>Image courtesy of Effective Altruism Israel\nOpen Philanthropy recommended a grant of $139,228 to Effective Altruism Israel to support a pilot program for information security talent development.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/effective-altruism-israel-information-security-talent-development/">https://www.openphilanthropy.org/grants/effective-altruism-israel-information-security-talent-development/</a></td>
    </tr>
    <tr>
      <td>Faculty AI — Wargame for AI Documentary</td>
      <td>Faculty AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$92,597</td>
      <td>May 2024</td>
      <td>Photo courtesy of Joe Maher\nOpen Philanthropy recommended a grant of £75,000 (approximately $92,597 at the time of conversion) over two years to Faculty AI to support the development of a wargame for a documentary about the potential risks of advanced AI.\nThis falls within our focus area of growing and empowering the community of people focused on global catastrophic risk reduction.</td>
      <td><a href="https://www.openphilanthropy.org/grants/faculty-ai-wargame-for-ai-documentary/">https://www.openphilanthropy.org/grants/faculty-ai-wargame-for-ai-documentary/</a></td>
    </tr>
    <tr>
      <td>Stanford University — LLM-Generated Research Ideation Benchmark</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$880,000</td>
      <td>May 2024</td>
      <td>Open Philanthropy recommended a grant of $880,000 over two years to Stanford University to support a project evaluating the abilities of Large Language Model (LLM) agents at generating Machine Learning (ML) research project ideas via a large-scale experiment, led by Assistant Professor Tatsunori Hashimoto.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-llm-generated-research-ideation-benchmark/">https://www.openphilanthropy.org/grants/stanford-university-llm-generated-research-ideation-benchmark/</a></td>
    </tr>
    <tr>
      <td>Northeastern University — Large Language Model Interpretability Research (2024)</td>
      <td>Northeastern University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,095,017</td>
      <td>May 2024</td>
      <td>Open Philanthropy recommended a grant of $1,095,017 over two years to Northeastern University to support Professor David Bau‘s research on interpreting large language models.\nThis follows our November 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research-2024/">https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research-2024/</a></td>
    </tr>
    <tr>
      <td>Carnegie Mellon University — LLM Use Case Database</td>
      <td>Carnegie Mellon University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$266,805</td>
      <td>May 2024</td>
      <td>Open Philanthropy recommended a gift of $266,805 to Carnegie Mellon University to support a project led by Professors Hong Shen and Fei Fang to build a database and taxonomy of large language model (LLM) use cases. This is one of two grants we recently made to support this work.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs. It falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/carnegie-mellon-university-llm-use-case-database/">https://www.openphilanthropy.org/grants/carnegie-mellon-university-llm-use-case-database/</a></td>
    </tr>
    <tr>
      <td>Metaculus — Forecasting Tournaments</td>
      <td>Metaculus</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$532,400</td>
      <td>May 2024</td>
      <td>Image courtesy of Metaculus\nOpen Philanthropy recommended a grant of $532,400 to Metaculus to support a series of tournaments comparing the forecasting abilities of large language models and humans.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/metaculus-forecasting-tournaments/">https://www.openphilanthropy.org/grants/metaculus-forecasting-tournaments/</a></td>
    </tr>
    <tr>
      <td>Sage — AI Explainers</td>
      <td>Sage</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$550,000</td>
      <td>May 2024</td>
      <td>Image courtesy of Sage\nOpen Philanthropy recommended a grant of $550,000 to Sage to support the creation of public explainers and demos explaining AI capabilities and their effects for AI Digest, a website focused on recent developments in AI.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/sage-ai-explainers/">https://www.openphilanthropy.org/grants/sage-ai-explainers/</a></td>
    </tr>
    <tr>
      <td>AI Standards Lab — AI Standards and Risk Management Frameworks</td>
      <td>AI Standards Lab</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$200,000</td>
      <td>May 2024</td>
      <td>Open Philanthropy recommended a grant of $200,000 to the AI Standards Lab to support the development of AI standards and risk management frameworks.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-standards-lab-ai-standards-and-risk-management-frameworks/">https://www.openphilanthropy.org/grants/ai-standards-lab-ai-standards-and-risk-management-frameworks/</a></td>
    </tr>
    <tr>
      <td>Apollo Research — General Support</td>
      <td>Apollo Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,178,700</td>
      <td>May 2024</td>
      <td>Image courtesy of Apollo Research\nOpen Philanthropy recommended a grant of $2,178,700 to Apollo Research for general support. Apollo Research is a research organization that develops evaluations to test the alignment and safety of AI models, with a focus on capability evaluations, interpretability research, and governance.\nThis follows our June 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/apollo-research-general-support/">https://www.openphilanthropy.org/grants/apollo-research-general-support/</a></td>
    </tr>
    <tr>
      <td>Rethink Priorities — Research on LLM Use</td>
      <td>Rethink Priorities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$115,887</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $115,887 to Rethink Priorities to support a series of surveys and interviews led by David Moss on how various groups use and perceive large language models in their professional and day-to-day lives.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rethink-priorities-research-on-llm-use/">https://www.openphilanthropy.org/grants/rethink-priorities-research-on-llm-use/</a></td>
    </tr>
    <tr>
      <td>University of Wisconsin–Madison — Scalable Oversight Research</td>
      <td>University of Wisconsin–Madison</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $100,000 to the University of Wisconsin–Madison to support Professor Dimitris Papailiopoulos’ empirical and theoretical research on scalable oversight of AI. This is one of three grants we’re making to support this work.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-wisconsin-madison-scalable-oversight-research/">https://www.openphilanthropy.org/grants/university-of-wisconsin-madison-scalable-oversight-research/</a></td>
    </tr>
    <tr>
      <td>Institute for AI Policy and Strategy — General Support (April 2024)</td>
      <td>Institute for AI Policy and Strategy</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$828,049</td>
      <td>April 2024</td>
      <td>Image courtesy of the Institute for AI Policy and Strategy\nOpen Philanthropy recommended a grant of $828,049 to the Institute for AI Policy and Strategy to support staff salaries and other operational costs, including researcher development. The Institute for AI Policy and Strategy (IAPS) is a think tank that studies AI policy and standards, compute governance, and international governance.\nThis follows our January 2024 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/institute-for-ai-policy-and-strategy-general-support-april-2024/">https://www.openphilanthropy.org/grants/institute-for-ai-policy-and-strategy-general-support-april-2024/</a></td>
    </tr>
    <tr>
      <td>Princeton University — Scalable Oversight Research</td>
      <td>Princeton University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $100,000 to Princeton University to support Professor Jason Lee’s empirical and theoretical research on scalable oversight of AI. This is one of three grants we’re making to support this work.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/princeton-university-scalable-oversight-research/">https://www.openphilanthropy.org/grants/princeton-university-scalable-oversight-research/</a></td>
    </tr>
    <tr>
      <td>University of California, San Diego — AI Persuasiveness Evaluation</td>
      <td>University of California, San Diego</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$470,731</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $470,731 to the University of California, San Diego to support research led by Professor Benjamin Bergen on the persuasiveness of large language models (LLMs). ​​The study will use game-like evaluations to assess LLMs’ ability to change minds, foster cooperation or defection in classic games, and negotiate for hidden goals in resource-allocation scenarios, all through freeform conversation.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-california-san-diego-ai-persuasiveness-evaluation/">https://www.openphilanthropy.org/grants/university-of-california-san-diego-ai-persuasiveness-evaluation/</a></td>
    </tr>
    <tr>
      <td>Forecasting Research Institute – Forecasting Benchmark</td>
      <td>Forecasting Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $100,000 to the Forecasting Research Institute to support a collaboration with Jacob Steinhardt’s lab to work on a large language model (LLM) forecasting benchmark.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/forecasting-research-institute-forecasting-benchmark/">https://www.openphilanthropy.org/grants/forecasting-research-institute-forecasting-benchmark/</a></td>
    </tr>
    <tr>
      <td>Forecasting Research Institute — Red-line Evaluations</td>
      <td>Forecasting Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$125,000</td>
      <td>April 2024</td>
      <td>Open Philanthropy recommended a grant of $125,000 to the Forecasting Research Institute to support a three-month project operationalizing and forecasting red-line evaluations of advanced AI systems.\nThis grant was funded as part of a funding stream for studying and forecasting the real-world impacts of systems built from LLMs and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/forecasting-research-institute-red-line-evaluations/">https://www.openphilanthropy.org/grants/forecasting-research-institute-red-line-evaluations/</a></td>
    </tr>
    <tr>
      <td>RAND Corporation — Emerging Technology Fellowships and Research (2024)</td>
      <td>RAND Corporation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,500,000</td>
      <td>April 2024</td>
      <td>Open Philanthropy gave a gift of $2.5 million to RAND Corporation, to be spent at RAND President Jason Matheny’s discretion. Matheny has designated this funding to support two initiatives: a technology policy training program, and a research fund to help produce information that policymakers need to make wise decisions about emerging technology and security priorities.\nWe have been impressed with Matheny’s past work on technology and security — at IARPA, at the Center for Security and Technology, and in the White House — and we believe RAND is well-positioned to use such funding to great impact. This gift was recommended by Luke Muehlhauser, who leads our grantmaking on AI governance.\nThis follows our April 2023 support and falls within our work on potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research-2024/">https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research-2024/</a></td>
    </tr>
    <tr>
      <td>Simon Institute for Longterm Governance — General Support</td>
      <td>Simon Institute for Longterm Governance</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,461,194</td>
      <td>April 2024</td>
      <td>Photo courtesy of the Simon Institute for Longterm Governance\nOpen Philanthropy recommended a grant of $1,461,194 over two years to the Simon Institute for Longterm Governance for general support. The Simon Institute is a think tank that works to mitigate global catastrophic risks and promote agency for present and future generations by fostering multilateral governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/simon-institute-for-longterm-governance-general-support/">https://www.openphilanthropy.org/grants/simon-institute-for-longterm-governance-general-support/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Latent Adversarial Training Project</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$45,000</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a grant of $45,000 to the Berkeley Existential Risk Initiative to support a project led by Stephen Casper on latent adversarial training methods for large language models.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-latent-adversarial-training-project/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-latent-adversarial-training-project/</a></td>
    </tr>
    <tr>
      <td>FutureHouse — Benchmarks for Biology Research and Development</td>
      <td>FutureHouse</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,946,250</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a grant of $2,946,250 over two years to FutureHouse to support the creation of benchmarks to test how well large language models can perform biology R&D. FutureHouse is a nonprofit with a mission to build semi-autonomous AIs that can conduct scientific research.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/future-house-benchmarks-for-biology-research-and-development/">https://www.openphilanthropy.org/grants/future-house-benchmarks-for-biology-research-and-development/</a></td>
    </tr>
    <tr>
      <td>Carnegie Mellon University — Benchmark for Web-Based Tasks</td>
      <td>Carnegie Mellon University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$547,452</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a grant of $547,452 to Carnegie Mellon University to support research led by Professor Graham Neubig to develop a benchmark for the performance of large language models conducting web-based tasks in the work of software engineers, managers, and accountants.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/carnegie-mellon-university-benchmark-for-web-based-tasks/">https://www.openphilanthropy.org/grants/carnegie-mellon-university-benchmark-for-web-based-tasks/</a></td>
    </tr>
    <tr>
      <td>FutureSearch – Benchmark for Language Model Forecasting</td>
      <td>FutureSearch</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$606,600</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a grant of $606,600 to FutureSearch to support the development of a benchmark to evaluate the ability of large language models (LLMs) to forecast geopolitical events. The benchmark will be freely available to the public.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/futuresearch-benchmark-for-language-model-forecasting/">https://www.openphilanthropy.org/grants/futuresearch-benchmark-for-language-model-forecasting/</a></td>
    </tr>
    <tr>
      <td>Institute for Replication — Large Language Model Replication Games</td>
      <td>Institute for Replication</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$174,112</td>
      <td>February 2024</td>
      <td>Photo courtesy of Institute for Replication\nOpen Philanthropy recommended a grant of $174,112 to the University of Ottawa to support the Institute for Replication in producing a series of replication games led by Abel Brodeur that test how large language models can help humans reproduce the results of economics papers and find errors in original papers. The replication games will be run as randomized controlled trials.\nThis grant was funded via a request for proposals for projects studying and forecasting the real-world impacts of systems built from LLMs. It falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/institute-for-replication-large-language-model-replication-games/">https://www.openphilanthropy.org/grants/institute-for-replication-large-language-model-replication-games/</a></td>
    </tr>
    <tr>
      <td>University of Michigan — Scalable Oversight Research</td>
      <td>University of Michigan</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>February 2024</td>
      <td>Open Philanthropy recommended a grant of $100,000 to support Professor Samet Oymak’s empirical and theoretical research on scalable oversight of AI. This is one of three grants we’re making to support this work. \nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-michigan-scalable-oversight-research/">https://www.openphilanthropy.org/grants/university-of-michigan-scalable-oversight-research/</a></td>
    </tr>
    <tr>
      <td>Touro College & University System — AI Governance Legal Research</td>
      <td>Touro College & University System</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$19,783</td>
      <td>February 2024</td>
      <td>Open Philanthropy recommended a grant of $19,783 to the Touro College & University System to support Professor Gabriel Weil‘s legal research on AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/touro-college-university-system-ai-governance-legal-research/">https://www.openphilanthropy.org/grants/touro-college-university-system-ai-governance-legal-research/</a></td>
    </tr>
    <tr>
      <td>Straumli — LLM Cyberoffense Benchmark</td>
      <td>Straumli</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$400,000</td>
      <td>February 2024</td>
      <td>Photo courtesy of Straumli\nOpen Philanthropy recommended a grant of $400,000 to Straumli to support a project led by Paul Bricman to develop a benchmark for the cyberoffense capabilities of Large Language Model (LLM) agents.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/straumli-llm-cyberoffense-benchmark/">https://www.openphilanthropy.org/grants/straumli-llm-cyberoffense-benchmark/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — AI Risk Management Frameworks (2024)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$713,000</td>
      <td>February 2024</td>
      <td>Image courtesy of the Berkeley Existential Risk Initiative\nOpen Philanthropy recommended a grant of $713,000 to the Berkeley Existential Risk Initiative to support the development of risk management frameworks and the analysis of AI standards. An additional grant to the Center for Long-Term Cybersecurity will support related work.\nThis follows our April 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-risk-management-frameworks-2024/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-risk-management-frameworks-2024/</a></td>
    </tr>
    <tr>
      <td>Center for Long-Term Cybersecurity — AI Risk Management Frameworks (2024)</td>
      <td>Center for Long-Term Cybersecurity</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$200,000</td>
      <td>February 2024</td>
      <td>Image courtesy of the Center for Long-Term Cybersecurity\nOpen Philanthropy recommended a grant of $200,000 to the Center for Long-Term Cybersecurity to support the development of risk management frameworks and the analysis of AI standards. An additional grant to the Berkeley Existential Risk Initiative will support related work.\nThis follows our April 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-risk-management-frameworks-2024/">https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-risk-management-frameworks-2024/</a></td>
    </tr>
    <tr>
      <td>Lee Foster — LLM Misuse Database</td>
      <td>Lee Foster</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$223,000</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a grant of $223,000 to Lee Foster to support the creation of a database tracking instances of users attempting to misuse large language models, and the production of quarterly reports summarizing findings.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/lee-foster-llm-misuse-database/">https://www.openphilanthropy.org/grants/lee-foster-llm-misuse-database/</a></td>
    </tr>
    <tr>
      <td>Harvard University — AI Interpretability, Controllability, and Safety Research</td>
      <td>Harvard University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,000,000</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a grant of $1,000,000 over two years to Harvard University to support research led by Martin Wattenberg and Fernanda Viégas on artificial intelligence interpretability, controllability, and safety. Their research will focus on the extent to which large language models have developed internal models of the user and of themselves as distinct agents.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/harvard-university-ai-interpretability-controllability-and-safety-research/">https://www.openphilanthropy.org/grants/harvard-university-ai-interpretability-controllability-and-safety-research/</a></td>
    </tr>
    <tr>
      <td>FAR AI — AI Alignment Research Projects</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$645,750</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a grant of $645,750 to FAR AI to support research expenses for several AI alignment research projects. Ethan Perez will mentor the independent researchers undertaking these projects.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-ai-alignment-research-projects/">https://www.openphilanthropy.org/grants/far-ai-ai-alignment-research-projects/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — CHAI Internships</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$500,000</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a grant of $500,000 over two years to support the Berkeley Existential Risk Initiative’s collaboration with the Center for Human-Compatible (CHAI) to work on research relevant to potential risks from artificial intelligence and machine learning. The funding will be used to support interns at CHAI by providing them with research stipends, software, and compute.\nThis follows our February 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-internships/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-internships/</a></td>
    </tr>
    <tr>
      <td>Institute for AI Policy and Strategy — General Support</td>
      <td>Institute for AI Policy and Strategy</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$3,011,895</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a grant of $3,011,895 to the Institute for AI Policy and Strategy to support staff salaries and other operational costs, including researcher development. The Institute for AI Policy and Strategy (IAPS) is a think tank that studies AI policy and standards, compute governance, and international governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in July 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/institute-for-ai-policy-strategy-general-support/">https://www.openphilanthropy.org/grants/institute-for-ai-policy-strategy-general-support/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support – Astra Fellowship</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$617,000</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended $617,000 to AI Safety Support (AISS) to support individuals participating in the Astra Fellowship, a program that pairs independent AI safety researchers with experienced advisors to collaborate on a two- to three-month research project.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in July 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-astra-fellowship/">https://www.openphilanthropy.org/grants/ai-safety-support-astra-fellowship/</a></td>
    </tr>
    <tr>
      <td>University of Illinois Foundation — LLM Hacking Benchmarks</td>
      <td>University of Illinois Urbana-Champaign</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$800,000</td>
      <td>January 2024</td>
      <td>Image courtesy of The Grainger College of Engineering\nOpen Philanthropy recommended a grant of $800,000 to the University of Illinois Foundation to support a project that will develop benchmarks to measure how well LLM agents perform hacking tasks, led by Daniel Kang.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-illinois-foundation-llm-hacking-benchmarks/">https://www.openphilanthropy.org/grants/university-of-illinois-foundation-llm-hacking-benchmarks/</a></td>
    </tr>
    <tr>
      <td>Safe AI Forum — Operating Expenses</td>
      <td>Safe AI Forum</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,167,513</td>
      <td>December 2023</td>
      <td>Open Philanthropy recommended two grants totaling $1,167,513 to the Safe AI Forum to support its work on AI policy.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/safe-ai-forum-operating-expenses/">https://www.openphilanthropy.org/grants/safe-ai-forum-operating-expenses/</a></td>
    </tr>
    <tr>
      <td>ETH Zurich Foundation (USA) — Machine Learning Research Support</td>
      <td>ETH Zurich Fondation (USA)</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$25,000</td>
      <td>November 2023</td>
      <td>Open Philanthropy recommended a grant of $25,000 to the ETH Zurich Foundation (USA) to allow PhD student Javier Rando Ramirez and assistant professor Florian Tramer to run a Trojan detection competition at the 2024 Security and Trustworthy ML conference.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/eth-zurich-foundation-usa-machine-learning-research-support/">https://www.openphilanthropy.org/grants/eth-zurich-foundation-usa-machine-learning-research-support/</a></td>
    </tr>
    <tr>
      <td>Stanford University — AI Economic Impacts Workshop</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$120,000</td>
      <td>November 2023</td>
      <td>Photo courtesy of Christine Baker\nOpen Philanthropy recommended a gift of $120,000 to Stanford University to support a workshop on the economic and societal impacts of transformative AI. One of the organizers was the Stanford Digital Economy Lab at the Stanford Institute for Human-Centered Artificial Intelligence, which Erik Brynjolfsson currently leads.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-ai-economic-impacts-workshop/">https://www.openphilanthropy.org/grants/stanford-university-ai-economic-impacts-workshop/</a></td>
    </tr>
    <tr>
      <td>Eleuther AI — Interpretability Research</td>
      <td>Eleuther AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,642,273</td>
      <td>November 2023</td>
      <td>Open Philanthropy recommended a grant of $2,642,273 to Eleuther AI over four years to support the work of Nora Belrose. Nora will conduct research on AI interpretability and hire other researchers to assist her in this work.\n\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/eleuther-ai-interpretability-research/">https://www.openphilanthropy.org/grants/eleuther-ai-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>Alignment Research Engineer Accelerator — AI Safety Technical Program (2023)</td>
      <td>Alignment Research Engineer Accelerator</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$127,350</td>
      <td>November 2023</td>
      <td>Photo courtesy of the Alignment Research Engineer Accelerator\nOpen Philanthropy recommended a grant of £100,000 ($127,350 at the time of conversion) to support the Alignment Research Engineer Accelerator (ARENA), a program to help individuals interested in AI safety develop technical expertise in machine learning.\nThis follows our February 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/alignment-research-engineer-accelerator-ai-safety-technical-program-2023/">https://www.openphilanthropy.org/grants/alignment-research-engineer-accelerator-ai-safety-technical-program-2023/</a></td>
    </tr>
    <tr>
      <td>London Initiative for Safe AI (LISA) — General Support</td>
      <td>London Initiative for Safe AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$237,000</td>
      <td>November 2023</td>
      <td>Image courtesy of London Initiative for Safe AI\nOpen Philanthropy recommended a grant of $237,000 to the London Initiative for Safe AI (LISA) for general support. LISA is a research center that improves the safety of advanced AI systems by supporting and empowering the London AI safety community.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/london-initiative-for-safe-ai-lisa-general-support/">https://www.openphilanthropy.org/grants/london-initiative-for-safe-ai-lisa-general-support/</a></td>
    </tr>
    <tr>
      <td>University of California, Berkeley — AI Alignment Workshop</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$26,000</td>
      <td>October 2023</td>
      <td>Photo courtesy of the University of California, Berkeley\nOpen Philanthropy recommended a gift of $26,000 to UC Berkeley to support a small workshop bringing together experts in computational social choice theory and AI alignment. Professor Wesley H. Holliday will organize the workshop, and participants will discuss AI alignment and AI as a tool for democracy. The grant will also allow Dr. Jobst Heitzig, a co-organizer of the workshop, to attend three related machine learning conferences.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-california-berkeley-ai-alignment-workshop/">https://www.openphilanthropy.org/grants/university-of-california-berkeley-ai-alignment-workshop/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — University Collaboration Program</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>October 2023</td>
      <td>Open Philanthropy recommended a grant of $70,000 to the Berkeley Existential Risk Initiative (BERI) to support its university collaboration program. Selected applicants become eligible for support and services from BERI that would be difficult or impossible to obtain through normal university channels. BERI will use these funds to increase the size of its 2024 cohort.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-university-collaboration-program/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-university-collaboration-program/</a></td>
    </tr>
    <tr>
      <td>RAND Corporation — Emerging Technology Initiatives</td>
      <td>RAND Corporation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$10,500,000</td>
      <td>October 2023</td>
      <td>Open Philanthropy gave a gift of $10.5 million to RAND Corporation, to be spent at the discretion of RAND president and CEO Jason Matheny. Matheny has allocated this funding across multiple initiatives, including:\n\nA technology policy training program.\nSupport for the Pardee RAND Graduate School.\nA new research center focused on China studies.\nA research fund that will help to produce information for policymakers about emerging technology and security priorities.\n\nThis gift was recommended by Luke Muehlhauser, who leads our grantmaking on AI governance. It follows our April 2023 support, and falls within our work on potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-initiatives/">https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-initiatives/</a></td>
    </tr>
    <tr>
      <td>Northeastern University — Mechanistic Interpretability Research</td>
      <td>Northeastern University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$116,072</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended a grant of $116,072 to Northeastern University to support a postdoctoral position for Sam Marks in Professor David Bau’s lab, where Sam will conduct research on mechanistic interpretability.\nThis follows our November 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/northeastern-university-mechanistic-interpretability-research/">https://www.openphilanthropy.org/grants/northeastern-university-mechanistic-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>OpenMined — Software for AI Audits</td>
      <td>OpenMined</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,000,000</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended a grant of $6,000,000 to OpenMined to support work on developing software that facilitates access to advanced AI systems for external researchers and auditors while preserving privacy, security, and intellectual property.\nThis follows our April 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/openmined-software-for-ai-audits/">https://www.openphilanthropy.org/grants/openmined-software-for-ai-audits/</a></td>
    </tr>
    <tr>
      <td>FAR AI — Alignment Workshop</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$166,500</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended a grant of $166,500 to FAR AI to support a two-day alignment workshop in advance of NeurIPS 2023, a major machine learning and computational neuroscience conference.\nThis follows our August 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-alignment-workshop/">https://www.openphilanthropy.org/grants/far-ai-alignment-workshop/</a></td>
    </tr>
    <tr>
      <td>Matthew Kenney – Language Model Capabilities Benchmarking</td>
      <td>Matthew Kenney</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$397,350</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended a grant of $397,350 to Matthew Kenney to support a project developing evaluations that will benchmark the ability of language model agents to accelerate AI R&D.\nThis grant was funded via a request for proposals for projects benchmarking LLM agents on consequential real-world tasks. This falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/matthew-kenney-language-model-capabilities-benchmarking/">https://www.openphilanthropy.org/grants/matthew-kenney-language-model-capabilities-benchmarking/</a></td>
    </tr>
    <tr>
      <td>Surge AI — Data Production for AI Safety Research</td>
      <td>Surge AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$126,250</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended two contracts totaling $126,250 over two years with Surge AI to support Gabriel Recchia in producing data points for a research project on sandwiching experiments and capability evaluations of large language models.\nThis project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe contract amount was updated in March 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/surge-ai-data-production-for-ai-safety-research/">https://www.openphilanthropy.org/grants/surge-ai-data-production-for-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>University of Pennsylvania — AI Governance Roundtables</td>
      <td>University of Pennsylvania</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$110,000</td>
      <td>September 2023</td>
      <td>Open Philanthropy recommended a gift of $110,000 over two years to the University of Pennsylvania to support a series of roundtables led by Professor Peter Conti-Brown. At these events, experts will discuss how insights from financial regulation might inform emerging discussions on AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-pennsylvania-ai-governance-roundtables/">https://www.openphilanthropy.org/grants/university-of-pennsylvania-ai-governance-roundtables/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Scalable Oversight Dataset</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>September 2023</td>
      <td>Photo courtesy of Berkeley Existential Risk Initiative\nOpen Philanthropy recommended a grant of $70,000 to the Berkeley Existential Risk Initiative to support the creation of a scalable oversight dataset. The purpose of the dataset is to collect questions that non-experts can’t answer even with the internet at their disposal; these kinds of questions can be used to test how well AI systems can lead humans to the right answers without misleading them.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-scalable-oversight-dataset/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-scalable-oversight-dataset/</a></td>
    </tr>
    <tr>
      <td>Effective Ventures Foundation — AI Safety Communications Centre</td>
      <td>Effective Ventures Foundation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$288,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $288,000 to the Effective Ventures Foundation to support the AI Safety Communications Centre. This project provides the AI safety community with communications support, and connects journalists to AI safety experts and resources.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/effective-ventures-foundation-ai-safety-communications-centre/">https://www.openphilanthropy.org/grants/effective-ventures-foundation-ai-safety-communications-centre/</a></td>
    </tr>
    <tr>
      <td>Guide Labs — Open Access Interpretability Project</td>
      <td>Guide Labs</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$750,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $750,000 over 1.5 years to Guide Labs to support a project developing and testing AI error diagnostics and model guiding tools. To support AI safety and alignment, these tools will be made freely available to the general public.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/guide-labs-open-access-interpretability-project/">https://www.openphilanthropy.org/grants/guide-labs-open-access-interpretability-project/</a></td>
    </tr>
    <tr>
      <td>Swiss AI Safety Summer Camp — AI Safety Bootcamp</td>
      <td>Swiss AI Safety Summer Camp</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$51,248</td>
      <td>August 2023</td>
      <td>Photo courtesy of Swiss AI Safety Summer Camp\nOpen Philanthropy recommended a grant of CHF 45,165 (approximately $51,248 at the time of conversion) to the Swiss AI Safety Summer Camp to support its 2023 bootcamp. The program offers a multidisciplinary learning experience through activities such as deep learning courses, paper readings, discussions, presentations, and lectures.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/swiss-ai-safety-summer-camp-ai-safety-bootcamp/">https://www.openphilanthropy.org/grants/swiss-ai-safety-summer-camp-ai-safety-bootcamp/</a></td>
    </tr>
    <tr>
      <td>AI Worldviews Contest Winners</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$225,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy awarded a total of $225,000 to the winners of the 2023 Open Philanthropy AI Worldviews Contest. The goal of the contest was to surface novel considerations that could influence our views on AI timelines and AI risk. The winners were:\n\nBasil Halperin, Zachary Mazlish, and Trevor Chow\nQuintin Pope\nDavid Wheaton\nZach Freitas-Groff\nMichael Cohen\nAlex Bates\n\nWe sought entries to this contest here. The winning entries are available to read here. \nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-worldviews-contest-winners/">https://www.openphilanthropy.org/grants/ai-worldviews-contest-winners/</a></td>
    </tr>
    <tr>
      <td>Legal Priorities Project — Law & AI Summer Research Fellowship</td>
      <td>Legal Priorities Project</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$75,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $75,000 to the Legal Priorities Project (LPP) to support their Summer Research Fellowship in Law & AI. Participants will work with researchers at LPP on projects at the intersection of law and risks from advanced AI.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/legal-priorities-project-law-ai-summer-research-fellowship/">https://www.openphilanthropy.org/grants/legal-priorities-project-law-ai-summer-research-fellowship/</a></td>
    </tr>
    <tr>
      <td>Modulo Research — AI Safety Research</td>
      <td>Modulo Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$408,255</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $408,255 over two years to Modulo Research to support research — led by Gabriel Recchia — into large language model sandwiching experiments, dataset development, and capability evaluations.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/modulo-research-ai-safety-research/">https://www.openphilanthropy.org/grants/modulo-research-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>University of Maryland — Research on Neural Network Generalization</td>
      <td>University of Maryland</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$350,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $350,000 over three years to the University of Maryland to support research led by Professor Tom Goldstein on how neural networks generalize.\nWe sought applications for this funding to support FTX Future Fund grantees affected by the fund’s collapse. The application is still open, and we may fund more applicants.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-maryland-research-on-neural-network-generalization/">https://www.openphilanthropy.org/grants/university-of-maryland-research-on-neural-network-generalization/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — General Support (2023)</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$620,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $620,000 over two years to AI Impacts for general support. AI Impacts works to answer questions about the future of artificial intelligence.\nThis follows our June 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-general-support-2023/">https://www.openphilanthropy.org/grants/ai-impacts-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — Expert Survey on Progress in AI</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$345,000</td>
      <td>August 2023</td>
      <td>Open Philanthropy recommended a grant of $345,000 to AI Impacts to support an expert survey on progress in artificial intelligence. AI Impacts works to answer questions about the future of artificial intelligence.\nThis follows our June 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in March 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-expert-survey-on-progress-in-ai/">https://www.openphilanthropy.org/grants/ai-impacts-expert-survey-on-progress-in-ai/</a></td>
    </tr>
    <tr>
      <td>Massachusetts Institute of Technology — AI Trends and Impacts Research (2023)</td>
      <td>Massachusetts Institute of Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,911,324</td>
      <td>July 2023</td>
      <td>Open Philanthropy recommended a grant of $2,911,324 over four years to the Massachusetts Institute of Technology to support research led by Neil Thompson on modeling the trends and impacts of AI and computing. \nThis follows our March 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2023/">https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2023/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Lab Retreat</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$35,000</td>
      <td>July 2023</td>
      <td>Open Philanthropy recommended a grant of $35,000 to the Berkeley Existential Risk Initiative to support a retreat for Anca Dragan’s BAIR lab group, where members will discuss potential risks from advanced artificial intelligence.\nThis follows our November 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-lab-retreat/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-lab-retreat/</a></td>
    </tr>
    <tr>
      <td>FAR AI — General Support (2023)</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$460,000</td>
      <td>July 2023</td>
      <td>Open Philanthropy recommended a grant of $460,000 to FAR AI for general support.\nThis follows our March 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-general-support-2023/">https://www.openphilanthropy.org/grants/far-ai-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — SERI MATS 4.0</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$428,942</td>
      <td>June 2023</td>
      <td>Photo courtesy of Berkeley Existential Risk Initiative\nOpen Philanthropy recommended a grant of $428,942 to the Berkeley Existential Risk Initiative to support their collaboration with Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with in-person alignment research communities.\nThis grant will support the MATS program’s fourth cohort. This follows our November 2022 support for the previous iteration of MATS, and falls within our focus area of potential risks from advanced artificial intelligence. We also made a separate grant to AI Safety Support for this cohort.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-4-0/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-4-0/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support — SERI MATS 4.0</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,240,840</td>
      <td>June 2023</td>
      <td>Photo courtesy of Berkeley Existential Risk Initiative\nOpen Philanthropy recommended three grants totaling $1,240,840 to AI Safety Support to support their collaboration with Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with in-person alignment research communities.\nThese grants will support the MATS program’s fourth cohort. They follow our November 2022 support for the previous iteration of MATS, and fall within our focus area of growing and empowering the community of people focused on global catastrophic risk reduction. We also made a separate grant to the Berkeley Existential Risk Initiative for this cohort.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-4-0/">https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-4-0/</a></td>
    </tr>
    <tr>
      <td>Foundation for American Innovation — AI Safety Policy Advocacy</td>
      <td>Foundation for American Innovation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$552,500</td>
      <td>June 2023</td>
      <td>Open Philanthropy recommended a grant of $552,500 to the Foundation for American Innovation to support its work on policy research and advocacy intended to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/foundation-for-american-innovation-ai-safety-policy-advocacy/">https://www.openphilanthropy.org/grants/foundation-for-american-innovation-ai-safety-policy-advocacy/</a></td>
    </tr>
    <tr>
      <td>Redwood Research — General Support (2023)</td>
      <td>Redwood Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,300,000</td>
      <td>June 2023</td>
      <td>Open Philanthropy recommended a grant of $5,300,000 to Redwood Research for general support. Redwood Research is a nonprofit research institution focused on aligning advanced AI with human interests.\nThis follows our August 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/">https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>Apollo Research — Startup Funding</td>
      <td>Apollo Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,535,480</td>
      <td>June 2023</td>
      <td>Open Philanthropy recommended a grant of $1,535,480 to Apollo Research for startup costs. Apollo Research is a new organization that will conduct research on how to evaluate whether AI models are aligned and safe, with a focus on interpretability and detecting whether models are deceptive. Apollo also plans to do research on AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/apollo-research-startup-funding/">https://www.openphilanthropy.org/grants/apollo-research-startup-funding/</a></td>
    </tr>
    <tr>
      <td>Owain Evans Research Group — AI Evaluations Research</td>
      <td>Effective Ventures Foundation USA</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$838,620</td>
      <td>May 2023</td>
      <td>Open Philanthropy recommended a grant of $838,620 to Effective Ventures Foundation USA to support a new AI safety research group, led by Owain Evans, that will focus on evaluating whether AI models have dangerous capabilities.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/owain-evans-research-group-ai-evaluations-research/">https://www.openphilanthropy.org/grants/owain-evans-research-group-ai-evaluations-research/</a></td>
    </tr>
    <tr>
      <td>University of Chicago — Research on Complementary AI</td>
      <td>University of Chicago</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$250,000</td>
      <td>May 2023</td>
      <td>Open Philanthropy recommended a grant of $250,000 to the University of Chicago to support research, led by Professor Chenhao Tan, on how to train AI systems to complement human efforts.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-chicago-research-on-complementary-ai/">https://www.openphilanthropy.org/grants/university-of-chicago-research-on-complementary-ai/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support — AI Safety Technical Program (May 2023)</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$146,165</td>
      <td>May 2023</td>
      <td>Open Philanthropy recommended a grant of $146,165 to AI Safety Support to support the Alignment Research Engineer Accelerator (ARENA) program, which helps individuals interested in AI safety improve their technical expertise in machine learning. This is one of two grants we’re making to support the program; the other will go to Conjecture.\nThis follows our February 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-ai-safety-technical-program-may-2023/">https://www.openphilanthropy.org/grants/ai-safety-support-ai-safety-technical-program-may-2023/</a></td>
    </tr>
    <tr>
      <td>Mila — Workshop on Human-Level AI</td>
      <td>Mila</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$50,000</td>
      <td>May 2023</td>
      <td>Open Philanthropy recommended a grant of $50,000 to Mila – Quebec Artificial Intelligence Institute to support a workshop on human-level artificial intelligence, led by Professor Jacob Steinhardt, that will bring together experts on AI and AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/mila-workshop-on-human-level-ai/">https://www.openphilanthropy.org/grants/mila-workshop-on-human-level-ai/</a></td>
    </tr>
    <tr>
      <td>Centre for the Governance of AI — General Support (2023)</td>
      <td>Centre for the Governance of AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$3,000,000</td>
      <td>May 2023</td>
      <td>Open Philanthropy recommended a grant of $3,000,000 to the Centre for the Governance of AI (GovAI) for general support. GovAI conducts research on AI governance and works to develop a talent pipeline for those interested in entering the field.\nThis follows our December 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in July 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support-2/">https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support-2/</a></td>
    </tr>
    <tr>
      <td>Conjecture — AI Safety Technical Program</td>
      <td>Conjecture</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$223,569</td>
      <td>May 2023</td>
      <td>Photo courtesy of ARENA\nOpen Philanthropy recommended a grant of $223,569 to Conjecture to support the Alignment Research Engineer Accelerator (ARENA) program, which helps individuals interested in AI safety improve their technical expertise in machine learning. Conjecture will host the program. This is one of two grants we’re making to support the program; the other will go to AI Safety Support.\nThis follows our February 2023 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/conjecture-ai-safety-technical-program/">https://www.openphilanthropy.org/grants/conjecture-ai-safety-technical-program/</a></td>
    </tr>
    <tr>
      <td>Rethink Priorities — AI Governance Research (2023)</td>
      <td>Rethink Priorities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$154,810</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $154,801 to Rethink Priorities to support research on AI governance, with a focus on hardware security features.\nThis follows our March 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2023/">https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2023/</a></td>
    </tr>
    <tr>
      <td>Epoch — General Support (2023)</td>
      <td>Epoch</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,922,565</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $6,922,565 to Epoch for general support. Epoch researches trends in machine learning to better understand the pace of progress in artificial intelligence, and to help forecast the development of advanced AI and its subsequent economic impacts.\nThis follows our June 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/epoch-general-support-2023/">https://www.openphilanthropy.org/grants/epoch-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>University of Maryland — Policy Fellowship (2023)</td>
      <td>University of Maryland</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$312,959</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $312,959 to the University of Maryland to support a fellowship related to technology and national security.\nThis follows our December 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-maryland-policy-fellowship-2023/">https://www.openphilanthropy.org/grants/university-of-maryland-policy-fellowship-2023/</a></td>
    </tr>
    <tr>
      <td>Conjecture — SERI MATS (2023)</td>
      <td>Conjecture</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$245,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $245,000 to Conjecture to support its collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment.\nThis grant will support a London-based extension of the MATS program’s third cohort, which we supported last year.\nThis follows our October 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/">https://www.openphilanthropy.org/grants/conjecture-seri-mats-2023/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Language Model Evaluations</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$250,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $250,000 to Stanford University to support Holistic Evaluation of Language Models (HELM), which is a project to evaluate language models using many different tests designed to assess their capabilities, safety, and fairness. This work is led by Professor Percy Liang, and this funding will support an additional engineer for HELM.\nThis follows our November 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-language-model-evaluations/">https://www.openphilanthropy.org/grants/stanford-university-language-model-evaluations/</a></td>
    </tr>
    <tr>
      <td>University of Utah — AI Alignment Research</td>
      <td>University of Utah</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$140,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $140,000 to the University of Utah to support research led by Professor Daniel Brown on ways to verify the extent to which an AI system is aligned with human values.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-utah-ai-alignment-research/">https://www.openphilanthropy.org/grants/university-of-utah-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Center for AI Safety — General Support (2023)</td>
      <td>Center for AI Safety</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$4,025,729</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $4,025,729 to the Center for AI Safety for general support. The Center for AI Safety works on research, field-building, and advocacy to reduce existential risks from artificial intelligence.\nThis follows our November 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support-2023/">https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>Leap Labs — Interpretability Research</td>
      <td>Leap Labs</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$230,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $230,000 to Leap Labs to support research on AI interpretability, particularly model agnostic interpretability.\nThis grant was made primarily based on the recommendation of an external technical advisor. It falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/leap-labs-interpretability-research/">https://www.openphilanthropy.org/grants/leap-labs-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>Rethink Priorities — AI Governance Workshop</td>
      <td>Rethink Priorities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$302,390</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $302,390 to Rethink Priorities to support an in-person workshop bringing together professionals working on AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-workshop/">https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-workshop/</a></td>
    </tr>
    <tr>
      <td>National Science Foundation — Safe Learning-Enabled Systems</td>
      <td>National Science Foundation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,000,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $5,000,000 to the National Science Foundation to support its Safe Learning-Enabled Systems program, which will regrant the funds to foundational research projects aimed at finding ways to guarantee the safety of machine learning systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/national-science-foundation-safe-learning-enabled-systems/">https://www.openphilanthropy.org/grants/national-science-foundation-safe-learning-enabled-systems/</a></td>
    </tr>
    <tr>
      <td>RAND Corporation — Emerging Technology Fellowships and Research</td>
      <td>RAND Corporation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,500,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy gave a gift of $5.5 million to RAND Corporation, to be spent at RAND President Jason Matheny’s discretion. Matheny has designated this funding to launch two new initiatives: a technology policy training program, and a research fund to help produce information that policymakers need to make wise decisions about emerging technology and security priorities.\nWe have been impressed with Matheny’s past work on technology and security — at IARPA, at the Center for Security and Technology, and in the White House — and we believe RAND is well-positioned to use such funding to great impact. This gift was recommended by Luke Muehlhauser, who leads our grantmaking on AI governance.\nThis falls within our work on potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research/">https://www.openphilanthropy.org/grants/rand-corporation-emerging-technology-fellowships-and-research/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support — Situational Awareness Research</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$443,716</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended three grants totaling $443,716 to support research led by Owain Evans to evaluate whether machine learning models have situational awareness. These grants were made to AI Safety Support, Effective Ventures Foundation USA, and the Berkeley Existential Risk Initiative, and will support salaries, office space, and compute for this research project.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-situational-awareness-research/">https://www.openphilanthropy.org/grants/ai-safety-support-situational-awareness-research/</a></td>
    </tr>
    <tr>
      <td>FAR AI — AI Interpretability Research</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>March 2023</td>
      <td>Open Philanthropy recommended a grant of $100,000 to FAR AI to support a research project, led by Open Philanthropy AI Fellow Alex Tamkin, aimed at developing a neural network architecture that could serve as a more interpretable alternative to the transformer architecture used in leading language models.\nThis follows our December 2022 support and falls within our focus area of potential risks from advanced artificial intelligence</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-ai-interpretability-research/">https://www.openphilanthropy.org/grants/far-ai-ai-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>University of Illinois — AI Alignment Research</td>
      <td>University of Illinois</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$80,000</td>
      <td>March 2023</td>
      <td>Open Philanthropy recommended a grant of $80,000 to the University of Illinois to support Professor Ben Levinstein’s research on AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-illinois-ai-alignment-research/">https://www.openphilanthropy.org/grants/university-of-illinois-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>FAR AI — FAR Labs Office Space</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$280,000</td>
      <td>March 2023</td>
      <td>Open Philanthropy recommended a grant of $280,000 to FAR AI to support FAR Labs, an office space in Berkeley for people working on AI safety and alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-far-labs-office-space/">https://www.openphilanthropy.org/grants/far-ai-far-labs-office-space/</a></td>
    </tr>
    <tr>
      <td>San José State University — AI Research</td>
      <td>California State University, San José</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$39,000</td>
      <td>March 2023</td>
      <td>Open Philanthropy recommended a grant of $39,000 to San José State University to support research by Professor Yan Zhang on AI forecasting and AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/san-jose-state-university-ai-research/">https://www.openphilanthropy.org/grants/san-jose-state-university-ai-research/</a></td>
    </tr>
    <tr>
      <td>Longview Philanthropy — AI Policy Development at the OECD</td>
      <td>Longview Philanthropy</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$770,076</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of €720,000 (approximately $770,076 at the time of conversion) to Longview Philanthropy to support their collaboration with the Organization for Economic Co-operation and Development (OECD) on a project to develop potential policies that could reduce existential risks from artificial intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/longview-philanthropy-ai-policy-development-at-the-oecd/">https://www.openphilanthropy.org/grants/longview-philanthropy-ai-policy-development-at-the-oecd/</a></td>
    </tr>
    <tr>
      <td>Epoch — AI Worldview Investigations</td>
      <td>Epoch</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$188,558</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $188,558 to Epoch to support its “worldview investigations” related to AI.\nThis follows our June 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/epoch-ai-worldview-investigations/">https://www.openphilanthropy.org/grants/epoch-ai-worldview-investigations/</a></td>
    </tr>
    <tr>
      <td>University of Tuebingen — Adversarial Robustness Research</td>
      <td>University of Tuebingen</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$575,000</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $575,000 to the University of Tuebingen to support research led by Professor Matthias Bethge on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-tuebingen-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/university-of-tuebingen-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Cornell University — AI Safety Research</td>
      <td>Cornell University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$342,645</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $342,645 to Cornell University to support Professor Lionel Levine’s research related to AI alignment and safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/cornell-university-ai-safety-research/">https://www.openphilanthropy.org/grants/cornell-university-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>Brian Christian — Psychology Research</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$37,903</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of £29,700 (approximately $37,903 at the time of conversion) to Brian Christian to support a DPhil in psychology at the University of Oxford. His research will focus on human preferences, with the goal of informing efforts to align AI systems with human values.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/brian-christian-psychology-research/">https://www.openphilanthropy.org/grants/brian-christian-psychology-research/</a></td>
    </tr>
    <tr>
      <td>Alignment Research Engineer Accelerator — AI Safety Technical Program</td>
      <td>Alignment Research Engineer Accelerator</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$18,800</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $18,800 to support the Alignment Research Engineer Accelerator (ARENA), which is a program to help individuals interested in AI safety improve their technical skills in machine learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/alignment-research-engineer-accelerator-ai-safety-technical-program/">https://www.openphilanthropy.org/grants/alignment-research-engineer-accelerator-ai-safety-technical-program/</a></td>
    </tr>
    <tr>
      <td>Center for AI Safety — Philosophy Fellowship and NeurIPS Prizes</td>
      <td>Center for AI Safety</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,433,000</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $1,433,000 to the Center for AI Safety (CAIS) to support the CAIS Philosophy Fellowship, which is a research fellowship that will support philosophers researching topics related to AI safety. This grant also supported a workshop on adversarial robustness, as well as prizes for safety-related competitions at the 2022 NeurIPS conference.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-ai-safety-philosophy-fellowship/">https://www.openphilanthropy.org/grants/center-for-ai-safety-philosophy-fellowship/</a></td>
    </tr>
    <tr>
      <td>Responsible AI Collaborative — AI Incident Database</td>
      <td>Responsible AI Collaborative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $100,000 to the Responsible AI Collaborative to support its work maintaining the AI Incident Database, which is a database of incidents where AI systems have caused real-world harm.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/responsible-ai-collaborative-ai-incident-database/">https://www.openphilanthropy.org/grants/responsible-ai-collaborative-ai-incident-database/</a></td>
    </tr>
    <tr>
      <td>Neel Nanda — Interpretability Research</td>
      <td>Neel Nanda</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>January 2023</td>
      <td>Open Philanthropy recommended a grant of $70,000 to Neel Nanda to support his independent research on interpretability. His work is aimed at improving human understanding of neural networks and machine learning models.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/neel-nanda-interpretability-research/">https://www.openphilanthropy.org/grants/neel-nanda-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>University of Toronto — Alignment Research</td>
      <td>University of Toronto</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$80,000</td>
      <td>January 2023</td>
      <td>Toryn Klassen. Photo courtesy of Diana Tyszko, University of Toronto.\nOpen Philanthropy recommended a grant of $80,000 to the University of Toronto to support Toryn Klassen’s research on topics related to AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-toronto-alignment-research/">https://www.openphilanthropy.org/grants/university-of-toronto-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Adam Jermyn — Independent AI Alignment Research</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$19,231</td>
      <td>January 2023</td>
      <td>Open Philanthropy recommended a grant of $19,231 to Adam Jermyn to support his independent technical research on AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/adam-jermyn-independent-ai-alignment-research/">https://www.openphilanthropy.org/grants/adam-jermyn-independent-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>UC Santa Cruz — Adversarial Robustness Research (2023)</td>
      <td>University of California, Santa Cruz</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$114,000</td>
      <td>January 2023</td>
      <td>Open Philanthropy recommended a grant of $114,000 to the University of California, Santa Cruz to support research, led by Professor Cihang Xie, on adversarial robustness in AI systems. This funding will support salaries and other costs for two graduate students in Professor Xie’s lab.\nThis follows our January 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-california-santa-cruz-adversarial-robustness-research-2023/">https://www.openphilanthropy.org/grants/university-of-california-santa-cruz-adversarial-robustness-research-2023/</a></td>
    </tr>
    <tr>
      <td>University of British Columbia — AI Alignment Research</td>
      <td>University of British Columbia</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,375</td>
      <td>January 2023</td>
      <td>Open Philanthropy recommended a grant of $100,375 over two years to the University of British Columbia to support research led by Professor Jeff Clune on AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-british-columbia-ai-alignment-research/">https://www.openphilanthropy.org/grants/university-of-british-columbia-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Purdue University — Language Model Research</td>
      <td>Purdue University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$170,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $170,000 to Purdue University to support research led by Professor Xiangyu Zhang on improving the robustness of language models against adversarial attacks.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/purdue-university-language-model-research/">https://www.openphilanthropy.org/grants/purdue-university-language-model-research/</a></td>
    </tr>
    <tr>
      <td>Simon McGregor — AI Risk Workshop</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$7,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $7000 to Simon McGregor to support his work to organize a workshop on AI risk.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/simon-mcgregor-ai-risk-workshop/">https://www.openphilanthropy.org/grants/simon-mcgregor-ai-risk-workshop/</a></td>
    </tr>
    <tr>
      <td>FAR AI — General Support (2022)</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$625,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $625,000 to FAR AI for general support. FAR AI works to incubate and accelerate research agendas to ensure AI systems are more trustworthy and beneficial to society.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-general-support/">https://www.openphilanthropy.org/grants/far-ai-general-support/</a></td>
    </tr>
    <tr>
      <td>FAR AI — Inverse Scaling Prize</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$49,500</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $49,500 to FAR AI to support their Inverse Scaling Prize, which is a contest that awards prizes to contestants who find examples of tasks where language models perform worse as they scale.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-inverse-scaling-prize/">https://www.openphilanthropy.org/grants/far-ai-inverse-scaling-prize/</a></td>
    </tr>
    <tr>
      <td>Apart Research — AI Alignment Hackathons</td>
      <td>Apart Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$89,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended two grants totaling $130,050 to Apart Research to support their work hosting four “hackathons” where participants will work on small projects related to AI alignment.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/apart-research-ai-alignment-hackathons/">https://www.openphilanthropy.org/grants/apart-research-ai-alignment-hackathons/</a></td>
    </tr>
    <tr>
      <td>FAR AI — Interpretability Research</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$50,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $50,000 to FAR AI to support their research on machine learning interpretability, in collaboration with Open Philanthropy AI Fellow Alex Tamkin.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/far-ai-interpretability-research/">https://www.openphilanthropy.org/grants/far-ai-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>Jérémy Scheurer — Independent AI Alignment Research</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$110,000</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $110,000 to Jérémy Scheurer to support his independent research on AI alignment. This is part of our strategy to grow the field of AI researchers who are focused on reducing potential risks from advanced artificial intelligence.\nThis falls within our focus area of growing and empowering the EA community.</td>
      <td><a href="https://www.openphilanthropy.org/grants/jeremy-scheurer-independent-ai-alignment-research/">https://www.openphilanthropy.org/grants/jeremy-scheurer-independent-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Georgetown University — Policy Fellowship (2022)</td>
      <td>Georgetown University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$239,061</td>
      <td>December 2022</td>
      <td>Open Philanthropy recommended a grant of $239,061 to Georgetown University to support a fellowship related to AI and cybersecurity policy.\nThis follows our December 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2022/">https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2022/</a></td>
    </tr>
    <tr>
      <td>Northeastern University — Large Language Model Interpretability Research</td>
      <td>Northeastern University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$562,128</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of $562,128 over two years to Northeastern University to support Professor David Bau’s research on interpreting large language models.\nThis follows our April 2022 support for David Bau’s research and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research/">https://www.openphilanthropy.org/grants/northeastern-university-large-language-model-interpretability-research/</a></td>
    </tr>
    <tr>
      <td>AI Safety Hub — Safety Labs</td>
      <td>AI Safety Hub</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$47,359</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of £37,359 (approximately $47,359 at the time of conversion) to AI Safety Hub to support its Safety Labs program, which will match students with mentors while the students research questions related to AI safety.\nThis follows our September 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-hub-safety-labs/">https://www.openphilanthropy.org/grants/ai-safety-hub-safety-labs/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support — SERI MATS Program</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,538,000</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended three grants totaling $1,538,000 to AI Safety Support to support their collaboration with Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with in-person alignment research communities.\nThis grant will support the MATS program’s third cohort. This follows our April 2022 support for the previous iteration of MATS, and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-program/">https://www.openphilanthropy.org/grants/ai-safety-support-seri-mats-program/</a></td>
    </tr>
    <tr>
      <td>Alignment Research Center — General Support (November 2022)</td>
      <td>Alignment Research Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,250,000</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of $1,250,000 over two years to the Alignment Research Center for general support. The Alignment Research Center conducts research on how to align AI with human interests, with a focus on techniques that could be adopted in existing machine learning systems and effectively scale up to future systems.\nThis follows our March 2022 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/alignment-research-center-general-support-november-2022/">https://www.openphilanthropy.org/grants/alignment-research-center-general-support-november-2022/</a></td>
    </tr>
    <tr>
      <td>Jacob Steinhardt — AI Alignment Research</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of $100,000 to Jacob Steinhardt to provide operational support to Steinhardt’s lab at the University of California Berkeley, which specializes in research on how to align machine learning systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/jacob-steinhardt-ai-alignment-research/">https://www.openphilanthropy.org/grants/jacob-steinhardt-ai-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Center for AI Safety — General Support (2022)</td>
      <td>Center for AI Safety</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,160,000</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of $5,160,000 to the Center for AI Safety (CAIS) for general support. The Center for AI Safety does technical research and field-building aimed at reducing catastrophic and existential risks from artificial intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support/">https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support/</a></td>
    </tr>
    <tr>
      <td>Mordechai Rorvig — Independent AI Journalism</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$110,000</td>
      <td>November 2022</td>
      <td>Open Philanthropy recommended a grant of $110,000 to Mordechai Rorvig to support his independent journalism on topics related to computer science, AI, and AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/mordechai-rorvig-independent-ai-journalism/">https://www.openphilanthropy.org/grants/mordechai-rorvig-independent-ai-journalism/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Machine Learning Alignment Theory Scholars</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,047,268</td>
      <td>November 2022</td>
      <td>Photo courtesy of BERI\nOpen Philanthropy recommended a grant of $2,047,268 to the Berkeley Existential Risk Initiative to support their collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with the Berkeley alignment research community.\nThis grant will support the MATS program’s third cohort. This follows our April 2022 support for the previous iteration of MATS, and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-machine-learning-alignment-theory-scholars/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — General Support (2022)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>November 2022</td>
      <td>Photo courtesy of the Berkeley Existential Risk Initiative\nOpen Philanthropy recommended a grant of $100,000 to the Berkeley Existential Risk Initiative (BERI) for general support. BERI seeks to reduce existential risks to humanity by providing services and support to university-based research groups, including the Center for Human-Compatible AI at the University of California, Berkeley.\nThis follows our January 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support-2/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support-2/</a></td>
    </tr>
    <tr>
      <td>Catherine Brewer — OxAI Safety Hub</td>
      <td>OxAI Safety Hub</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$11,622</td>
      <td>October 2022</td>
      <td>Open Philanthropy recommended a grant of £10,540 (approximately $11,622 at the time of conversion) to Catherine Brewer to support the OxAI Safety Hub, which is a new Oxford-based group working on building the AI safety community.\nThis falls within our focus area of growing and empowering the EA community.</td>
      <td><a href="https://www.openphilanthropy.org/grants/catherine-brewer-oxai-safety-hub/">https://www.openphilanthropy.org/grants/catherine-brewer-oxai-safety-hub/</a></td>
    </tr>
    <tr>
      <td>Thomas Liao — Foundation Model Tracker</td>
      <td>Foundation Model Tracker</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$15,000</td>
      <td>October 2022</td>
      <td>Open Philanthropy recommended a grant of $15,000 to Thomas Liao to support his work on maintaining Foundation Model Tracker, a website that tracks the release of large AI models.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/thomas-liao-foundation-model-tracker/">https://www.openphilanthropy.org/grants/thomas-liao-foundation-model-tracker/</a></td>
    </tr>
    <tr>
      <td>Conjecture — SERI MATS Program in London</td>
      <td>Conjecture</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$457,380</td>
      <td>October 2022</td>
      <td>Open Philanthropy recommended a grant of $457,380 to Conjecture to support their collaboration with the Stanford Existential Risks Initiative (SERI) on SERI’s Machine Learning Alignment Theory Scholars (MATS) program.\nMATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment. This grant will support a London-based extension for a MATS cohort that started in Berkeley. Conjecture will use this funding to provide office space in London and operations support.\nThis follows our April 2022 support for the MATS program and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/conjecture-seri-mats-program-in-london/">https://www.openphilanthropy.org/grants/conjecture-seri-mats-program-in-london/</a></td>
    </tr>
    <tr>
      <td>Centre for the Governance of AI — Research Assistant</td>
      <td>Centre for the Governance of AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$19,200</td>
      <td>September 2022</td>
      <td>Open Philanthropy recommended a grant of $19,200 to the Centre for the Governance of AI to support a new research assistant.\nThis follows our December 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-research-assistant/">https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-research-assistant/</a></td>
    </tr>
    <tr>
      <td>AI Alignment Awards — Shutdown Problem Contest</td>
      <td>AI Alignment Awards</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$75,000</td>
      <td>September 2022</td>
      <td>Open Philanthropy recommended a grant of $75,000 over 1.5 years to AI Alignment Awards to support a contest asking participants to share ideas on how AI systems can be designed or trained to avoid the shutdown problem.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in January 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-alignment-awards-shutdown-problem-contest/">https://www.openphilanthropy.org/grants/ai-alignment-awards-shutdown-problem-contest/</a></td>
    </tr>
    <tr>
      <td>Centre for the Governance of AI — Compute Strategy Workshop</td>
      <td>Centre for the Governance of AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$50,532</td>
      <td>September 2022</td>
      <td>Open Philanthropy recommended a grant of $50,532 to the Centre for the Governance of AI to support a workshop bringing together compute experts from several subfields, such as large-model infrastructure, ASIC design, and governance, to discuss compute governance ideas that could reduce existential risk from artificial intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-compute-strategy-workshop/">https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-compute-strategy-workshop/</a></td>
    </tr>
    <tr>
      <td>AI Safety Hub — Startup Costs</td>
      <td>AI Safety Hub</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$203,959</td>
      <td>September 2022</td>
      <td>Open Philanthropy recommended two grants totaling $203,959 to the AI Safety Hub to support their initial development costs, and to hire several contractors to work on projects related to AI safety. The AI Safety Hub, directed by Century Fellow Julia Karbing, is a new organization that will work on movement building in the AI safety field.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-hub-startup-costs/">https://www.openphilanthropy.org/grants/ai-safety-hub-startup-costs/</a></td>
    </tr>
    <tr>
      <td>Centre for Effective Altruism — Harvard AI Safety Office</td>
      <td>Centre for Effective Altruism</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$250,000</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $250,000 to the Centre for Effective Altruism to rent and refurbish a temporary office space for one year for the Harvard AI Safety Team.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-effective-altruism-harvard-ai-safety-office/">https://www.openphilanthropy.org/grants/centre-for-effective-altruism-harvard-ai-safety-office/</a></td>
    </tr>
    <tr>
      <td>Fund for Alignment Research — Language Model Misalignment (2022)</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$463,693</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $463,693 over 18 months to the Fund for Alignment Research to support research projects, led by Ethan Perez, related to misalignment in language models.\nThis follows our October 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/fund-for-alignment-research-language-model-misalignment-2022/">https://www.openphilanthropy.org/grants/fund-for-alignment-research-language-model-misalignment-2022/</a></td>
    </tr>
    <tr>
      <td>Arizona State University — Adversarial Robustness Research</td>
      <td>Arizona State University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$200,000</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $200,000 over three years to Arizona State University to support research led by Professor Chaowei Xiao on adversarial robustness in machine learning systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nUpdate (March 2024): Dr. Xiao recently transferred to the University of Wisconsin-Madison, where he will continue his research. The remaining funds for this grant ($180,707) were transferred to UW-Madison.</td>
      <td><a href="https://www.openphilanthropy.org/grants/arizona-state-university-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/arizona-state-university-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Redwood Research — General Support (2022)</td>
      <td>Redwood Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$10,700,000</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $10,700,000 over 18 months to Redwood Research for general support. Redwood Research is a nonprofit research institution focused on aligning advanced AI with human interests.\nThis follows our 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/redwood-research-general-support-2/">https://www.openphilanthropy.org/grants/redwood-research-general-support-2/</a></td>
    </tr>
    <tr>
      <td>Daniel Dewey — AI Alignment Projects (2022)</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$175,000</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $175,000 to Daniel Dewey, formerly Open Philanthropy’s program officer for potential risks from advanced artificial intelligence, to support his work on AI alignment. Daniel will continue work on a website explaining how artificial intelligence poses a global risk, and continue work on proposals for experiments related to AI safety.\nThis follows our May 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-projects-2022/">https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-projects-2022/</a></td>
    </tr>
    <tr>
      <td>Center for a New American Security — Work on AI Governance</td>
      <td>Center for a New American Security</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,149,398</td>
      <td>July 2022</td>
      <td>Open Philanthropy recommended a grant of $5,149,398 over three years to the Center for a New American Security to support work related to artificial intelligence policy and governance. \nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in April 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-a-new-american-security-work-on-ai-governance/">https://www.openphilanthropy.org/grants/center-for-a-new-american-security-work-on-ai-governance/</a></td>
    </tr>
    <tr>
      <td>Carnegie Mellon University — Research on Adversarial Examples</td>
      <td>Carnegie Mellon University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$343,235</td>
      <td>July 2022</td>
      <td>Open Philanthropy recommended a grant of $343,235 over three years to Carnegie Mellon University to support research led by Professor Aditi Raghunathan on adversarial examples (inputs optimized to cause machine learning models to make mistakes).\nThis follows our August 2021 support for Professor Raghunathan’s research, and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/carnegie-mellon-university-research-on-adversarial-examples/">https://www.openphilanthropy.org/grants/carnegie-mellon-university-research-on-adversarial-examples/</a></td>
    </tr>
    <tr>
      <td>Stanford University — AI Alignment Research (Barrett and Viteri)</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$153,820</td>
      <td>July 2022</td>
      <td>Open Philanthropy recommended a grant of $153,820 to Stanford University to support research on AI alignment by Professor Clark Barrett and Stanford student Scott Viteri.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-barrett-and-viteri/">https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-barrett-and-viteri/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Language Model Alignment Research</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$40,000</td>
      <td>June 2022</td>
      <td>Open Philanthropy recommended a grant of $40,000 over three years to the Berkeley Existential Risk Initiative to support a project led by Professor Samuel Bowman of New York University to develop a dataset and accompanying methods for language model alignment research.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in April 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-language-model-alignment-research/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-language-model-alignment-research/</a></td>
    </tr>
    <tr>
      <td>Epoch — General Support</td>
      <td>Epoch</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,960,000</td>
      <td>June 2022</td>
      <td>Open Philanthropy recommended a grant of $1,960,000 to Epoch for general support. Epoch is a research organization that works on investigating trends in machine learning and forecasting the development of transformative artificial intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/epoch-general-support/">https://www.openphilanthropy.org/grants/epoch-general-support/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — General Support</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$364,893</td>
      <td>June 2022</td>
      <td>Open Philanthropy recommended a grant of $364,893 to AI Impacts for general support. AI Impacts works on strategic questions related to advanced artificial intelligence.\nThis follows our November 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-general-support/">https://www.openphilanthropy.org/grants/ai-impacts-general-support/</a></td>
    </tr>
    <tr>
      <td>OpenMined — Research on Privacy-Enhancing Technologies and AI Safety</td>
      <td>OpenMined</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$28,320</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a grant of $28,320 to OpenMined to support research on the intersection between privacy-enhancing technologies and technical infrastructure for AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/openmined-research-on-privacy-enhancing-technologies-and-ai-safety/">https://www.openphilanthropy.org/grants/openmined-research-on-privacy-enhancing-technologies-and-ai-safety/</a></td>
    </tr>
    <tr>
      <td>Open Phil AI Fellowship — 2022 Class</td>
      <td>Open Phil AI Fellowship</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,840,000</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a total of approximately $1,840,000 over five years in PhD fellowship support to eleven promising machine learning researchers that together represent the 2022 class of the Open Phil AI Fellowship. This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. This number may be updated as costs are finalized.\nThese fellows were selected for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. This falls within our focus area of potential risks from advanced artificial intelligence.\nWe believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutions; while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI.\nThe intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence.\nFellows with an asterisk (*) next to their names are also Vitalik Buterin Postdoctoral Fellows — winners of grants from the Future of Life Institute (FLI). Open Philanthropy and FLI split funding equally for those fellows.\nThe 2022 Class of Open Phil AI Fellows\nAdam Gleave\n\nAdam is a fifth-year PhD candidate in Computer Science at UC Berkeley. His research focuses on out-of-distribution robustness for deep RL, with a particular emphasis on value learning and multi-agent adversarial robustness. To validate learned reward functions, Adam has developed the EPIC distance and interpretability methods. Adam has demonstrated the existence of adversarial policies in multi-agent environments — policies that cause an opponent to fail despite behaving seemingly randomly — and is currently investigating if narrowly superhuman policies have similar failure modes. For more information, see his website.\nCassidy Laidlaw\n\nCassidy is a PhD student in computer science at UC Berkeley, advised by Stuart Russell and Anca Dragan. He is interested in developing scalable and robust methods for aligning AI systems with human values. His current work focuses on modeling uncertainty in reward learning, scaling methods for human-AI interaction, and adversarial robustness to unforeseen attacks. Prior to his PhD, Cassidy received bachelor’s degrees in computer science and mathematics from the University of Maryland, College Park. For more information, see his website.\nNote: Cassidy is deferring his fellowship until 2023\nCynthia Chen*\n\nCynthia is an incoming PhD student at ETH Zurich, supervised by Prof. Andreas Krause. She is broadly interested in building AI systems that are aligned with human preferences, especially under situations where mistakes are costly and human signals are sparse. She aspires to develop AI solutions that can improve the world in the long run. Prior to ETH, Cynthia interned at the Center for Human-Compatible AI at UC Berkeley and graduated with honours from the University of Hong Kong. You can find out more about Cynthia’s research at her website.\nDaniel Kunin\n\nDaniel is a PhD student in the Institute for Computational and Mathematical Engineering at Stanford, co-advised by Surya Ganguli and Daniel Yamins. His research uses tools from physics to build theoretical models for the learning dynamics of neural networks trained with stochastic gradient descent. Thus far, his research has focused on the effect of regularization on the geometry of the loss landscape, identification of symmetry and conservation laws in the learning dynamics, and how ideas from thermodynamics can be used to understand the impact of stochasticity in SGD. His current research focuses on linking the dynamics of SGD to implicit biases impacting the generalization and robustness properties of the trained network. To learn more about his research, visit his Google Scholar page.\nErik Jenner*\n\nErik is an incoming CS PhD student at UC Berkeley. He is interested in developing techniques for aligning AI with human values that could scale to arbitrarily powerful future AI systems. Erik has previously worked on reward learning with the Center for Human-Compatible AI, focusing on interpretability of reward models and on a better theoretical understanding of the structure of reward functions. He received a BSc in physics from the University of Heidelberg in 2020 and is currently finishing a Master’s in artificial intelligence at the University of Amsterdam. For more information about his research, see his website.\nJohannes Treutlein*\n\nJohannes is an incoming PhD student in Computer Science at UC Berkeley. He is broadly interested in empirical and theoretical research to ensure that AI systems remain safe and reliable with increasing capabilities. He is currently working on investigating learned optimization in machine learning models and on developing models whose objectives generalize robustly out of distribution. Previously, Johannes studied computer science and mathematics at the University of Toronto, the University of Oxford, and the Technical University of Berlin. For more information, visit his website.\nLauro Langosco\n\nLauro is a PhD student with David Krueger at the University of Cambridge. His primary research interest is AI alignment: the problem of building generally intelligent systems that do what their operator wants them to do. He also investigates the science and theory of deep learning, that is the study of how DL systems generalize and scale. Previously, Lauro interned at the Center for Human-Compatible AI in Berkeley and studied mathematics at ETH Zurich.\nMaksym Andriushchenko\n\nMaksym is a PhD student in Computer Science at École Polytechnique Fédérale de Lausanne (EPFL), advised by Nicolas Flammarion. His research focuses on making machine learning algorithms adversarially robust and improving their reliability. He is also interested in developing a better understanding of generalization in deep learning, including generalization under distribution shifts. He has co-developed RobustBench, an ongoing standardized robustness benchmark, and his research on robust image fingerprinting models has been applied for content authenticity purposes. Prior to EPFL, he worked with Matthias Hein at the University of Tübingen on adversarial robustness. To learn more about his research, visit his scholar page.\nQian Huang\n\nQian is a first-year PhD student in Computer Science at Stanford University, advised by Jure Leskovec and Percy Liang. She is broadly interested in aligning machine reasoning with human reasoning, especially for rationality, interpretability, and extensibility with new knowledge. Currently, she is excited about disentangling general reasoning ability from domain knowledge, particularly through the use of graph neural networks and foundation models. Qian received her B.A. in Computer Science and Mathematics from Cornell University. For more information, see her website.\nUsman Anwar*\n\nUsman is a PhD student at the University of Cambridge, where he is advised by David Krueger. His research interests span Reinforcement Learning, Deep Learning and Cooperative AI.  Usman’s goal in AI research is to develop useful, versatile and human-aligned AI systems that can learn from humans and each other. His research focuses on identifying the factors which make it difficult to develop human-aligned AI systems and developing techniques to work around these factors. In particular, he is interested in exploring ways through which rich human preferences and desires may be adaptively communicated to the AI agents, especially in complex scenarios such as multi-agent planning and time-varying preferences with the ultimate goal of both broadening the scope of tasks that AI agents can undertake as well as making the AI agents more aligned and trustworthy. For publications and other details, please visit https://uzman-anwar.github.io\nZhijing Jin*\n\nZhijing is a PhD student in Computer Science at Max Planck Institute, Germany, and ETH Zürich, Switzerland. She is co-supervised by Prof Bernhard Schoelkopf, Rada Mihalcea, Mrinmaya Sachan and Ryan Cotterell. She is broadly interested in making natural language processing (NLP) systems better serve for humanities. Specifically, she uses causal inference to improve the robustness and explainability of language models (as part of the “inner alignment” goal), and make language models align with human values (as part of the “outer alignment” goal). Previously, Zhijing received her bachelor’s degree at the University of Hong Kong, during which she had visiting semesters at MIT and National Taiwan University. She was also a research intern at Amazon AI with Prof Zheng Zhang. For more information, see her website.</td>
      <td><a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/">https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2022-class/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — AI Standards (2022)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$210,000</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a grant of $210,000 to the Berkeley Existential Risk Initiative to support work on the development and implementation of AI safety standards that may reduce potential risks from advanced artificial intelligence. An additional grant to the Center for Long-Term Cybersecurity will support related work.\n\nThis follows our July 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards-2022/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards-2022/</a></td>
    </tr>
    <tr>
      <td>Center for Long-Term Cybersecurity — AI Standards (2022)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$20,000</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a gift of $20,000 to the Center for Long-Term Cybersecurity (CLTC), via UC Berkeley, to support work by CLTC’s AI Security Initiative on the development and implementation of AI standards. An additional grant to the Berkeley Existential Risk Initiative will support related work.\nThis follows our July 2021 grant and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards-2022/">https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards-2022/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — SERI MATS Program (2022)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,008,127</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended three grants totaling $1,008,127 to the Berkeley Existential Risk Initiative to support its collaboration with the Stanford Existential Risks Initiative (SERI) on the second cohort of the SERI Machine Learning Alignment Theory Scholars (MATS) Program. MATS is an educational seminar and independent research program that aims to provide talented scholars with talks, workshops, and research mentorship in the field of AI alignment, and connect them with the Berkeley alignment research community.\nThis follows our November 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in January 2023.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program/</a></td>
    </tr>
    <tr>
      <td>Funding for AI Alignment Projects Working With Deep Learning Systems</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$16,306,737</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a total of $16,306,737 in funding for projects working with deep learning systems that could help us understand and make progress on AI alignment. We sought applications for this funding here.\nRecipients include (organized by research direction, with institution name in parentheses):\nInterpretability:\n\nDavid Bau (Northeastern University)\nEugene Belilovsky (Concordia University)\nRuth Fong (Princeton)\nSurya Ganguli (Stanford University)\nRoger Grosse (University of Toronto)\nYonatan Belinkov (Technion – Israel Institute of Technology)\n\nMeasuring and forecasting risks:\n\nDavid McAllester (Toyota Technological Institute at Chicago)\nMichael Wellman (University of Michigan)\n\nTechniques for enhancing human feedback:\n\nYoav Artzi (Cornell University)\nSamuel Bowman (New York University)\nGreg Durrett (University of Texas at Austin)\nFaramarz Fekri (Georgia Institute of Technology)\nDaniel Kang (University of Illinois)\nMohit Iyyer (University of Massachusetts, Amherst)\nVictor Veitch (University of Chicago)\n\nTruthful and honest AI:\n\nDavid Blei (Columbia University)\nPeter Clark (Allen Institute of AI)\nDylan Hadfield-Menell (Massachusetts Institute of Technology)\nTatsunori Hashimo (Stanford University)\nHe He (New York University)\nDan Klein (University of California, Berkeley)\nDavid Krueger (University of Cambridge)\nColin Raffel (University of North Carolina, Chapel Hill)\n\nOther:\n\nChelsea Finn (Stanford University)\n\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in August 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/funding-for-ai-alignment-projects-working-with-deep-learning-systems/">https://www.openphilanthropy.org/grants/funding-for-ai-alignment-projects-working-with-deep-learning-systems/</a></td>
    </tr>
    <tr>
      <td>AI Safety Support — Research on Trends in Machine Learning</td>
      <td>AI Safety Support</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$42,000</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a grant of $42,000 to AI Safety Support to scale up a research group, led by Jaime Sevilla, which studies trends in machine learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-safety-support-research-on-trends-in-machine-learning/">https://www.openphilanthropy.org/grants/ai-safety-support-research-on-trends-in-machine-learning/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — David Krueger Collaboration</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$140,050</td>
      <td>April 2022</td>
      <td>Open Philanthropy recommended a grant of $140,050 to the Berkeley Existential Risk Initiative to support its collaboration with Professor David Krueger.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in August 2023.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-david-krueger-collaboration/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-david-krueger-collaboration/</a></td>
    </tr>
    <tr>
      <td>Carnegie Endowment for International Peace — AI Governance Research</td>
      <td>Carnegie Endowment for International Peace</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$597,717</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of $597,717 over two years to the Carnegie Endowment for International Peace to support a research project on international AI governance led by Matt Sheehan.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/carnegie-endowment-for-international-peace-ai-governance-research/">https://www.openphilanthropy.org/grants/carnegie-endowment-for-international-peace-ai-governance-research/</a></td>
    </tr>
    <tr>
      <td>Hofvarpnir Studios — Compute Cluster for AI Safety Research</td>
      <td>Hofvarpnir Studios</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,443,540</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of $1,443,540 over three years to Hofvarpnir Studios to create and maintain a compute cluster for Jacob Steinhardt’s lab that will also be used by researchers at the Center for Human-Compatible Artificial Intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/hofvarpnir-studios-compute-cluster-for-ai-safety-research/">https://www.openphilanthropy.org/grants/hofvarpnir-studios-compute-cluster-for-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>Usman Anwar — Research Collaboration with David Krueger</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,526</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of £5,000 (approximately $6,526 at the time of conversion) to Usman Anwar to support his research on machine learning in collaboration with Professor David Krueger\nThis follows our April 2021 support for Professor Krueger’s research and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/usman-anwar-research-collaboration-with-david-krueger/">https://www.openphilanthropy.org/grants/usman-anwar-research-collaboration-with-david-krueger/</a></td>
    </tr>
    <tr>
      <td>Massachusetts Institute of Technology — AI Trends and Impacts Research (2022)</td>
      <td>Massachusetts Institute of Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$13,277,348</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of $13,277,348 over four years to the Massachusetts Institute of Technology (MIT) to support research led by Neil Thompson on modeling the trends and impacts of AI and computing. Thompson will use this funding to hire new staff and expand his lab work.\nThis follows our November 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2022/">https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research-2022/</a></td>
    </tr>
    <tr>
      <td>Rethink Priorities — AI Governance Research (2022)</td>
      <td>Rethink Priorities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,728,319</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of $2,728,319 over two years to Rethink Priorities to expand its research on topics related to AI governance.\nThis follows our July 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2022/">https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research-2022/</a></td>
    </tr>
    <tr>
      <td>Egor Krasheninnikov — Research Collaboration with David Krueger</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,526</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of £5,000 (approximately $6,526 at the time of conversion) to Egor Krasheninnikov to support his research on machine learning in collaboration with Professor David Krueger\nThis follows our April 2021 support for Professor Krueger’s research and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/egor-krasheninnikov-research-collaboration-with-david-krueger/">https://www.openphilanthropy.org/grants/egor-krasheninnikov-research-collaboration-with-david-krueger/</a></td>
    </tr>
    <tr>
      <td>Stiftung Neue Verantwortung — AI Policy Analysis</td>
      <td>Stiftung Neue Verantwortung</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$444,000</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of €390,528 (approximately $444,000 at the time of conversion) to Stiftung Neue Verantwortung to support data-driven reports on AI-related talent flows and the global microchip supply chain.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stiftung-neue-verantwortung-ai-policy-analysis/">https://www.openphilanthropy.org/grants/stiftung-neue-verantwortung-ai-policy-analysis/</a></td>
    </tr>
    <tr>
      <td>Alignment Research Center — General Support</td>
      <td>Alignment Research Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$265,000</td>
      <td>March 2022</td>
      <td>Open Philanthropy recommended a grant of $265,000 to the Alignment Research Center (ARC) for general support. ARC focuses on developing strategies for AI alignment that can be adopted by industry today and scaled to future machine learning systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/alignment-research-center-general-support/">https://www.openphilanthropy.org/grants/alignment-research-center-general-support/</a></td>
    </tr>
    <tr>
      <td>AI Funding for Individuals — Work and Study Support</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$113,372</td>
      <td>February 2022</td>
      <td>Open Philanthropy recommended a total of $113,372 to support individuals pursuing work and study related to potential risks from advanced artificial intelligence.\nThe grant amount was updated in March 2023.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-funding-for-individuals-work-and-study-support/">https://www.openphilanthropy.org/grants/ai-funding-for-individuals-work-and-study-support/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — CHAI Collaboration (2022)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,126,160</td>
      <td>February 2022</td>
      <td>Open Philanthropy recommended a grant of $1,126,160 to the Berkeley Existential Risk Initiative (BERI) to support continued work with the Center for Human-Compatible AI (CHAI) at UC Berkeley. BERI will use the funding to facilitate the creation of an in-house compute cluster for CHAI’s use, purchase compute resources, and hire a part-time system administrator to help manage the cluster.\nThis follows our November 2019 support and falls within our focus area of potential risks from artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2022/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2022/</a></td>
    </tr>
    <tr>
      <td>Michael Page — Career Transition Grant</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$52,500</td>
      <td>February 2022</td>
      <td>Open Philanthropy recommended a grant of $52,500 to Michael Page to work on several short-term projects while he explores different career options. Page recently finished his tenure as a Research Fellow at the Center for Security and Emerging Technology, and we believe that his expertise on forecasting and AI policy makes him an exceptionally strong candidate for an impactful career.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/michael-page-career-transition-grant/">https://www.openphilanthropy.org/grants/michael-page-career-transition-grant/</a></td>
    </tr>
    <tr>
      <td>National Academies of Sciences, Engineering, and Medicine — Safety-Critical Machine Learning</td>
      <td>National Academies of Sciences, Engineering, and Medicine</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$309,441</td>
      <td>February 2022</td>
      <td>Open Philanthropy recommended a grant of $309,441 to the National Academies of Sciences, Engineering, and Medicine to support research on machine learning in safety-critical environments.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/national-academies-of-sciences-engineering-and-medicine-safety-critical-machine-learning/">https://www.openphilanthropy.org/grants/national-academies-of-sciences-engineering-and-medicine-safety-critical-machine-learning/</a></td>
    </tr>
    <tr>
      <td>Wilson Center — AI Policy Training Program (2022)</td>
      <td>The Wilson Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,023,322</td>
      <td>January 2022</td>
      <td>Open Philanthropy recommended a grant of $2,023,322 over two years to the Wilson Center to support their AI policy training program, which is aimed at staffers for members of Congress and other policymakers. The program’s ultimate goal is to increase policymakers’ access to technical AI expertise.\nThis follows our April 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/wilson-center-ai-policy-training-program-2022/">https://www.openphilanthropy.org/grants/wilson-center-ai-policy-training-program-2022/</a></td>
    </tr>
    <tr>
      <td>Centre for the Governance of AI — AI Field Building</td>
      <td>Centre for the Governance of AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,537,600</td>
      <td>December 2021</td>
      <td>Open Philanthropy recommended a grant of $2,537,600 over two years to the Centre for the Governance of AI (GovAI), via the Centre for Effective Altruism, to support activities related to building the field of AI governance research. GovAI intends to use this funding to conduct AI governance research and to develop a talent pipeline for those interested in entering the field.\nThis follows our May 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-ai-field-building/">https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-ai-field-building/</a></td>
    </tr>
    <tr>
      <td>Georgetown University — Policy Fellowship (2021)</td>
      <td>Georgetown University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$246,564</td>
      <td>December 2021</td>
      <td>Open Philanthropy recommended a grant of $246,564 to Georgetown University to support a fellowship related to AI and cybersecurity policy.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2021/">https://www.openphilanthropy.org/grants/georgetown-university-policy-fellowship-2021/</a></td>
    </tr>
    <tr>
      <td>Redwood Research — General Support</td>
      <td>Redwood Research</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$9,420,000</td>
      <td>November 2021</td>
      <td>Image courtesy of Redwood Research\n \n \n\n\n\nGrant Investigator: Nick Beckstead\n\nThis page was reviewed but not written by the grant investigator. Redwood Research staff also reviewed this page prior to publication.\n\n\n\n\n\n\n\nOpen Philanthropy recommended four grants totaling $9,420,000 to Redwood Research for general support. Redwood Research is a new research institution that conducts research to better understand and make progress on AI alignment in order to reduce global catastrophic risks.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/redwood-research-general-support/">https://www.openphilanthropy.org/grants/redwood-research-general-support/</a></td>
    </tr>
    <tr>
      <td>Stanford University — AI Alignment Research (2021)</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,500,000</td>
      <td>November 2021</td>
      <td>Open Philanthropy recommended a grant of $1,500,000 over three years to Stanford University to support research led by Professor Percy Liang on AI safety and alignment. We hope this funding will accelerate progress on technical problems and help to build a pipeline for younger researchers to work on AI alignment.\nThis follows our 2017 support for Professor Liang’s research and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-2021/">https://www.openphilanthropy.org/grants/stanford-university-ai-alignment-research-2021/</a></td>
    </tr>
    <tr>
      <td>Mila — Research Project on Artificial Intelligence</td>
      <td>Montreal Institute for Learning Algorithms</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$237,931</td>
      <td>November 2021</td>
      <td>Image courtesy of MILA staff.\n \nGrant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. MILA staff also reviewed this page prior to publication.\n\n\n\n\n\n\n\nOpen Philanthropy recommended a grant of CAN$295,900 ($237,931 at the time of conversion) to Mila (previously, the Montreal Institute for Learning Algorithms) to support a research project investigating AI consciousness and moral patienthood. The research will be conducted in collaboration with the Université de Montréal and the Future of Humanity Institute. This funding will support postdoctoral researchers and students studying the topic, as well as publications and workshops.\nThis follows our July 2017 support for the Montreal Institute for Learning Algorithms and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/mila-research-project-on-artificial-intelligence/">https://www.openphilanthropy.org/grants/mila-research-project-on-artificial-intelligence/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — SERI MATS Program</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$195,000</td>
      <td>November 2021</td>
      <td>Open Philanthropy recommended three grants totaling $195,000 to the Berkeley Existential Risk Initiative to support its collaboration with the Stanford Existential Risks Initiative (SERI) on the SERI ML Alignment Theory Scholars (MATS) Program. MATS is a two-month program where students will research problems related to AI alignment while supervised by a mentor.\nThis follows our May 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program-2/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-mats-program-2/</a></td>
    </tr>
    <tr>
      <td>Fund for Alignment Research — Language Model Misalignment</td>
      <td>FAR AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$425,800</td>
      <td>October 2021</td>
      <td>Open Philanthropy recommended a grant of $425,800 to the Fund for Alignment Research, led by Ethan Perez, to support salaries and equipment for projects related to misalignment in language models. Perez plans to hire and supervise four engineers to work on these projects.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/language-model-safety-fund-language-model-misalignment/">https://www.openphilanthropy.org/grants/language-model-safety-fund-language-model-misalignment/</a></td>
    </tr>
    <tr>
      <td>University of Washington — Adversarial Robustness Research</td>
      <td>University of Washington</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$730,000</td>
      <td>October 2021</td>
      <td>Open Philanthropy recommended a grant of $730,000 over three years to the University of Washington to support early-career research by Ludwig Schmidt on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-washington-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/university-of-washington-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Stanford University — AI Index</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$78,000</td>
      <td>September 2021</td>
      <td>Open Philanthropy recommended a grant of $78,000 to Stanford University to support the AI Index, which collects and reports data related to artificial intelligence, including data relevant to AI safety and AI ethics.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-ai-index/">https://www.openphilanthropy.org/grants/stanford-university-ai-index/</a></td>
    </tr>
    <tr>
      <td>Université de Montréal — Research Project on Artificial Intelligence</td>
      <td>Université de Montréal</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$210,552</td>
      <td>September 2021</td>
      <td>Open Philanthropy recommended a grant of CAN$266,200 ($210,552 at the time of conversion) to the Université de Montréal to support a research project investigating AI consciousness and moral patienthood. The research will be conducted in collaboration with Mila and the Future of Humanity Institute. This funding will support post-docs and students studying the topic, as well as publications and workshops.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/universite-de-montreal-research-project-on-artificial-intelligence/">https://www.openphilanthropy.org/grants/universite-de-montreal-research-project-on-artificial-intelligence/</a></td>
    </tr>
    <tr>
      <td>Center for a New American Security — Risks from Militarized AI</td>
      <td>Center for a New American Security</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$101,187</td>
      <td>September 2021</td>
      <td>Open Philanthropy recommended a grant of $101,187 over 18 months to the Center for a New American Security (CNAS) to support a working group that will focus on mitigating risks from possible military applications of artificial intelligence. This group will be composed of technical and policy experts from the US, Russia, China, and Europe, and will investigate possible confidence-building measures (actions designed to prevent miscalculation and conflict between states) for militarized AI.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-a-new-american-security-risks-from-militarized-ai/">https://www.openphilanthropy.org/grants/center-for-a-new-american-security-risks-from-militarized-ai/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Adversarial Robustness Research (Aditi Raghunathan)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$87,829</td>
      <td>August 2021</td>
      <td>Open Philanthropy recommended a grant of $87,829 to UC Berkeley to support postdoctoral research by Aditi Raghunathan on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in July 2023.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-aditi-raghunathan/">https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-aditi-raghunathan/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Adversarial Robustness Research (Dimitris Tsipras)</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$330,792</td>
      <td>August 2021</td>
      <td>Dr. Dimitris Tsipras. (Photo courtesy of Dimitris Tsipras.)\n\nGrant Investigator: Catherine Olsson\nThis page was reviewed but not written by the grant investigators. Stanford University staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $330,792 over three years to Stanford University to support early-career research by Dimitris Tsipras on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-adversarial-robustness-research-dimitris-tsipras/">https://www.openphilanthropy.org/grants/stanford-university-adversarial-robustness-research-dimitris-tsipras/</a></td>
    </tr>
    <tr>
      <td>University of Southern California — Adversarial Robustness Research</td>
      <td>University of Southern California</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$320,000</td>
      <td>August 2021</td>
      <td>Open Philanthropy recommended a grant of $320,000 over three years to the University of Southern California to support early-career research by Robin Jia on adversarial robustness and out-of-distribution generalization as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-southern-california-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/university-of-southern-california-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Adversarial Robustness Research (Shibani Santurkar)</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$330,792</td>
      <td>August 2021</td>
      <td>Dr. Shibani Santurkar. (Photo courtesy of Shibani Santurkar.)\n\nGrant Investigator: Catherine Olsson\nThis page was reviewed but not written by the grant investigators. Stanford University staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $330,792 over three years to Stanford University to support early-career research by Shibani Santurkar on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-adversarial-robustness-research-shibani-santurkar/">https://www.openphilanthropy.org/grants/stanford-university-adversarial-robustness-research-shibani-santurkar/</a></td>
    </tr>
    <tr>
      <td>Center for Security and Emerging Technology — General Support (August 2021)</td>
      <td>Center for Security and Emerging Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$38,920,000</td>
      <td>August 2021</td>
      <td>The CSET team (image courtesy of CSET).\n\nGrant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for Security and Emerging Technology staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $38,920,000 over three years to the Center for Security and Emerging Technology, via Georgetown University, for general support. CSET is a think tank, incubated by our January 2019 support, dedicated to policy analysis at the intersection of national and international security and emerging technologies. This funding is intended to augment our original support for CSET, particularly for its work on security and artificial intelligence.\nThis follows our January 2021 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-security-and-emerging-technology-general-support-august-2021/">https://www.openphilanthropy.org/grants/center-for-security-and-emerging-technology-general-support-august-2021/</a></td>
    </tr>
    <tr>
      <td>Center for Long-Term Cybersecurity — AI Standards (2021)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$25,000</td>
      <td>July 2021</td>
      <td>Open Philanthropy recommended a gift of $25,000 to the Center for Long-Term Cybersecurity (CLTC), via UC Berkeley, to support work by CLTC’s AI Security Initiative on the development and implementation of AI standards. An additional grant to the Berkeley Existential Risk Initiative will support related work.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards/">https://www.openphilanthropy.org/grants/center-for-long-term-cybersecurity-ai-standards/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — AI Standards (2021)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$300,000</td>
      <td>July 2021</td>
      <td>Open Philanthropy recommended a grant of $300,000 to the Berkeley Existential Risk Initiative to support work on the development and implementation of AI safety standards that may reduce potential risks from advanced artificial intelligence. An additional grant to the Center for Long-Term Cybersecurity will support related work.\nThis follows our January 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-ai-standards/</a></td>
    </tr>
    <tr>
      <td>Center for International Security and Cooperation — AI and Strategic Stability</td>
      <td>Center for International Security and Cooperation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$365,361</td>
      <td>July 2021</td>
      <td>Open Philanthropy recommended a grant of $365,361 to the Center for International Security and Cooperation (CISAC) to support work studying hypothetical scenarios related to AI and strategic stability.\nThis follows our September 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-international-security-and-cooperation-ai-and-strategic-stability/">https://www.openphilanthropy.org/grants/center-for-international-security-and-cooperation-ai-and-strategic-stability/</a></td>
    </tr>
    <tr>
      <td>Applied Research Laboratory for Intelligence and Security — Report on Security Clearances</td>
      <td>Applied Research Laboratory for Intelligence and Security</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>July 2021</td>
      <td>Open Philanthropy recommended a grant of $70,000 to the Applied Research Laboratory for Intelligence and Security to conduct a study on US security clearance reform options.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/applied-research-laboratory-for-intelligence-and-security-report-on-security-clearances/">https://www.openphilanthropy.org/grants/applied-research-laboratory-for-intelligence-and-security-report-on-security-clearances/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — MineRL BASALT Competition</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$70,000</td>
      <td>July 2021</td>
      <td>Open Philanthropy recommended a grant of $70,000 to the Berkeley Existential Risk Initiative to support the MineRL BASALT competition. The competition asks participants to build AI systems that learn from human feedback within the Minecraft video game, with the intent that the competition will spur more interest in learning from human feedback, using feedback efficiently, and doing so in complex environments.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-minerl-basalt-competition/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-minerl-basalt-competition/</a></td>
    </tr>
    <tr>
      <td>Rethink Priorities — AI Governance Research (2021)</td>
      <td>Rethink Priorities</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$612,185</td>
      <td>July 2021</td>
      <td>Grant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Rethink Priorities staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a contract of $612,185 with Rethink Priorities to support research projects on topics related to AI governance. We believe that Rethink Priorities’ research outputs may help inform our AI policy grantmaking strategy.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThis project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.\nThe grant amount was updated in March 2022.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research/">https://www.openphilanthropy.org/grants/rethink-priorities-ai-governance-research/</a></td>
    </tr>
    <tr>
      <td>Carnegie Mellon University — Adversarial Robustness Research</td>
      <td>Carnegie Mellon University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$330,000</td>
      <td>May 2021</td>
      <td>Grant Investigator: Catherine Olsson\n\nThis page was reviewed but not written by the grant investigators. Carnegie Mellon staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $330,000 over three years to Carnegie Mellon University to support research by Professor Zico Kolter on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/carnegie-mellon-university-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/carnegie-mellon-university-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Jennifer Dodd — Machine Learning Self-Study</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$19,200</td>
      <td>May 2021</td>
      <td>Open Philanthropy recommended a grant of $19,200 to Jennifer Dodd to support four months of self-study in machine learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/jennifer-dodd-machine-learning-self-study/">https://www.openphilanthropy.org/grants/jennifer-dodd-machine-learning-self-study/</a></td>
    </tr>
    <tr>
      <td>Daniel Dewey — AI Alignment Project</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$175,000</td>
      <td>May 2021</td>
      <td>Grant Investigator: Nick Beckstead\n\nThis page was reviewed but not written by the grant investigators. Daniel Dewey also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $175,000 to Daniel Dewey, formerly Open Philanthropy’s program officer for potential risks from advanced artificial intelligence, to support his work on an AI alignment project and related field-building efforts. Daniel plans to use this funding to produce writing and reports summarizing existing research and investigating potentially valuable projects relevant to AI alignment, with the goal of helping junior researchers and others understand how they can contribute to the field.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-project/">https://www.openphilanthropy.org/grants/daniel-dewey-ai-alignment-project/</a></td>
    </tr>
    <tr>
      <td>Open Phil AI Fellowship — 2021 Class</td>
      <td>Open Phil AI Fellowship</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,000,000</td>
      <td>April 2021</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator.\n\nOpen Philanthropy recommended a total of approximately $1,000,000 over five years in PhD fellowship support to four promising machine learning researchers that together represent the 2021 class of the Open Phil AI Fellowship. This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. This number may be updated as costs are finalized. These fellows were selected from 397 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. This falls within our focus area of potential risks from advanced artificial intelligence.\nWe believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutions; while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI.\nThe intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests.\nThe 2021 Class of Open Phil AI Fellows\nCollin Burns\n\nCollin is an incoming PhD student in Computer Science at UC Berkeley. He is broadly interested in doing foundational work to make AI systems more trustworthy, aligned with human values, and helpful for human decision making. He is especially excited about using language to control and interpret machine learning models more effectively. Collin received his B.A. in Computer Science from Columbia University. For more information, see his website.\nJared Quincy Davis\n\nJared Quincy Davis is a PhD Student in Computer Science at Stanford University. His research asks what progress is necessary for the most compelling advances of machine learning (e.g. those that powered AlphaGo) to be applied more broadly and extensively in real-world, non-stationary, multi-particle domains with nth order dynamics. Jared is motivated by the potential of such AI advances to accelerate the rate of progress in, and adoption of, technology. Thus far, his work has focused on addressing the specific computational complexity, memory, and optimization challenges that arise when learning high resolution representations of the dynamics within complex systems. Jared’s fundamental research has thus far been applied to great effect in problems spanning structural biology, industrial systems control, robotic planning and navigation, natural language processing, and beyond. To learn more about his research, visit his scholar page.\nJesse Mu\n\nJesse is a PhD student in Computer Science at Stanford University, advised by Noah Goodman and affiliated with the Stanford NLP Group and Stanford AI Lab. He is interested in using language and communication to improve the interpretability and generalization of machine learning models, especially in multimodal or embodied settings. Previously, Jesse received an MPhil in Advanced Computer Science from the University of Cambridge as a Churchill scholar, and a BS in Computer Science from Boston College. For more information, see his website.\nMeena Jagadeesan\n\nMeena Jagadeesan is a first-year PhD student at UC Berkeley, advised by Moritz Hardt, Michael I. Jordan, and Jacob Steinhardt. She aims to develop theoretical foundations for machine learning systems that account for economic and societal effects, especially in strategic or dynamic environments. Her work currently focuses on reasoning about the incentives created by decision-making systems and on ensuring fairness in multi-stage systems. Meena completed her Bachelor’s and Master’s degrees at Harvard University in 2020, where she studied computer science, mathematics, and statistics. For more information, visit her website.\nTan Zhi-Xuan\n\nXuan (Sh-YEN) is a PhD student at MIT co-advised by Vikash Mansinghka and Joshua Tenenbaum. Their research sits at the intersection of AI, philosophy, and cognitive science, asking questions like: How can we specify and perform inference over rich yet structured generative models of human motivation and bounded reasoning, in order to accurately infer human goals and values? To answer these questions, Xuan’s work includes the development of probabilistic programming infrastructure, so as to enable fast and flexible Bayesian inference over complex models of agents and their environments. Prior to MIT, Xuan worked with Desmond Ong at the National University of Singapore on deep generative models, and Brian Scassellati at Yale on human-robot interaction. They graduated from Yale with a B.S. in Electrical Engineering & Computer Science.</td>
      <td><a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2021-class/">https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2021-class/</a></td>
    </tr>
    <tr>
      <td>University of Cambridge — Machine Learning Research</td>
      <td>University of Cambridge</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$250,000</td>
      <td>April 2021</td>
      <td>Grant Investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigators. University of Cambridge staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $250,000 over four years to Cambridge in America for the University of Cambridge to support Professor David Krueger’s machine learning research.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-cambridge-machine-learning-research/">https://www.openphilanthropy.org/grants/university-of-cambridge-machine-learning-research/</a></td>
    </tr>
    <tr>
      <td>Wilson Center — AI Policy Training Program</td>
      <td>The Wilson Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$291,214</td>
      <td>April 2021</td>
      <td>Grant Investigator: Luke Muehlhauser\n\nThis page was reviewed but not written by the grant investigator. The Wilson Center Staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $291,214 to the Wilson Center to pilot an AI policy training program. The Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue.\nThis follows our June 2020 support.</td>
      <td><a href="https://www.openphilanthropy.org/grants/wilson-center-ai-policy-training-program/">https://www.openphilanthropy.org/grants/wilson-center-ai-policy-training-program/</a></td>
    </tr>
    <tr>
      <td>International Conference on Learning Representations — Security and Safety in Machine Learning Systems Workshop</td>
      <td>International Conference on Learning Representations</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$3,000</td>
      <td>April 2021</td>
      <td>Open Philanthropy recommended a grant of $3,000 to the International Conference on Learning Representations to support the Security and Safety in Machine Learning Systems workshop. The workshop will bring together experts in machine learning, computer security, and AI safety.\nThis follows our May 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/international-conference-on-learning-representations-security-and-safety-in-machine-learning-systems-workshop/">https://www.openphilanthropy.org/grants/international-conference-on-learning-representations-security-and-safety-in-machine-learning-systems-workshop/</a></td>
    </tr>
    <tr>
      <td>Brian Christian — The Alignment Problem Book Promotion</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$60,859</td>
      <td>March 2021</td>
      <td>Grant Investigator: Nick Beckstead\nThis page was reviewed but not written by the grant investigator. Brian Christian also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a contract of $60,859 with Brian Christian to support the promotion of his book The Alignment Problem: Machine Learning and Human Values. Our team focused on potential risks from advanced artificial intelligence hopes that the book will generate interest in AI alignment among academics and others.\nThis project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.</td>
      <td><a href="https://www.openphilanthropy.org/grants/brian-christian-the-alignment-problem-book-promotion/">https://www.openphilanthropy.org/grants/brian-christian-the-alignment-problem-book-promotion/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative – SERI Summer Fellowships</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$210,000</td>
      <td>March 2021</td>
      <td>(Image courtesy of BERI.)\n \nGrant investigator: Claire Zabel\n\nThis page was reviewed but not written by the grant investigator. BERI staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $210,000 to the Berkeley Existential Risk Initiative (BERI) to provide stipends for the Stanford Existential Risks Initiative (SERI) summer research fellowship program.\nThis follows our January 2020 support for BERI and our January 2021 support for SERI and falls within our work on global catastrophic risks.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-summer-fellowships/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-seri-summer-fellowships/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Adversarial Robustness Research (David Wagner)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$330,000</td>
      <td>February 2021</td>
      <td>Grant Investigator: Catherine Olsson and Daniel Dewey\nThis page was reviewed but not written by the grant investigators. UC Berkeley staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $330,000 over three years to UC Berkeley to support research by Professor David Wagner on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-david-wagner/">https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-david-wagner/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Adversarial Robustness Research (Dawn Song)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$330,000</td>
      <td>February 2021</td>
      <td>Grant Investigator: Catherine Olsson and Daniel Dewey\nThis page was reviewed but not written by the grant investigators. UC Berkeley staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $330,000 over three years to UC Berkeley to support research by Professor Dawn Song on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-dawn-song/">https://www.openphilanthropy.org/grants/uc-berkeley-adversarial-robustness-research-dawn-song/</a></td>
    </tr>
    <tr>
      <td>University of Tübingen — Robustness Research (Wieland Brendel)</td>
      <td>University of Tübingen</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$590,000</td>
      <td>February 2021</td>
      <td>Grant Investigators: Catherine Olsson and Nick Beckstead\nThis page was reviewed but not written by the grant investigators. University of Tübingen staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $590,000 over three years to the University of Tübingen to support early-career research by Wieland Brendel on robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-tubingen-robustness-research-wieland-brendel/">https://www.openphilanthropy.org/grants/university-of-tubingen-robustness-research-wieland-brendel/</a></td>
    </tr>
    <tr>
      <td>University of Tübingen — Adversarial Robustness Research (Matthias Hein)</td>
      <td>University of Tübingen</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$300,000</td>
      <td>February 2021</td>
      <td>Grant Investigator: Catherine Olsson and Daniel Dewey\n\nThis page was reviewed but not written by the grant investigators. University of Tübingen staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $300,000 over three years to the University of Tübingen to support research by Professor Matthias Hein on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-tubingen-adversarial-robustness-research-matthias-hein/">https://www.openphilanthropy.org/grants/university-of-tubingen-adversarial-robustness-research-matthias-hein/</a></td>
    </tr>
    <tr>
      <td>Massachusetts Institute of Technology — Adversarial Robustness Research</td>
      <td>Massachusetts Institute of Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,430,000</td>
      <td>February 2021</td>
      <td>Grant Investigator: Catherine Olsson and Daniel Dewey\nThis page was reviewed but not written by the grant investigators. Massachusetts Institute of Technology staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $1,430,000 over three years to the Massachusetts Institute of Technology to support research by Professor Aleksander Madry on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>Center for Security and Emerging Technology — General Support (January 2021)</td>
      <td>Center for Security and Emerging Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$8,000,000</td>
      <td>January 2021</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for Security and Emerging Technology staff also reviewed this page prior to publication.\n\n\n\n\n\n\n\nOpen Philanthropy recommended a grant of $8,000,000 to the Center for Security and Emerging Technology (CSET), via Georgetown University, for general support. CSET is a new think tank, incubated by our January 2019 support, dedicated to policy analysis at the intersection of national and international security and emerging technologies. It is led by Jason Matheny, former Assistant Director of National Intelligence and Director of Intelligence Advanced Research Projects Activity (IARPA), the U.S. intelligence community’s research organization. This funding is intended to augment our original support for CSET, particularly for its work on the intersection of security and artificial intelligence.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-security-and-emerging-technology-general-support/">https://www.openphilanthropy.org/grants/center-for-security-and-emerging-technology-general-support/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Center for Human-Compatible Artificial Intelligence (2021)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$11,955,246</td>
      <td>January 2021</td>
      <td>Grant investigator: Nick Beckstead\nThis page was reviewed but not written by the grant investigator. UC Berkeley staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $11,955,246 over five years to UC Berkeley to support the Center for Human-Compatible Artificial Intelligence (CHAI). The multi-year commitment and increased funding will enable CHAI to expand its research and student training related to potential risks from advanced artificial intelligence.\nThis is a renewal of our August 2016 grant.\nThe grant amount was updated in April 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-artificial-intelligence-2021/">https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-artificial-intelligence-2021/</a></td>
    </tr>
    <tr>
      <td>Berryville Institute of Machine Learning — Machine Learning Security Research</td>
      <td>Berryville Institute of Machine Learning</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$150,000</td>
      <td>January 2021</td>
      <td>Grant investigators: Catherine Olsson and Daniel Dewey\nThis page was reviewed but not written by the grant investigators. Berryville Institute of Machine Learning staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $150,000 to the Berryville Institute of Machine Learning to support research led by Gary McGraw on machine learning security. The research will focus on building a taxonomy of known attacks on machine learning, exploring a hypothesis of representation and machine learning risk, and performing an architectural risk analysis of machine learning systems. Our potential risks from advanced artificial intelligence team hopes that the research will help advance the field of machine learning security.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berryville-institute-of-machine-learning-machine-learning-security-research/">https://www.openphilanthropy.org/grants/berryville-institute-of-machine-learning-machine-learning-security-research/</a></td>
    </tr>
    <tr>
      <td>UC Santa Cruz — Adversarial Robustness Research</td>
      <td>University of California, Santa Cruz</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$265,000</td>
      <td>January 2021</td>
      <td>Photo of Cihang Xie.\n\nGrant Investigator: Catherine Olsson and Nick Beckstead\nThis page was reviewed but not written by the grant investigators. UC Santa Cruz staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $265,000 over three years to UC Santa Cruz to support early-career research by Cihang Xie on adversarial robustness as a means to improve AI safety.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-santa-cruz-adversarial-robustness-research/">https://www.openphilanthropy.org/grants/uc-santa-cruz-adversarial-robustness-research/</a></td>
    </tr>
    <tr>
      <td>University of Toronto — Machine Learning Research</td>
      <td>University of Toronto</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$520,000</td>
      <td>December 2020</td>
      <td>Photo of Chris Maddison. (Image courtesy of Dan Komoda/Institute for Advanced Study.)\n\nGrant Investigator: Daniel Dewey and Catherine Olsson\nThis page was reviewed but not written by the grant investigators. University of Toronto staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $520,000 over four years to the University of Toronto to support research on understanding, predicting, and controlling machine learning systems, led by Professor Chris Maddison, a former Open Phil AI Fellow. This funding is intended to enable three students and a postdoctoral researcher to work with Professor Maddison on the research.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-toronto-machine-learning-research/">https://www.openphilanthropy.org/grants/university-of-toronto-machine-learning-research/</a></td>
    </tr>
    <tr>
      <td>Massachusetts Institute of Technology — AI Trends and Impacts Research (2020)</td>
      <td>Massachusetts Institute of Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$550,688</td>
      <td>November 2020</td>
      <td>Neil Thompson after giving a talk in Ghana. (Photo courtesy of MIT.)\n\nGrant investigators: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Massachusetts Institute of Technology staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $550,688 to the Massachusetts Institute of Technology to support research led by Neil Thompson on modeling the trends and impacts of AI and computing. The research will consist of projects to learn how algorithmic improvement affects economic growth, gather data on the performance and compute usage of machine learning methods, and estimate cost models for deep learning projects.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in October 2021.</td>
      <td><a href="https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research/">https://www.openphilanthropy.org/grants/massachusetts-institute-of-technology-ai-trends-and-impacts-research/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — General Support (2020)</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$50,000</td>
      <td>November 2020</td>
      <td>Grant Investigator: Tom Davidson and Ajeya Cotra\nThis page was reviewed but not written by the grant investigators. AI Impacts also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $50,000 to AI Impacts, via the Machine Intelligence Research Institute, for general support. AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence.\nThis follows our June 2018 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-general-support-2020/">https://www.openphilanthropy.org/grants/ai-impacts-general-support-2020/</a></td>
    </tr>
    <tr>
      <td>Center for a New American Security — AI and Security Projects</td>
      <td>Center for a New American Security</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$116,744</td>
      <td>October 2020</td>
      <td>Grant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for a New American Security staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $116,744 to the Center for a New American Security to support work by Paul Scharre on projects related to AI and security.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-and-security-projects/">https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-and-security-projects/</a></td>
    </tr>
    <tr>
      <td>Center for a New American Security — AI Governance Projects</td>
      <td>Center for a New American Security</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$24,350</td>
      <td>October 2020</td>
      <td>(Image courtesy of the Center for a New American Security.)\n\nGrant investigators: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for a New American Security staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a planning grant of $24,350 to the Center for a New American Security to support work exploring possible projects related to AI governance.\nThis falls within our focus area of potential risks from advanced artificial intelligence</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-governance-projects/">https://www.openphilanthropy.org/grants/center-for-a-new-american-security-ai-governance-projects/</a></td>
    </tr>
    <tr>
      <td>Smitha Milli — Participatory Approaches to Machine Learning Workshop</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$370</td>
      <td>October 2020</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Smitha Milli also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $370 to Smitha Milli to support Participatory Approaches to Machine Learning, a virtual workshop held during the 2020 International Conference on Machine Learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/smitha-milli-participatory-approaches-to-machine-learning-workshop/">https://www.openphilanthropy.org/grants/smitha-milli-participatory-approaches-to-machine-learning-workshop/</a></td>
    </tr>
    <tr>
      <td>Center for International Security and Cooperation — AI Accident Risk and Technology Competition</td>
      <td>Center for International Security and Cooperation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$67,000</td>
      <td>September 2020</td>
      <td>Grant investigators: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for International Security and Cooperation staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a planning grant of $67,000 to Stanford University’s Center for International Security and Cooperation (CISAC) to explore possible projects related to AI accident risk in the context of technology competition.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition/">https://www.openphilanthropy.org/grants/center-for-international-security-and-cooperation-ai-accident-risk-and-technology-competition/</a></td>
    </tr>
    <tr>
      <td>Rice, Hadley, Gates and Manuel LLC — AI Accident Risk and Technology Competition</td>
      <td>Rice, Hadley, Gates & Manuel LLC</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$25,000</td>
      <td>September 2020</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Rice, Hadley, Gates and Manuel LLC staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a contract of $25,000 with Rice, Hadley, Gates and Manuel LLC to explore possible projects related to AI accident risk and technology competition.\nThis falls within our focus area of potential risks from advanced artificial intelligence and was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rice-hadley-gates-and-manuel-llc-ai-accident-risk-and-technology-competition/">https://www.openphilanthropy.org/grants/rice-hadley-gates-and-manuel-llc-ai-accident-risk-and-technology-competition/</a></td>
    </tr>
    <tr>
      <td>Center for Strategic and International Studies — AI Accident Risk and Technology Competition</td>
      <td>Center for Strategic and International Studies</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$118,307</td>
      <td>September 2020</td>
      <td>Speakers from the Center for Strategic and International Studies. (Photo courtesy of CSIS.)\n\nGrant investigators: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Center for Strategic and International Studies staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a planning grant of $118,307 to the Center for Strategic and International Studies to explore possible projects related to AI accident risk in the context of technology competition.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in May 2021.</td>
      <td><a href="https://www.openphilanthropy.org/grants/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition/">https://www.openphilanthropy.org/grants/center-for-strategic-and-international-studies-ai-accident-risk-and-technology-competition/</a></td>
    </tr>
    <tr>
      <td>Wilson Center — AI Policy Seminar Series (June 2020)</td>
      <td>The Wilson Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$496,540</td>
      <td>June 2020</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. The Wilson Center Staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $496,540 to the Wilson Center to organize additional in-depth AI policy seminars as part of its seminar series. The Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue. We continue to believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy.\nThis follows our February 2020 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series-june-2020/">https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series-june-2020/</a></td>
    </tr>
    <tr>
      <td>Andrew Lohn — Paper on Machine Learning Model Robustness</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$15,000</td>
      <td>June 2020</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Andrew Lohn also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $15,000 to Andrew Lohn to write a paper on machine learning model robustness for safety-critical AI systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/andrew-lohn-a-paper-on-machine-learning-model-robustness/">https://www.openphilanthropy.org/grants/andrew-lohn-a-paper-on-machine-learning-model-robustness/</a></td>
    </tr>
    <tr>
      <td>Centre for the Governance of AI — General Support (2020)</td>
      <td>Centre for the Governance of AI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$450,000</td>
      <td>May 2020</td>
      <td>Grant investigator: Committee for Effective Altruism Support\nThis page was reviewed but not written by members of the committee. GovAI staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $450,000 to the Centre for the Governance of AI (GovAI), via the Berkeley Existential Risk Initiative, for general support. GovAI intends to use these funds to support the visit of two senior researchers and a postdoc researcher.\nThis falls within our focus area of potential risks from advanced artificial intelligence. While we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter, our ultimate grant figure was set by the aggregated judgments of our committee reviewing the grant proposal.</td>
      <td><a href="https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support/">https://www.openphilanthropy.org/grants/centre-for-the-governance-of-ai-general-support/</a></td>
    </tr>
    <tr>
      <td>International Conference on Learning Representations — Machine Learning Paper Awards</td>
      <td>International Conference on Learning Representations</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$3,500</td>
      <td>May 2020</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. International Conference on Learning Representations (ICLR) staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $3,500 to the International Conference on Learning Representations to provide awards for the best papers submitted as part of the “Towards Trustworthy Machine Learning” virtual workshop.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/international-conference-on-learning-representations-machine-learning-paper-awards/">https://www.openphilanthropy.org/grants/international-conference-on-learning-representations-machine-learning-paper-awards/</a></td>
    </tr>
    <tr>
      <td>Open Phil AI Fellowship — 2020 Class</td>
      <td>Open Phil AI Fellowship</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,300,000</td>
      <td>May 2020</td>
      <td>Grant investigators: Catherine Olsson and Daniel Dewey\nThis page was reviewed but not written by the grant investigators.\n\nOpen Philanthropy recommended a total of approximately $2,300,000 over five years in PhD fellowship support to 10 promising machine learning researchers that together represent the 2020 class of the Open Phil AI Fellowship. This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. This number may be updated as costs are finalized. These fellows were selected from more than 380 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. This falls within our focus area of potential risks from advanced artificial intelligence.\nWe believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutionsCHAR(59) while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI.\nThe intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests.\nThe 2020 Class of Open Phil AI Fellows\nAlex Tamkin\n\nAlex is a PhD student in Computer Science at Stanford University, where he is advised by Noah Goodman and a member of the Stanford NLP Group. Alex’s research focuses on unsupervised learning: how can we understand and guide systems that learn general-purpose representations of the world? Alex received his B.S. in Computer Science from Stanford, and has spent time at Google Research on the Brain and Language teams. For more information, visit his website.\nClare Lyle\n\nClare is pursuing a PhD in Computer Science at the University of Oxford as a Rhodes Scholar, advised by Yarin Gal and Marta Kwiatkowska. She is interested in developing theoretical tools to better understand the generalization properties of modern ML methods, and in creating principled algorithms based on these insights. She received a BSc in mathematics and computer science from McGill University in 2018. For more information, see her website.\nCody Coleman\n\nCody is a computer science Ph.D. student at Stanford University, advised by Professors Matei Zaharia and Peter Bailis. His research focuses on democratizing machine learning by reducing the cost of producing state-of-the-art models and creating novel abstractions that simplify machine learning development and deployment. His work spans from performance benchmarking of hardware and software systems (i.e., DAWNBench and MLPerf) to computationally efficient methods for active learning and core-set selection. He completed his B.S. and M.Eng. in electrical engineering and computer science at MIT. For more information, visit his website.\nDami Choi\n\nDami is a PhD student in computer science at the University of Toronto, supervised by David Duvenaud and Chris Maddison. Dami is interested in ways to make neural network training faster, more reliable, and more interpretable via inductive bias. Previously, she spent a year at Google as an AI Resident, working with George Dahl on studying optimizers, and speeding up neural network training. She obtained her Bachelor’s degree in Engineering Science from the University of Toronto. You can find more about Dami’s research in her scholar page.\nDan Hendrycks\n\nDan Hendrycks is a second-year PhD student at UC Berkeley, advised by Jacob Steinhardt and Dawn Song. His research aims to disentangle and concretize the components necessary for safe AI. This leads him to work on quantifying and improving the performance of models in unforeseen out-of-distribution scenarios, and he works towards constructing tasks to measure a model’s alignment with human values. Dan received his BS from the University of Chicago. You can find out more about his research at his website.\nEthan Perez\n\nEthan is a PhD student in Computer Science at New York University working with Kyunghyun Cho and Douwe Kiela of Facebook AI Research. His research focuses on developing learning algorithms that have the long-term potential to answer questions that people cannot. Supervised learning cannot answer such questions, even in principle, so he is investigating other learning paradigms for generalizing beyond the available supervision. Previously, Ethan worked with Aaron Courville and Hugo Larochelle at the Montreal Institute for Learning Algorithms, and he has also spent time at Facebook AI Research and Google. Ethan earned a Bachelor’s from Rice University as the Engineering department’s Outstanding Senior. For more information, visit his website.\nFrances Ding\n\nFrances is a PhD student in Computer Science at UC Berkeley advised by Jacob Steinhardt and Moritz Hardt. Her research aims to improve the reliability of machine learning systems and ensure that they have positive, equitable social impacts. She is interested in developing algorithms that can handle dynamic environments and adaptive behavior in the real world, and in building empirical and theoretical understanding of modern ML methods. Frances received her B.A. in Biology from Harvard University and her M.Phil. in Machine Learning from the University of Cambridge. For more information, visit her website.\nLeqi Liu\n\nLeqi is a PhD student in machine learning at Carnegie Mellon University, where she is advised by Zachary Lipton. Her research aims to develop learning systems that can infer human preferences from their behaviors, and better facilitate humans to achieve their goals and well-being. In particular, she is interested in bringing theory from social sciences into algorithmic design. You can learn more about her research on her website.\nPeter Henderson\n\nPeter is a PhD student at Stanford University advised by Dan Jurafsky. His research focuses on creating robust decision-making systems grounded in causal inference mechanisms — particularly in natural language domains. He also spends time investigating reproducible and thorough evaluation methodologies to ensure that such systems perform as expected when deployed. Peter’s other work reaches into policy and technical issues related to the use of machine learning in governance and law, as well as applications of machine learning for positive social impact. Previously he earned his B.Eng. and M.Sc. from McGill University with a thesis on reproducibility and reusability in deep reinforcement learning advised by Joelle Pineau and David Meger. For more information, see his website.\nStanislav Fort\n\nStanislav is a PhD student at Stanford University, advised by Surya Ganguli. His research focuses on developing a scientific understanding of deep learning and on applications of machine learning and artificial intelligence in the physical sciences, in domains spanning from X-ray astrophysics to quantum computing. Stanislav spent a year as a Google AI Resident, where he worked on deep learning theories and their applications in collaboration with colleagues from Google Brain and DeepMind. He received his Bachelor’s and Master’s degrees in Physics at Trinity College, University of Cambridge, and a Master’s degree at Stanford University. For more information, visit his website.</td>
      <td><a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2020-class/">https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2020-class/</a></td>
    </tr>
    <tr>
      <td>World Economic Forum — Global AI Council Workshop</td>
      <td>World Economic Forum</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$50,000</td>
      <td>April 2020</td>
      <td>(Image courtesy of the World Economic Forum.)\n\nGrant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. World Economic Forum staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of up to $50,000 to the World Economic Forum to support a workshop hosted by the Global AI Council and co-developed with the Center for Human-Compatible AI at UC Berkeley. The workshop will facilitate the development of AI policy recommendations that could lead to future economic prosperity, and is part of a series of workshops examining solutions to maximize economic productivity and human wellbeing.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/world-economic-forum-global-ai-council-workshop/">https://www.openphilanthropy.org/grants/world-economic-forum-global-ai-council-workshop/</a></td>
    </tr>
    <tr>
      <td>Johns Hopkins University — Support for Jared Kaplan and Brice Ménard</td>
      <td>Johns Hopkins University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$55,000</td>
      <td>March 2020</td>
      <td>Grant investigators: Nick Beckstead\nThis page was reviewed but not written by the grant investigator. Johns Hopkins University staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $55,000 to Johns Hopkins University to support the initial research of Professors Jared Kaplan and Brice Ménard on principles underlying neural network training and performance.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/johns-hopkins-university-support-for-jared-kaplan-and-brice-menard/">https://www.openphilanthropy.org/grants/johns-hopkins-university-support-for-jared-kaplan-and-brice-menard/</a></td>
    </tr>
    <tr>
      <td>Study and Training Related to AI Policy Careers — Scholarship Support</td>
      <td>Study and Training Related to AI Policy Careers</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$593,540</td>
      <td>March 2020</td>
      <td>Grant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Scholarship recipients also reviewed the page prior to publication.\n\nOpen Philanthropy recommended flexible support to enable individuals to pursue and explore careers in artificial intelligence policy. We sought applications for this funding here.\nWe have also offered funding to a few other applicants, but are waiting to learn whether they accept (e.g. based on whether they are admitted to their graduate school program of choice). We plan to update this page again once the full list of recipients is known, at which time we will also list the total amount of funding provided.\nThis falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount has been updated to reflect $700 in funds returned to us in September 2020.</td>
      <td><a href="https://www.openphilanthropy.org/grants/study-and-training-related-to-ai-policy-careers-scholarship-support/">https://www.openphilanthropy.org/grants/study-and-training-related-to-ai-policy-careers-scholarship-support/</a></td>
    </tr>
    <tr>
      <td>WestExec — Report on Assurance in Machine Learning Systems</td>
      <td>WestExec</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$540,000</td>
      <td>February 2020</td>
      <td>Grant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. WestExec staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a contract of $540,000 with WestExec to support the production and distribution of a report on advancing policy, process, and funding for the Department of Defense’s work on test, evaluation, verification, and validation for deep learning systems. This falls within our focus on the future of artificial intelligence.\nThis project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.\nThe grant amount was updated in October and November 2020 and May 2021.</td>
      <td><a href="https://www.openphilanthropy.org/grants/westexec-report-on-assurance-in-machine-learning-systems/">https://www.openphilanthropy.org/grants/westexec-report-on-assurance-in-machine-learning-systems/</a></td>
    </tr>
    <tr>
      <td>Wilson Center — AI Policy Seminar Series (February 2020)</td>
      <td>The Wilson Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$368,440</td>
      <td>February 2020</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Wilson Center Staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $368,440 over two years to the Wilson Center to continue support for a series of in-depth AI policy seminars. The Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue. We continue to believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy.\nThis follows our July 2018 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series-february-2020/">https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series-february-2020/</a></td>
    </tr>
    <tr>
      <td>Machine Intelligence Research Institute — General Support (2020)</td>
      <td>Machine Intelligence Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$7,703,750</td>
      <td>February 2020</td>
      <td>Grant investigator: Committee for Effective Altruism Support\nThis page was reviewed but not written by members of the committee. MIRI staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $7,703,750 to the Machine Intelligence Research Institute (MIRI) for general support. MIRI plans to use these funds for ongoing research and activities related to reducing potential risks from advanced artificial intelligence, one of our focus areas.\nThis follows our February 2019 support. While we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter, our ultimate grant figure was set by the aggregated judgments of our committee for effective altruism support, described in more detail here.</td>
      <td><a href="https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2020/">https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2020/</a></td>
    </tr>
    <tr>
      <td>Stanford University — AI Safety Seminar</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,500</td>
      <td>January 2020</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Stanford University staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $6,500 to Stanford University to support an artificial intelligence (AI) safety seminar led by Professor Dorsa Sadigh. This grant is intended to fund the travel costs for experts on AI safety to present at the seminar.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-ai-safety-seminar/">https://www.openphilanthropy.org/grants/stanford-university-ai-safety-seminar/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — General Support</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$150,000</td>
      <td>January 2020</td>
      <td>Grant investigator: Claire Zabel\nThis page was reviewed but not written by the grant investigator. BERI staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $150,000 to the Berkeley Existential Risk Initiative (BERI) for general support. BERI seeks to reduce existential risks to humanity, and collaborates with other longtermist organizations, including the Center for Human-Compatible AI at UC Berkeley. This funding is intended to help BERI establish new collaborations.\nThis follows our January 2019 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-general-support/</a></td>
    </tr>
    <tr>
      <td>RAND Corporation — Research on the State of AI Assurance Methods</td>
      <td>The RAND Corporation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$30,751</td>
      <td>January 2020</td>
      <td>Andrew Lohn facilitating a discussion of the vulnerabilities of machine learning at the RAND Corporation. (Photo courtesy of the RAND Corporation.)\n\nGrant investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. The RAND Corportation staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a planning grant of $30,751 to the RAND Corporation to support exploratory research by Andrew Lohn on the state of AI assurance methods.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/rand-corporation-research-on-the-state-of-ai-assurance-methods/">https://www.openphilanthropy.org/grants/rand-corporation-research-on-the-state-of-ai-assurance-methods/</a></td>
    </tr>
    <tr>
      <td>Press Shop — Support for Human Compatible</td>
      <td>Press Shop</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$17,000</td>
      <td>January 2020</td>
      <td>Stuart Russell delivering his 2017 TED talk. (Photo courtesy of the Center for Human-Compatible AI.)\n\nGrant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Press Shop staff also reviewed this page prior to publication\n\nOpen Philanthropy recommended a grant of $17,000 to the publicity firm Press Shop to support expenses related to publicizing Professor Stuart Russell’s book Human Compatible: Artificial Intelligence and the Problem of Control. Russell is the director of the Center for Human-Compatible Artificial Intelligence (CHAI) at UC Berkeley. His book examines and proposes solutions to risks posed by advanced artificial intelligence.\nThis follows our August 2016 and November 2019 support to CHAI and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/press-shop-support-for-human-compatible/">https://www.openphilanthropy.org/grants/press-shop-support-for-human-compatible/</a></td>
    </tr>
    <tr>
      <td>Ought — General Support (2020)</td>
      <td>Ought</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,593,333</td>
      <td>January 2020</td>
      <td>Grant investigator: Committee for Effective Altruism Support\nThis page was reviewed but not written by the grant investigator. Ought staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $1,593,333 to Ought for general support. Ought conducts research on factored cognition, which we consider relevant to AI alignment and to reducing potential risks from advanced artificial intelligence.\nThis follows our November 2019 support. While we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter, our ultimate grant figure was set by the aggregated judgments of our committee for effective altruism support, described in more detail here.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ought-general-support-2020/">https://www.openphilanthropy.org/grants/ought-general-support-2020/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — AI Safety Research (2019)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,111,000</td>
      <td>December 2019</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. UC Berkeley staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $1,111,000 over three years to UC Berkeley to support research relevant to potential risks from artificial intelligence and machine learning, led by Jacob Steinhardt. This funding will allow Professor Steinhardt to fund students to work on robustness, value learning, aggregating preferences, and other areas of machine learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research-2019/">https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research-2019/</a></td>
    </tr>
    <tr>
      <td>Ought — General Support (2019)</td>
      <td>Ought</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,000,000</td>
      <td>November 2019</td>
      <td>Grant Investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Ought staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $1,000,000 over two years to Ought for general support. Ought conducts research on factored condition, which we consider relevant to AI alignment.\nThis follows our May 2018 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ought-general-support-2019/">https://www.openphilanthropy.org/grants/ought-general-support-2019/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Center for Human-Compatible AI (2019)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$200,000</td>
      <td>November 2019</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. UC Berkeley staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $200,000 over two years to UC Berkeley to support the Center for Human-Compatible Artificial Intelligence (CHAI). CHAI plans to use these funds to support graduate student and postdoc research.\nThis supplements our August 2016 support for CHAI’s launch, and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2019/">https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2019/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — CHAI Collaboration (2019)</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$705,000</td>
      <td>November 2019</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. BERI staff also reviewed this page prior to publication.\n\nOpen Philanthropy recommended a grant of $705,000 over two years to the Berkeley Existential Risk Initiative (BERI) to support continued work with the Center for Human-Compatible AI (CHAI) at UC Berkeley. This includes one year of support for machine learning researchers hired by BERI, and two years of support for CHAI.\nThis follows our July 2017 and January 2019 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2019/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-collaboration-2019/</a></td>
    </tr>
    <tr>
      <td>Open Phil AI Fellowship — 2019 Class</td>
      <td>Open Phil AI Fellowship</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,325,000</td>
      <td>May 2019</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator.\n\nThe Open Philanthropy Project recommended a total of approximately $2,325,000 over five years in PhD fellowship support to eight promising machine learning researchers that together represent the 2019 class of the 2019 class of the Open Phil AI Fellowship1 .This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. This number may be updated as costs are finalized. These fellows were selected from more than 175 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. This falls within our focus area of potential risks from advanced artificial intelligence.\nWe believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutions; while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI.\nThe intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests.\nThe 2019 Class of Open Phil AI Fellows\nAidan Gomez\n\nAidan is a doctoral student of Yarin Gal and Yee Whye Teh at the University of Oxford. He leads the research group FOR.ai, focusing on providing resources, mentorship, and facilitating collaboration between academia and industry. On a technical front, Aidan’s research pursues new methods of scaling individual neural networks towards trillions of parameters, and hundreds of tasks. On an ethical front, his work takes a humanist stance on machine learning applications and their risks. Aidan is a Student Researcher at Google Brain, working with Jakob Uszkoreit; Previously at Brain, he worked with Geoffrey Hinton and Łukasz Kaiser. He obtained his B.Sc from The University of Toronto with supervision from Roger Grosse.\nAndrew Ilyas\n\nAndrew Ilyas is a first-year PhD student at MIT working on machine learning. His interests are in building robust and reliable learning systems, and in understanding the underlying principles of modern ML methods. Andrew completed his B.Sc and MEng. in Computer Science as well as B.Sc. in Mathematics at MIT in 2018. For more information, see his website.\nJulius Adebayo\n\nJulius is a PhD student in Computer Science at MIT. He is interested in provable methods to enable algorithms and machine learning systems exhibit robust and reliable behavior. Specifically, he is interested in constraints relating to privacy/security, bias/fairness, and robustness to distribution shift for agents and systems deployed in the real world. Julius received masters degrees in computer science and technology policy from MIT, where he looked at bias and interpretability of machine learning models. For more information, visit his website.\nLydia T. Liu\n\nLydia T. Liu is a PhD student in Computer Science at the University of California, Berkeley, advised by Moritz Hardt and Michael I. Jordan. Her research aims to establish the theoretical foundations for machine learning algorithms to have reliable and robust performance, as well as positive long-term societal impact. She is interested in developing learning algorithms with multifaceted guarantees and understanding their distributional effects in dynamic or interactive settings. Lydia graduated with a Bachelor of Science in Engineering degree from Princeton University. She is the recipient of an ICML Best Paper Award (2018) and a Microsoft Ada Lovelace Fellowship. For more information, visit her website.\nMax Simchowitz\n\nMax Simchowitz is a PhD student in Electrical Engineering and Computer Science at UC Berkeley, co-advised by Benjamin Recht and Michael Jordan. He works on machine learning problems with temporal structure: either because the learning agent is allowed to make adaptive decisions about how to collect data, or because the agent’s the environment dynamically reacts to measurements taken. He received his A.B. in mathematics from Princeton University in 2015, and is a co-recipient of the ICML 2018 best paper award. You can find out more about his research on his website.\nPratyusha Kalluri\n\nPratyusha “Ria” Kalluri is a second year PhD student in Computer Science at Stanford, advised by Stefano Ermon and Dan Jurafsky. She is working towards discovering and inducing conceptual reasoning inside machine learning models. This leads her to work on interpretability, novel learning objectives, and learning disentangled representations. She believes this work can help shape a more radical and equitable AI future. Ria received her Bachelors degree in Computer Science at MIT in 2016 and was a Visiting Researcher at Complutense University of Madrid before beginning her PhD. For more information, visit her website.\nSiddharth Karamcheti\n\nSidd is an incoming PhD student in Computer Science at Stanford University. He is interested in grounded language understanding, with a goal of building agents that can collaborate with humans and act safely in different environments. He is finishing up a one-year residency at Facebook AI Research in New York. He received his Sc.B. from Brown University, where he did research in human-robot interaction and natural language processing advised by Professors Stefanie Tellex and Eugene Charniak. You can find more information on his website.\nSmitha Milli\n\nSmitha is a 2nd year PhD student in computer science at UC Berkeley, where she is advised by Moritz Hardt and Anca Dragan. Her research aims to create machine learning systems that are more value-aligned. She focuses, in particular, on difficulties that arise from complexities of human behavior. For example, learning what a user prefers the system to do, despite “irrationalities” in the user’s behavior, or learning the right decisions to make, despite strategic adaptation from humans. For links to publications and other information, you can visit her website.</td>
      <td><a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2019-class/">https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2019-class/</a></td>
    </tr>
    <tr>
      <td>Machine Intelligence Research Institute — General Support (2019)</td>
      <td>Machine Intelligence Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,652,500</td>
      <td>February 2019</td>
      <td>Grant investigator: Committee for Effective Altruism Support\nThis page was reviewed but not written by members of the committee. MIRI staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $2,652,500 over two years to the Machine Intelligence Research Institute (MIRI) for general support. MIRI plans to use these funds for ongoing research and activities related to reducing potential risks from advanced artificial intelligence, one of our focus areas. Planned activities include alignment research, a summer fellows program, computer scientist workshops, and internship programs.\nThis grant supplements our three-year October 2017 support. While we see the basic pros and cons of this support similarly to what we’ve presented in past writeups on the matter, our ultimate grant figure was set by the aggregated judgments of our committee for effective altruism support, described in more detail here.\nUpdate: In November 2019, we added funding to the original award amount. The “grant amount” above has been updated to reflect this.</td>
      <td><a href="https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2019/">https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2019/</a></td>
    </tr>
    <tr>
      <td>Georgetown University — Center for Security and Emerging Technology</td>
      <td>Center for Security and Emerging Technology</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$55,000,000</td>
      <td>January 2019</td>
      <td>Grant Investigator: Luke Muehlhauser\n\nThis page was reviewed but not written by the grant investigator. Center for Security and Emerging Technology staff also reviewed this page prior to publication.\n\n\n\n\n\n\n\nThe Open Philanthropy Project recommended a grant of $55,000,000 over five years to Georgetown University to launch the Center for Security and Emerging Technology (CSET), a new think tank dedicated to policy analysis at the intersection of national and international security and emerging technologies. CSET is led by Jason Matheny, former Assistant Director of National Intelligence and Director of Intelligence Advanced Research Projects Activity (IARPA), the U.S. intelligence community’s research organization.\nCSET plans to provide nonpartisan technical analysis and advice related to emerging technologies and their security implications to the government, key media outlets, and other stakeholders. Its initial focus will be on the intersection of security and artificial intelligence, a key issue of relevance to our focus area on the future of AI.\nBackground\nWe have written in detail about the case we see for funding work related to AI on our blog. As we wrote in that post, we see AI and machine learning research as being on a very short list of the most dynamic, unpredictable, and potentially world-changing areas of science. Broadly, we expect the results of continuing progress in AI research to be positive for society at large, but we see some risks (both from unintended consequences of AI use, and from deliberate misuse), and believe that we—as a philanthropic organization, separate from academia, industry, and government—may be well-placed to support work to reduce those risks.\nThe organization\nCSET is a new Georgetown University-based think tank dedicated to policy analysis at the intersection of national and international security and emerging technologies. CSET is led by Jason Matheny, former Assistant Director of National Intelligence and Director of IARPA, the U.S. intelligence community’s research organization. He is a Commissioner of the National Security Commission on AI, created by the US Congress in 2018. Based on multiple conversations with leaders in the defense, intelligence, and policymaking communities, we believe he is respected and considered well-informed and credible in the relevant circles.1\nMatheny is knowledgeable about several emerging technologies, particularly AI, and their intersection with security. We believe he is unusually attentive to risks from emerging technologies (discussed below). We also believe he broadly shares our interest in cost-effective ways of doing as much good as possible; he co-founded New Harvest, worked for many years in global health, and contributed to the World Bank publication Disease Control Priorities in Developing Countries. We believe Matheny combines these qualities with strong government experience and national security qualifications to a unique degree.\nMatheny has assembled an impressive founding team with strong qualifications to provide high-quality, safety-conscious advice to policymakers on AI. Team members include:\n\nDewey Murdick, CSET’s Director of Data Science, was previously the Director of Science Analytics at the Chan Zuckerberg Initiative, where he led metric development, data science, and machine learning and statistical research for Meta and science-related initiatives; Chief Analytics Officer and Deputy Chief Scientist within the Department of Homeland Security; and IARPA Program Manager and Office Co-Director.\nWilliam Hannas, CSET Lead Analyst, was a member of the Senior Intelligence Service at the Central Intelligence Agency, where he served as the Open Source Enterprise’s primary China science and technology analyst. He was previously an Assistant Professor of Chinese at Georgetown, where he taught Chinese and Korean, and concurrently served with the Foreign Broadcast Information Service, monitoring Asian language publications.\nHelen Toner, CSET’s Director of Strategy and Plans, previously worked as a Senior Research Analyst here at the Open Philanthropy Project, advising policymakers and grantmakers on AI policy and strategy. Between working at Open Philanthropy and joining CSET, she lived in Beijing, studying the Chinese AI ecosystem as a Research Affiliate of Oxford University’s Center for the Governance of AI.\nTessa Baker, CSET’s Director of Operations, conducted survey and qualitative research among Fortune 500 business leaders as a Sr. Principal at Gartner, and served government executives at OPM, DHS, Joint Staff, OSD, and FEMA as a consultant with IBM and NSI.\nBen Buchanan, CSET Faculty Fellow, is an Assistant Teaching Professor at Georgetown University’s School of Foreign Service, where he conducts research on the intersection of cybersecurity and statecraft. His first book, The Cybersecurity Dilemma, was published by Oxford University Press in 2017.\nJamie Baker, CSET Distinguished Fellow, is a Professor at Syracuse University, where he directs the Institute for National Security and Counterterrorism. Judge Baker served in the US Department of State, Foreign Intelligence Advisory Board, and National Security Council. He served on the US Court of Appeals for the Armed Forces for 15 years—the last four as Chief Judge.\nMichael Sulmeyer, CSET Senior Fellow, was the Director of the Cyber Security Project at Harvard University. Before Harvard, he served as the Director for Plans and Operations for Cyber Policy in the Office of the Secretary of Defense, and previously worked at the Pentagon on arms control and the maintenance of strategic stability between the United States, Russia, and China.\n\nRead full bios for these and other founding team members here.\nCase for the grant\nGiven our focus on increasing potential benefits and reducing potential risks from AI, we are interested in opportunities to inform current and future policies that could affect long-term outcomes. We think one of the key factors in whether AI is broadly beneficial for society is whether policymakers are well-informed and well-advised about the nature of AI’s potential benefits, potential risks, and how these relate to potential policy actions. If AI research continues to progress quickly in the coming years, we anticipate that demands for government action will become more frequent. Government action—such as major funding decisions or regulatory measures—could greatly affect the benefits and risks of AI, depending on details that are difficult to foresee today, and we accordingly think that good information and advice could be key.\nWe share the view expressed in Technology Roulette2 that key policymaking communities in national security and foreign policy are often overly focused on technological superiority, which is not synonymous with security because it does not reduce accidents or address emergent effects.3 We think some of the most important potential risks from advanced AI are in the category of accidents or emergent effects (the latter could include arms-race dynamics or “use it or lose it” first strikes); that the single-minded pursuit of technological superiority could make such risks much worse; and that a well-informed policymaking apparatus that internalizes concern over the potential accidents and emergent risks of AI could be especially important in reducing risks.\nOverall, we feel that ensuring high-quality and well-informed advice to policymakers over the long run is one of the most promising ways to increase the benefits and reduce the risks from advanced AI, and that the team put together by CSET is uniquely well-positioned to provide such advice. Hence, we believe this grant represents the best chance we can expect to see for some time to address one of the most important gaps we’re aware of in this cause. We recognize a number of risks to this grant (below), but within our “hits-based” framework, we believe it is an excellent bet. For context, this grant is large compared to other individual grants we’ve made to date, in part because of the long commitment necessary to get a new center started, but even with it our annual giving around potential risks from artificial intelligence continues to be smaller than our giving in global health or scientific research.\n\nGoals and plans for follow-up\nCSET has identified a number of primary goals that fall under the umbrella of informing key discussions and decisions related to AI, computing, and potentially other high-impact emerging technologies:\n\nAssess global developments in key technology areas, with particular focus on developments in countries of interest to U.S. policy communities.\nGenerate written products and briefings tailored to policy communities, with practical policy options.\nTrain and prepare staff, students, faculty, and affiliates for key roles within the policy community.\n\n\nRisks and reservations\n\nWe see a number of potential risks for this grant:\nIt’s inherently difficult to provide useful advice on emerging technologies, and the potential benefits and risks of advanced AI are currently speculative and poorly understood, so it could easily be the case that this grant is premature for the goal of generating useful important advice to policymakers.\nWe worry that heavy government involvement in, and especially regulation of, AI could be premature and might be harmful at this time. We think it’s possible that by drawing attention to the nexus of security and emerging technologies (including AI), CSET could lead to premature regulatory attention and thus to harm. However, we believe CSET shares our interest in caution on this front and is well-positioned to communicate carefully.\nFor a new organization like CSET, we see leadership as even more important than usual, and Matheny’s leadership contributes substantially to our case for this grant. Were he to depart, we would need to reconsider our support for, and alignment with, the future plans for CSET.\nMore generally, we expect any new organization to face significant challenges on the path to impact, many of which will likely be unanticipated.</td>
      <td><a href="https://www.openphilanthropy.org/grants/georgetown-university-center-for-security-and-emerging-technology/">https://www.openphilanthropy.org/grants/georgetown-university-center-for-security-and-emerging-technology/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — CHAI ML Engineers</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$250,000</td>
      <td>January 2019</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. BERI staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $250,000 to the Berkeley Existential Risk Initiative (BERI) to temporarily or permanently hire machine learning research engineers dedicated to BERI’s collaboration with the Center for Human-compatible Artificial Intelligence (CHAI).\nBased on conversations with various professors and students, we believe CHAI could make more progress with more engineering support.\nThis grant follows previous support to UC Berkeley to launch CHAI and to BERI to collaborate with CHAI, and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-ml-engineers/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-chai-ml-engineers/</a></td>
    </tr>
    <tr>
      <td>Daniel Kang, Jacob Steinhardt, Yi Sun, and Alex Zhai — Study of the Robustness of Machine Learning Models</td>
      <td>NaN</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,351</td>
      <td>November 2018</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Daniel Kang, Jacob Steinhardt, Yi Sun, and Alex Zhai also reviewed this page prior to publication.\n\nOpen Philanthropy contracted with Daniel Kang, Jacob Steinhardt, Yi Sun, and Alex Zhai for $2,351 to reimburse technology costs for their efforts to study the robustness of machine learning models, especially robustness to unforeseen adversaries. We believe this will accelerate progress in adversarial, worst-case robustness in machine learning.\nThis falls within our focus area of potential risks from advanced artificial intelligence. This project was supported through a contractor agreement. While we typically do not publish pages for contractor agreements, we occasionally opt to do so.</td>
      <td><a href="https://www.openphilanthropy.org/grants/daniel-kang-jacob-steinhardt-yi-sun-and-alex-zhai-study-of-the-robustness-of-machine-learning-models/">https://www.openphilanthropy.org/grants/daniel-kang-jacob-steinhardt-yi-sun-and-alex-zhai-study-of-the-robustness-of-machine-learning-models/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — AI Safety Research (2018)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,145,000</td>
      <td>November 2018</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. UC Berkeley staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended two grants totaling $1,145,000 over three years to UC Berkeley for machine learning researchers Pieter Abbeel and Aviv Tamar to study uses of generative models for robustness and interpretability. This funding will allow Mr. Abbeel and Mr. Tamar to fund PhD students and summer undergraduates to work on classifiers, imitation learning systems, and reinforcement learning systems.\nThis falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research-2018/">https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research-2018/</a></td>
    </tr>
    <tr>
      <td>GoalsRL — Workshop on Goal Specifications for Reinforcement Learning</td>
      <td>GoalsRL</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$7,500</td>
      <td>August 2018</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. GoalsRL staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended an award of $7,500 to offset travel, registration, and other expenses associated with attending the GoalsRL 2018 workshop on goal specifications for reinforcement learning. The workshop was organized by Ashley Edwards, a recent computer science PhD candidate interested in reward learning.\nThis funding is discretionary and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/goalsrl-workshop-on-goal-specifications-for-reinforcement-learning/">https://www.openphilanthropy.org/grants/goalsrl-workshop-on-goal-specifications-for-reinforcement-learning/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Machine Learning Security Research Led by Dan Boneh and Florian Tramer</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>July 2018</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. Stanford University staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a gift of $100,000 to Stanford University to support machine learning security research led by Professor Dan Boneh and his PhD student, Florian Tramer. Machine learning security probes worst-case performance of learned models, and we consider work in this area a promising way of ensuring that models are “doing the right thing” in a generalizable way.\nOur main rationale for making this gift include:\n\nWe consider Florian Tramer a very strong PhD student who is currently conducting excellent machine learning security work.\nWe expect excellent machine learning security work to be very important for AI safety.\nGenerally speaking, we expect increased funding in areas relevant to AI safety—like machine learning security—to move the field in a direction we consider positive and aligned with our interests in mitigating potential risks from advanced AI; we therefore consider this gift a small nudge in that direction.\n\nThis gift falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-machine-learning-security-research-led-by-dan-boneh-and-florian-tramer/">https://www.openphilanthropy.org/grants/stanford-university-machine-learning-security-research-led-by-dan-boneh-and-florian-tramer/</a></td>
    </tr>
    <tr>
      <td>Wilson Center — AI Policy Seminar Series</td>
      <td>The Wilson Center</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$400,000</td>
      <td>July 2018</td>
      <td>Grant Investigator: Luke Muehlhauser\nThis page was reviewed but not written by the grant investigator. Wilson Center Staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $400,000 over two years to the Wilson Center to support a series of in-depth AI policy seminars. The Wilson Center is a non-partisan policy forum for tackling global issues through independent research and open dialogue. We believe the seminar series can help inform AI policy discussions and decision-making in Washington, D.C., and could help identify and empower influential experts in those discussions, a key component of our AI policy grantmaking strategy.\nThis grant falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series/">https://www.openphilanthropy.org/grants/wilson-center-ai-policy-seminar-series/</a></td>
    </tr>
    <tr>
      <td>University of Oxford — Research on the Global Politics of AI</td>
      <td>University of Oxford</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$429,770</td>
      <td>July 2018</td>
      <td>Grant investigator: Nick Beckstead\n\nThis page was reviewed but not written by the grant investigator. Oxford University staff also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a gift of £323,646 ($429,769.52 at the time of conversion) to the University of Oxford to support research on the global politics of advanced artificial intelligence. The work will be led by Professor Allan Dafoe at the Future of Humanity Institute in Oxford, United Kingdom. The Open Philanthropy Project recommended additional funds to support this work in 2017, while Professor Dafoe was at Yale.\nThis funding falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/oxford-university-research-on-the-global-politics-of-ai/">https://www.openphilanthropy.org/grants/oxford-university-research-on-the-global-politics-of-ai/</a></td>
    </tr>
    <tr>
      <td>Machine Intelligence Research Institute — AI Safety Retraining Program</td>
      <td>Machine Intelligence Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$150,000</td>
      <td>June 2018</td>
      <td>Researchers at the Machine Intelligence Research Institute. (Photo courtesy of MIRI)\n\nGrant investigator: Claire Zabel\nThis page was reviewed but not written by the grant investigator. MIRI also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $150,000 to the Machine Intelligence Research Institute (MIRI) to support its artificial intelligence safety (AI) retraining project. MIRI intends to use these funds to provide stipends, structure, and guidance to promising computer programmers and other technically proficient individuals who are considering transitioning their careers to focus on potential risks from advanced artificial intelligence. MIRI believes the stipends will make it easier for aligned individuals to leave their jobs and focus full-time on safety. MIRI expects the transition periods to range from three to six months per individual.\nThis is a discretionary grant and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-ai-safety-retraining-program/">https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-ai-safety-retraining-program/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — General Support (2018)</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$100,000</td>
      <td>June 2018</td>
      <td>Grant Investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. AI Impacts also reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $100,000 over two years to AI Impacts via the Machine Intelligence Research Institute for general support. AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence.\nThis discretionary grant is a renewal of our December 2016 support and falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-general-support-2018/">https://www.openphilanthropy.org/grants/ai-impacts-general-support-2018/</a></td>
    </tr>
    <tr>
      <td>Open Phil AI Fellowship — 2018 Class</td>
      <td>Open Phil AI Fellowship</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,245,000</td>
      <td>May 2018</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator.\n\nThe Open Philanthropy Project recommended a total of approximately $1,245,000 over five years in PhD fellowship support to seven machine learning researchers that together represent the 2018 class of the Open Phil AI Fellowship. This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. These fellows were selected from more than 180 applicants for their academic excellence, technical knowledge, careful reasoning, and interest in making the long-term, large-scale impacts of AI a central focus of their research. This falls within our focus area of potential risks from advanced artificial intelligence.\nWe believe that progress in artificial intelligence may eventually lead to changes in human civilization that are as large as the agricultural or industrial revolutions; while we think it’s most likely that this would lead to significant improvements in human well-being, we also see significant risks. Open Phil AI Fellows have a broad mandate to think through which kinds of research are likely to be most valuable, to share ideas and form a community with like-minded students and professors, and ultimately to act in the way that they think is most likely to improve outcomes from progress in AI.\nThe intent of the Open Phil AI Fellowship is both to support a small group of promising researchers and to foster a community with a culture of trust, debate, excitement, and intellectual excellence. We plan to host gatherings once or twice per year where fellows can get to know one another, learn about each other’s work, and connect with other researchers who share their interests.\nThe 2018 AI Fellows\nAditi Raghunathan\n\nAditi is a second year PhD student in Computer Science at Stanford University, advised by Percy Liang. She is interested in making Machine Learning systems provably reliable and fair, especially in the presence of adversaries. Aditi received her Bachelors degree in Computer Science and Engineering from IIT Madras in 2016. For links to publications and more information, please visit her website.\nChris Maddison\n\nChris is a DPhil student at the University of Oxford, supervised by Yee Whye Teh and Arnaud Doucet, and a Research Scientist at DeepMind. Chris is interested in the tools used for inference and optimization in scalable and expressive models. He aims to expand the range of such models by expanding the toolbox needed to work with them. Chris received his MSc. from the University of Toronto, working with Geoffrey Hinton. He received a NIPS Best Paper Award in 2014 and was one of the founding members of the AlphaGo project. For more information, visit his website.\nFelix Berkenkamp\n\nFelix is a PhD student in Computer Science at ETH Zurich, working with Andreas Krause and Angela Schoellig (University of Toronto). He is interested in enabling robots to safely and autonomously learn in uncertain real-world environments, which requires new reinforcement learning algorithms that respect the physical limitations and constraints of dynamic systems and provide theoretical safety guarantees. He received his Masters degree in Mechanical Engineering from ETH Zurich in 2015. You can find out more about his research on his website.\nJon Gauthier\n\nJon is a PhD student at the Massachusetts Institute of Technology in the Department of Brain and Cognitive Sciences, where he works with Roger Levy and Joshua Tenenbaum to build computational models of how people acquire and understand language. His research bridges between artificial intelligence, cognitive science, and linguistics in order to specify better concrete objectives for building language understanding systems. Before joining MIT, Jon studied at Stanford University and worked with Christopher Manning in the Stanford Natural Language Processing Group. He also spent time at Google Brain and OpenAI, where his advisors included Ilya Sutskever and Oriol Vinyals. You can find out more about Jon, including his blog and research articles, at his website.\nMichael Janner\n\nMichael is an incoming PhD student at UC Berkeley. He is interested in reproducing humans’ flexible problem-solving abilities in machines, in particular through compositional representations. In June 2018, he will receive his Bachelors degree in computer science from MIT, where he worked with Professors Joshua Tenenbaum and Regina Barzilay. More information can be found on his website.\nNoam Brown\n\nNoam is a PhD student in computer science at Carnegie Mellon University advised by Tuomas Sandholm. His research applies computational game theory to produce AI systems capable of strategic reasoning in imperfect-information multi-agent interactions. He has applied this research to creating Libratus, the first AI to defeat top humans in no-limit poker. Noam received a NIPS Best Paper award in 2017 and an Allen Newell Award for Research Excellence. Prior to starting a PhD, Noam worked at the Federal Reserve researching the effects of algorithm trading on financial markets. Before that, he developed algorithmic trading strategies. His papers and videos are available on his website.\nRuth Fong\n\nRuth is a PhD student in Engineering Science at the University of Oxford, where she is advised by Andrea Vedaldi. She is interested in understanding, explaining, and improving the internal representations of deep neural networks. Ruth received her Bachelors degree in Computer Science from Harvard University in 2015; she also earned a Masters degree in Neuroscience from Oxford in 2016 as a Rhodes Scholar. For more information about her research, visit her website.</td>
      <td><a href="https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2018-class/">https://www.openphilanthropy.org/grants/open-phil-ai-fellowship-2018-class/</a></td>
    </tr>
    <tr>
      <td>Ought — General Support (2018)</td>
      <td>Ought</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$525,000</td>
      <td>May 2018</td>
      <td>Grant Investigator: Daniel Dewey\n\n\n\n\n\n\nThis page was reviewed but not written by the grant investigator. Ought staff also reviewed this page prior to publication.\n\n\n\n\n\n\nThe Open Philanthropy Project recommended a grant of $525,000 to Ought for general support. Ought is a new organization with a mission to “leverage machine learning to help people think.” Ought plans to conduct research on deliberation and amplification, a concept we consider relevant to AI alignment.1 Our funding, combined with another grant from Open Philanthropy Project technical advisor Paul Christiano, is intended to allow Ought to hire up to three new staff members and provide one to three years of support for Ought’s work, depending how quickly they hire.\nBackground\nThis grant falls within our work on potential risks from advanced artificial intelligence, one of our focus areas within global catastrophic risks. Ought is a new 501(c)(3) organization founded by Andreas Stuhlmüller, a former researcher at Stanford’s Computation and Cognition lab.2 Ought’s goal is to conduct research and build tools that leverage machine learning for deliberation, and to do so in a scalable way.\nAbout the grant\nProposed activities\nOught will conduct research on deliberation and amplification, aiming to organize the cognitive work of ML algorithms and humans so that the combined system remains aligned with human interests even as algorithms take on a much more significant role than they do today.\nAndreas believes that AI will ultimately be used to help people deliberate and make wise decisions. Ought will focus on this application, conducting theoretical and empirical work informed by real-world problems and data. Andreas thinks that it is helpful to pursue a concrete vision for how transformative AI might benefit and empower people, because such a vision can be criticized and improved, and can guide more theoretical research.\nEarly on, Ought plans to focus on conceptual research and implementation of prototypes for decomposing and automating deliberation. Depending on research outcomes, Ought expects to move towards a more empirical and application-driven approach over time.\nOught plans to publish its results, thoughts, code, and progress in online posts for the benefit of other researchers, and will publish in academic outlets if the additional effort is clearly justified. We do not expect a significant number of academic publications to result from this grant, and would consider such publications a bonus instead of part of the basic case for the funding.\nFor more information on Ought’s vision, see this page by Andreas.\nCase for the grant\nThe basic case for the grant is as follows:\n\nWe consider research on deliberation and amplification as an approach to AI safety both important and neglected.\nPaul Christiano is excited by Ought’s plan and work, and we trust his judgement.\nOught’s plan appears flexible and we think Andreas is ready to notice and respond to any problems by adjusting his plans.\nWe have seen some minor indications that Ought is well-run and has a reasonable chance at success, such as: an affiliation with Stanford’s Noah Goodman3, which we believe will help with attracting talent and funding; acceptance into the Stanford-Startx4 accelerator; and that Andreas has already done some research, application prototyping, testing, basic organizational set-up, and public talks at Stanford and USC.\n\nBudget\nOur funding is for general support. Ought intends to use it for hiring and supporting up to four additional employees between now and 2020. The hires will likely include a web developer, a research engineer, an operations manager, and another researcher.\nPlans for follow-up\nWe plan to check in annually with Ought through a phone call with Andreas as well as a review of new published results, such as online writeups, published code, and academic papers, if any. Our Program Officer and investigator for this grant, Daniel Dewey, will conduct these check-ins, accompanied by another technical advisor.\nKey questions for follow-up\nWe plan to consider the following questions when following up with Ought:\n\nHas there been any progress on hires?\nHow has research progressed?\nHow has implementation progressed?\nHow has testing progressed?\nHave there been any leads on other researchers who are noticing and/or building on your work?\nHave any significant plans changed?\n\nAdditionally, there are two situations where we might consider a renewal or expansion of funds:\n\nOught wants to make additional hires while maintaining a reasonable level of funding reserves.\nAfter 2-2.5 years, Ought would like to extend its runway while maintaining a four-person team.\n\nIn either situation, Daniel believes he would lean strongly toward renewal or increased support, provided Ought is making research progress that looks impressive to us and our technical advisors (we consider other metrics of success less important at this time).\nSources\n\n\n\nDOCUMENT\nSOURCE\n\n\nOught, Our Approach, 2018\nSource (archive)\n\n\nPaul Christiano, Directions and desiderata for AI alignment [archive only]\nSource\n\n\nStanford Computation and Cognition Lab, Homepage, December 2017 [archive only]\nSource\n\n\nStanford Computation and Cognition Lab, Noah Goodman, December 2017 [archive only]\nSource\n\n\nStanford-Startx, Homepage, December 2017 [archive only]\nSource</td>
      <td><a href="https://www.openphilanthropy.org/grants/ought-general-support-2018/">https://www.openphilanthropy.org/grants/ought-general-support-2018/</a></td>
    </tr>
    <tr>
      <td>Stanford University — NIPS Workshop on Machine Learning Security</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$6,771</td>
      <td>April 2018</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator.\n\nThe Open Philanthropy Project recommended a grant of $6,771 to Stanford University to support the Neural Information Processing System workshop “Machine Learning and Computer Security.”[1]Archived copy of link: NIPS, Program Highlights, April 2018 [archive only]\nThis is a discretionary grant and falls within our focus area of potential risks from advanced artificial intelligence.\nThe grant amount was updated in June 2020.\nSources\n\n\n\nDocument\nSource\n\n\nNIPS, Program Highlights, April 2018 [archive only]\nSource\n\n\n\n Footnotes[+]Footnotes[−] Footnotes \n 1 Archived copy of link: NIPS, Program Highlights, April 2018 [archive only]</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-nips-workshop-on-machine-learning-security/">https://www.openphilanthropy.org/grants/stanford-university-nips-workshop-on-machine-learning-security/</a></td>
    </tr>
    <tr>
      <td>AI Scholarships — Scholarship Support (2018)</td>
      <td>AI Scholarships</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$159,000</td>
      <td>February 2018</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator.\n\nDuring 2018, the Open Philanthropy Project recommended a total of approximately $159,000 over two years in scholarship support to two artificial intelligence researchers. This is an estimate because of uncertainty around future year tuition costs and currency exchange rates. The funding is intended to be used for the students’ tuition, fees, living expenses, and travel during their respective degree programs, and is part of an overall effort to grow the field of technical AI safety by supporting value-aligned and qualified early-career researchers.\nThis discretionary support falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-scholarships-scholarship-support-2018/">https://www.openphilanthropy.org/grants/ai-scholarships-scholarship-support-2018/</a></td>
    </tr>
    <tr>
      <td>Machine Intelligence Research Institute — General Support (2017)</td>
      <td>Machine Intelligence Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$3,750,000</td>
      <td>October 2017</td>
      <td>Researchers at the Machine Intelligence Research Institute. (Photo courtesy of MIRI)\n\n\n\n\n\n\n\n\nGrant investigator: Nick Beckstead\nThis page was reviewed but not written by the grant investigator. MIRI staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a grant of $3,750,000 over three years to the Machine Intelligence Research Institute (MIRI) for general support. MIRI plans to use these funds for ongoing research and activities related to reducing potential risks from advanced artificial intelligence, one of our focus areas.\nThis grant represents a renewal of and increase to our $500,000 grant recommendation to MIRI in 2016, which we made despite strong reservations about their research agenda, detailed here. In short, we saw value in MIRI’s work but decided not to recommend a larger grant at that time because we were unconvinced of the value of MIRI’s research approach to AI safety relative to other research directions, and also had difficulty evaluating the technical quality of their research output. Additionally, we felt a large grant might signal a stronger endorsement from us than was warranted at the time, particularly as we had not yet made many grants in this area.\nOur decision to renew and increase MIRI’s funding sooner than expected was largely the result of the following:\n\nWe received a very positive review of MIRI’s work on “logical induction” by a machine learning researcher who (i) is interested in AI safety, (ii) is rated as an outstanding researcher by at least one of our close advisors, and (iii) is generally regarded as outstanding by the ML community. As mentioned above, we previously had difficulty evaluating the technical quality of MIRI’s research, and we previously could find no one meeting criteria (i) – (iii) to a comparable extent who was comparably excited about MIRI’s technical research. While we would not generally offer a comparable grant to any lab on the basis of this consideration alone, we consider this a significant update in the context of the original case for the grant (especially MIRI’s thoughtfulness on this set of issues, value alignment with us, distinctive perspectives, and history of work in this area). While the balance of our technical advisors’ opinions and arguments still leaves us skeptical of the value of MIRI’s research, the case for the statement “MIRI’s research has a nontrivial chance of turning out to be extremely valuable (when taking into account how different it is from other research on AI safety)” appears much more robust than it did before we received this review.\nIn the time since our initial grant to MIRI, we have recommended several more grants within this focus area, and are therefore less concerned that a larger grant will signal an outsized endorsement of MIRI’s approach.\n\nWe are now aiming to support about half of MIRI’s annual budget. MIRI expects to use these funds mostly toward salaries of MIRI researchers, research engineers, and support staff.</td>
      <td><a href="https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2017/">https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2017/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — AI Safety Research</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,450,016</td>
      <td>October 2017</td>
      <td>Grant investigator: Daniel Dewey\nThis page was reviewed but not written by the grant investigator. UC Berkeley staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended two gifts totaling $1,450,016 to the University of California, Berkeley to support a four-year research project on AI safety. The work will be led by Professors Sergey Levine and Anca Dragan, who will each devote approximately 20% of their time to the project, with additional assistance from four graduate students. They initially intend to focus their research on how objective misspecification can produce subtle or overt undesirable behavior in robotic systems, though they have the flexibility to adjust their focus during the grant period.\nOur broad goals for this funding are to encourage top researchers to work on AI alignment and safety issues in order to build a pipeline for young researchers; to support progress on technical problems; and to generally support the growth of this area of study.\nThis funding falls within our focus area of potential risks from advanced artificial intelligence.\nSources\n\n\n\nDocument\nSource\n\n\nLevine and Dragan, Project Narrative, 2017\nSource</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research/">https://www.openphilanthropy.org/grants/uc-berkeley-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>Berkeley Existential Risk Initiative — Core Support and CHAI Collaboration</td>
      <td>Berkeley Existential Risk Initiative</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$403,890</td>
      <td>July 2017</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. BERI staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a grant of $403,890 to the Berkeley Existential Risk Initiative (BERI) to support BERI’s work with the Center for Human-Compatible AI (CHAI) at UC Berkeley. This funding is intended to help BERI hire contractors and part-time employees who will assist CHAI in a variety of ways; for example, BERI has previously provided CHAI with web development and event coordination support, and in the future BERI may hire or contract (e.g.) research engineers, software developers, or research illustrators. This funding is also intended to help support BERI’s core staff, who oversee BERI’s efforts at hiring and liaising with CHAI (and possibly with other “clients” in the future).\nOur impression is that it is often difficult for academic institutions to flexibly spend funds on technical, administrative, and other support services. We currently see BERI as valuable insofar as it can provide CHAI with these types of services, and think it’s plausible that BERI will be able to provide similar help to other academic institutions in the future.\nThis grant falls within our focus area of potential risks from advanced artificial intelligence.\n\n\n\nDocument\nSource\n\n\nBERI Grant Proposal, 2017\nSource\n\n\nBERI Budget for CHAI Collaboration, 2017\nSource</td>
      <td><a href="https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-core-support-and-chai-collaboration/">https://www.openphilanthropy.org/grants/berkeley-existential-risk-initiative-core-support-and-chai-collaboration/</a></td>
    </tr>
    <tr>
      <td>Yale University — Research on the Global Politics of AI</td>
      <td>Yale University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$299,320</td>
      <td>July 2017</td>
      <td>Grant investigator: Nick Beckstead\n\nThis page was reviewed but not written by the grant investigator. Yale University staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended two gifts totaling $299,320 to Yale University to support research on the global politics of advanced artificial intelligence. The work will be led by Assistant Professor of Political Science, Allan Dafoe, who will conduct part of the research at the Future of Humanity Institute in Oxford, United Kingdom over the next year. Funds from the two gifts will support the hiring of two full-time research assistants, travel, conferences, and other expenses related to the research efforts, as well as salary, relocation, and health insurance expenses related to Professor Dafoe’s work in Oxford.\nThis funding falls within our focus area of potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/yale-university-research-on-the-global-politics-of-ai/">https://www.openphilanthropy.org/grants/yale-university-research-on-the-global-politics-of-ai/</a></td>
    </tr>
    <tr>
      <td>Montreal Institute for Learning Algorithms — AI Safety Research</td>
      <td>Montreal Institute for Learning Algorithms</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$2,400,000</td>
      <td>July 2017</td>
      <td>Published: July 2017\n\nMILA staff reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of $2.4 million over four years to the Montreal Institute for Learning Algorithms (MILA) to support technical research on potential risks from advanced artificial intelligence (AI).\n$1.6 million of this grant will support Professor Yoshua Bengio and his co-investigators at the Université de Montréal, and $800,000 will support Professors Joelle Pineau and Doina Precup at McGill University. We see Professor Bengio’s research group as one of the world’s preeminent deep learning labs and are excited to provide support for it to undertake AI safety research.\nBackground\nThis grant falls within our work on potential risks from advanced AI, one of our focus areas within global catastrophic risks. Currently, two of our primary aims in this area are (1) to increase the amount of high-quality technical AI safety research being done, and (2) to increase the number of people who are deeply knowledgeable about both machine learning and potential risks from advanced AI. We believe we can pursue these aims both directly (by supporting this type of work) and indirectly (by supporting programs that can attract talented students to this area, can provide positive examples of AI safety work that draws on machine learning expertise, and can provide leadership for the broader machine learning community).\nThe organization\nThe Montreal Institute for Learning Algorithms (MILA) is a machine learning research group based at the Université de Montréal, led by Professors Yoshua Bengio, Pascal Vincent, Christopher Pal, Aaron Courville, Laurent Charlin, Simon Lacoste-Julien, and Jian Tang. We see MILA as one of the very top deep learning labs in academia, and among the top machine learning labs. Professors Joelle Pineau, Doina Precup, Hugo Larochelle, Alain Tapp, and Jackie Cheung are associate members of MILA.\nAbout the grant\nProposed activities\nProfessor Bengio has presented several potential AI safety research directions to us, along with some initial ideas about how he might work on them. However, we intend for Professor Bengio to have the flexibility to use our grant for whichever AI safety research projects may seem most promising in the future, rather than be restricted to projects that he has already proposed. In particular, we think it will be valuable for Professor Bengio’s students to be free to explore new ideas that they have and to talk to others in the AI safety community (such as Open Philanthropy’s technical advisors, or other grantees of ours) about which kinds of safety work may be most effective. Given that AI safety research is a relatively new area, we think it is particularly valuable to keep potential research options flexible.\nBased on discussion with our technical advisors, some portions of Professor Bengio’s currently proposed agenda appear to us quite likely to be valuable, while we have reservations about some others (see “Risks and reservations” below). However, we expect that we would consider this grant worthwhile even if Professor Bengio were to use it to pursue exactly the projects that he has already proposed.\nCase for the grant\nAmong potential grantees in the field, we believe that Professor Bengio is one of the best positioned to help build the talent pipeline in AI safety research. Our understanding, based on conversations with our technical advisors and our general impressions from the field, is that many of the most talented machine learning researchers spend some time in Professor Bengio’s lab before joining other universities or industry groups. This is an important contributing factor to our expectations for the impact of this grant, both because it increases our confidence in the quality of the research that this grant will support and because of the potential benefits for pipeline building.\nIn our conversations with Professor Bengio, we’ve found significant overlap between his perspective on AI safety and ours, and Professor Bengio was excited to be part of our overall funding activities in this area. We think that Professor Bengio is likely to serve as a valuable member of the AI safety research community, and that he will encourage his lab to be involved in that community as well. We believe that members of his lab could likely be valuable participants at future workshops on AI safety.\nBudget and room for more funding\nOur impression is that MILA is already fairly well-funded, and that its ability to use additional marginal funding is somewhat limited. Professor Bengio told us that the amount of additional yearly funding that he would be able to use productively for AI safety research is $400,000; we have decided to grant this full amount for four years ($1.6 million total). We have also granted two of Professor Bengio’s co-investigators at MILA who are also interested in working on this agenda, Professors Pineau and Precup, $200,000 per year ($800,000 total), which they estimated as the amount of funding they would be able to use productively.\nRisks and reservations\nSome of our technical advisors expressed some reservations about and offered significant feedback on Professor Bengio’s proposed research plan. We are not especially concerned about this; because AI safety is a relatively new field, we think it is reasonable to expect disagreements among researchers as to which research directions are most promising. We plan to continue having discussions with Professor Bengio and his team over the next few years in order to reach a greater degree of mutual understanding about his research agenda by the time we decide whether to renew our support in 2020.\nFollow-up expectations\nWe expect to have a conversation with Professor Bengio six months after the start of the grant, and annually after that, to discuss his projects and results, with public notes if the conversation warrants it. In the first few months of the grant, we plan to visit Montreal for several days to meet Professor Bengio’s co-investigators and discuss the project with them.\nAt the conclusion of this grant in 2020, we will decide whether to renew our support. If Professor Bengio’s research is going well (based on our technical advisors’ assessment and the impressions of others in the field), and if we have achieved a better mutual understanding with Professor Bengio about how his research is likely to be valuable, it is likely that we will decide to provide renewed funding. If Professor Bengio is using half or more of our funding to pursue research directions that we do not find particularly promising, it is likely that we would choose not to renew.\nOur process\nWe spoke with Professor Bengio and several of his students during our recent outreach to machine learning researchers and formed a positive impression of him and his work. Our technical advisors spoke highly of Professor Bengio’s capabilities, reputation, and goals.</td>
      <td><a href="https://www.openphilanthropy.org/grants/montreal-institute-for-learning-algorithms-ai-safety-research/">https://www.openphilanthropy.org/grants/montreal-institute-for-learning-algorithms-ai-safety-research/</a></td>
    </tr>
    <tr>
      <td>UCLA School of Law — AI Governance</td>
      <td>UCLA School of Law</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,536,222</td>
      <td>May 2017</td>
      <td>Grant investigator: Helen Toner\n\nThis page was reviewed but not written by the grant investigator. UCLA School of Law staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a gift of $1,536,222 to the UCLA School of Law to support a fellowship, research, and meetings on governance and policy issues related to advanced artificial intelligence. The work will be led by Professor Edward Parson and Assistant Professor Richard Re, who plan to use this funding to hire two fellows for three years to do academic research, publish papers, and hold meetings and workshops related to the topic. This gift falls within our work on potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ucla-school-of-law-ai-governance/">https://www.openphilanthropy.org/grants/ucla-school-of-law-ai-governance/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Support for Percy Liang</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,337,600</td>
      <td>May 2017</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. Stanford University staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a grant of $1,337,600 over four years (from July 2017 to July 2021) to Stanford University to support research by Professor Percy Liang and three graduate students on AI safety and alignment. The funds will be split approximately evenly across the four years (i.e. roughly $320,000 to $350,000 per year).\nThis is one of a number of grants we plan to recommend to support work by top AI researchers on AI safety and alignment issues, with the goals of a) building a pipeline for younger researchers, b) making progress on technical problems, and c) further establishing AI safety research as a field.\n \nBackground\nThis grant falls within our work on potential risks from advanced artificial intelligence, one of our focus areas within global catastrophic risks.\nWe previously recommended a $25,000 planning grant to Professor Liang in March 2017 to enable him to spend substantial time engaging with our process to determine whether to proceed with this larger funding recommendation.\nAbout the grant\nProposed activities\nWe asked Professor Liang to submit a research description of problems he currently plans to work on, why he finds these problems important, and how he thinks he might make progress. In broad terms, Professor Liang initially plans to focus on a subset of the following topics:\n\nRobustness against adversarial attacks on machine learning (ML) systems\nVerification of the implementation of ML systems\n“Knowing what you don’t know,” i.e. calibrated / uncertainty-aware ML\n“Natural language” supervision, i.e. using compositional, abstract, underspecified languages (e.g. English or some engineered language) in dialogue to specify rewards and goals\n\nProfessor Liang thinks that it is possible to make empirically verifiable progress on these topics, and that the general principles developed in this way are reasonably likely to be relevant for addressing global catastrophic risks from advanced AI (though this latter impact will be much harder to evaluate). Professor Liang also thinks that work on these topics will clarify how existing reliable ML research relates to potential risks from advanced AI, which could increase the number and quality of researchers working on potential risks from advanced AI.\nProfessor Liang plans to spend about 20% of his overall research time on the agenda supported by this grant. This grant will also support about three graduate students.\nRisks and reservations\nWe have some disagreements with Professor Liang about which problems and approaches in this area are most important and promising. These disagreements depend largely on differing intuitions about which research directions are likely to be most promising, and we can easily imagine later agreeing with Professor Liang on these issues. In one past instance, Daniel Dewey (our Program Officer for Potential Risks from Advanced Artificial Intelligence, “Daniel” throughout this page) was persuaded by Professor Liang of the likely usefulness of a line of research about which he had initially been skeptical.\nRather than trying to resolve our disagreements and settle on a fixed research agenda for this grant now, we expect it to be more valuable to keep Professor Liang’s potential research directions relatively open, and to facilitate discussion about these issues between Professor Liang, our technical advisors, other Open Philanthropy grantees, and other AI research organizations in order to move toward resolving our disagreements over time.\nOverall, we are highly confident that Professor Liang understands and shares our interests and values in this space.\nPlans for learning and follow-up\nKey questions for follow-up\n\nHow is the research going overall?\nHas Professor Liang’s team formed any new perspectives on research problems they investigate?\nHave there been any updates to the team’s research priorities?\nAre there other ways in which Open Philanthropy could help?\n\nFollow-up expectations\nWe plan to check in with Professor Liang roughly every six months for the duration of the grant to get in-depth updates on his results so far and plans for the future. We may also have less comprehensive, more informal discussions with Professor Liang roughly once a month (if both we and Professor Liang have time and think it would be beneficial).\nAt the end of the grant period, we will decide whether to renew our support based on our technical advisors’ evaluation of Professor Liang’s work so far, his proposed next steps, and our assessment of how well his research program has served as a pipeline for students entering the field. We are optimistic about the chances of renewing our support. We think the most likely reason we might choose not to renew would be if Professor Liang decides that AI alignment research isn’t a good fit for him or for his students.\nOur process\nTwo of Open Philanthropy’s technical advisors reviewed Professor Liang’s research proposal. Both felt largely positive about the proposed research directions and recommended to Daniel that Open Philanthropy make this grant, despite some disagreements with Professor Liang (and with each other) about the likely value of some specific components of the proposal (see above).</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-support-for-percy-liang/">https://www.openphilanthropy.org/grants/stanford-university-support-for-percy-liang/</a></td>
    </tr>
    <tr>
      <td>OpenAI — General Support</td>
      <td>OpenAI</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$30,000,000</td>
      <td>March 2017</td>
      <td>Published: March 2017\nWe decided to write about this grant largely because of its size and unusual structure, which initiates a partnership between Open Philanthropy and OpenAI. This page is a summary of the reasoning behind our decision to recommend the grant; it was reviewed but not written by the grant investigator.\nOpenAI staff reviewed this page prior to publication.\n\n\n\n\n\n\nThe Open Philanthropy Project recommended a grant of $30 million ($10 million per year for 3 years) in general support to OpenAI. This grant initiates a partnership between the Open Philanthropy Project and OpenAI, in which Holden Karnofsky (Open Philanthropy’s Executive Director, “Holden” throughout this page) will join OpenAI’s Board of Directors and, jointly with one other Board member, oversee OpenAI’s safety and governance work.\nWe expect the primary benefits of this grant to stem from our partnership with OpenAI, rather than simply from contributing funding toward OpenAI’s work. While we would also expect general support for OpenAI to be likely beneficial on its own, the case for this grant hinges on the benefits we anticipate from our partnership, particularly the opportunity to help play a role in OpenAI’s approach to safety and governance issues.\n1. Background\nThis grant falls within our work on potential risks from advanced artificial intelligence (AI), one of our focus areas within global catastrophic risks. We have written in detail about the case we see for funding work related to AI on our blog. As we wrote in that post, we see AI and machine learning research as being on a very short list of the most dynamic, unpredictable, and potentially world-changing areas of science. Broadly, we expect the results of continuing progress in AI research to be positive for society at large, but we see some risks (both from unintended consequences of AI use, and from deliberate misuse), and believe that we – as a philanthropic organization, separate from academia, industry, and government – may be well-placed to support work to reduce those risks.\n1.1 The organization\nOpenAI is a non-profit, founded in 2015. Its mission is to build safe AI, and ensure AI’s benefits are as widely and evenly distributed as possible. Currently, most of its work is on advancing the state of the art in technical AI research.\nOpenAI’s leadership appears to be highly value-aligned with us; this includes their focus on the societal impacts of AI and their concern about potential risks from advanced AI.\nWhen OpenAI launched, it characterized the nature of the risks – and the most appropriate strategies for reducing them – in a way that we disagreed with. In particular, it emphasized the importance of distributing AI broadly;1 our current view is that this may turn out to be a promising strategy for reducing potential risks, but that the opposite may also turn out to be true (for example, if it ends up being important for institutions to keep some major breakthroughs secure to prevent misuse and/or to prevent accidents). Since then, OpenAI has put out more recent content consistent with the latter view,2 and we are no longer aware of any clear disagreements. However, it does seem that our starting assumptions and biases on this topic are likely to be different from those of OpenAI’s leadership, and we won’t be surprised if there are disagreements in the future.\nGiven our shared values, different starting assumptions and biases, and mutual uncertainty, we expect partnering with OpenAI to provide a strong opportunity for productive discussion about the best paths to reducing AI risk. We also think the fact that OpenAI and Open Philanthropy are both located in San Francisco, and that some informal relationships already exist between Open Philanthropy and OpenAI personnel, will contribute to productive communication between our organizations.\n2. Case for the grant\nGiven our prioritization of reducing potential risks from advanced AI, we are interested in opportunities to become closely involved with any of the small number of existing organizations in “industry” (i.e. not in academia or government) that (a) are explicitly working toward the development of transformative AI; (b) are advancing the state of the art in AI research; (c) employ top AI research talent. We think it is likely that such organizations will be essential for:\n\nCreating an environment in which people can effectively do technical research on “AI safety” (i.e. work toward reducing potential risks from advanced AI). This is especially the case if groups in industry become more important, relative to groups in academia, as AI progress continues.\nRaising the general profile and legitimacy of work to reduce AI risk.\nHelping to shape discussions about policy and strategic considerations if and when it appears that transformative AI could be developed soon.\n\nWe see OpenAI and DeepMind as the two organizations currently best fitting the above description (based in large part on the views of our technical advisors). (We think it is possible that Vicarious may also fit this description, but do not feel we have enough information to be confident; in particular, Vicarious makes public substantially less information about its research and its progress than either OpenAI or DeepMind.)\nWe believe that close involvement with such an organization is likely one of the most effective avenues available to us for advancing our goal of increasing the amount of work done on reducing potential risks from advanced AI, both within the organization and outside it (the latter via increased legitimacy for safety-focused work and improved career options for leading thinkers interested in pursuing AI research that’s focused on safety). We also expect such a partnership to:\n\nImprove our understanding of the field of AI research, and give us a better sense of which interventions are most likely to be effective for reducing risks.\nImprove our ability to generically achieve our goals regarding technical AI safety research, particularly by helping us form relationships with top researchers in the field.\nBetter position us to generally promote the ideas and goals that we prioritize within AI.\n\nAs stated in the previous section, we think a partnership with OpenAI is particularly appealing due to our shared values, different starting assumptions and biases, and potential for productive communication.\nAs our views and OpenAI’s views evolve (regarding the most promising paths to reducing potential risks from advanced AI), it is likely that both we and OpenAI will be putting out further public content to share our thinking.\n2.1 Details on Open Philanthropy’s role\nHolden plans to visit OpenAI roughly once a week for all-hands and other meetings. Preliminarily, he expects to generally be an advocate for:\n\nEncouraging work on alignment research, to the extent that there is promising work to be done.\nIntensive analysis of potential future policy challenges with respect to AI (we expect to publish more on this topic in the future).\nFocusing on high-quality basic research.\n\nWe also believe we are well-positioned to help improve connections between OpenAI and groups in the risk-focused AI community, such as the Future of Humanity Institute (FHI) (which is also an Open Philanthropy grantee).\n2.2 A note on why this grant is larger than others we’ve recommended in this focus area\nWe expect that some readers will be surprised by the size of this grant, and wonder why it is so much larger than grants we’ve recommended to other groups working on potential risks from advanced AI. We think it would be prohibitively difficult to communicate our full thinking on this matter, but a few notes:\n\nAs we’ve written previously, we consider this cause to be an outstanding one, and we are generally willing to invest a lot for a relatively small chance of very large impact.\nWe think this cause will be disproportionately important if it turns out that transformative AI is developed sooner than expected (within 20 years, or perhaps even sooner). And if that happens, we think it’s fairly likely that OpenAI will be an extraordinarily important organization, with far more influence over how things play out than organizations that focus exclusively on risk reduction and do not advance the state of the art.\nWe generally feel it is very hard to make predictions about, and plans for, 10+ years from now. We think that working closely with OpenAI will put us in much better position to understand and react to a wide variety of potential situations, and that this is much more likely to result in the kind of impact we’re looking for than supporting any particular line of research (or other intervention targeting a specific scenario and specific risk) today.\nIn fact, much of our other work in this cause aims primarily to help build a general field and culture that can react to a wide variety of potential future situations, and prioritizes this goal above supporting any particular line of research. We think that OpenAI’s importance to the field of AI as a whole makes this partnership an excellent opportunity for that goal as well.\nWe are often hesitant to provide too high a proportion of a given organization’s funding, for a number of reasons.3 OpenAI has significant sources of revenue other than Open Philanthropy, and we are comfortable with the overall proportion of funding we are providing. (This is less true of other grantees in this space to date.)\n\n3. Plans for learning and follow-up\nKey questions for follow-up will include:\n\nHas our partnership resulted in concrete differences in OpenAI’s activities and/or changes in our thinking?\nDoes OpenAI still seem to be one of the few leading AI research groups, in terms of talent and likelihood of making progress toward transformative AI?\nHow well-aligned are we with OpenAI’s leadership on key issues relating to potential risks from advanced AI?\n\nWe plan to do informal reviews each year. We currently plan to do a more in-depth review to consider further renewal at the end of this three-year term. The key questions for renewal will be whether OpenAI appears to be a significant positive force for reducing potential risks from advanced AI, and/or whether our involvement is tangibly helping OpenAI move towards becoming a positive force for AI safety.\n4. Our process\nOpenAI initially approached Open Philanthropy about potential funding for safety research, and we responded with the proposal for this grant. Subsequent discussions included visits to OpenAI’s office, conversations with OpenAI’s leadership, and discussions with a number of other organizations (including safety-focused organizations and AI labs), as well as with our technical advisors.\n5. Relationship disclosures\nOpenAI researchers Dario Amodei and Paul Christiano are both technical advisors to Open Philanthropy and live in the same house as Holden. In addition, Holden is engaged to Dario’s sister Daniela.\n6. Sources\n\n\n\nDOCUMENT\nSOURCE\n\n\nOpenAI, “Introducing OpenAI”\nSource (archive)\n\n\nOpenAI, “Mission”\nSource (archive)\n\n\nSlate Star Codex, “Should AI Be Open?”\nSource (archive)</td>
      <td><a href="https://www.openphilanthropy.org/grants/openai-general-support/">https://www.openphilanthropy.org/grants/openai-general-support/</a></td>
    </tr>
    <tr>
      <td>Distill Prize for Clarity in Machine Learning — General Support</td>
      <td>Distill Prize for Clarity in Machine Learning</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$25,000</td>
      <td>March 2017</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. Distill staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a grant of $25,000 to support the Distill Prize for Clarity in Machine Learning, which will be awarded for clear explanations of concepts related to machine learning. The Open Philanthropy Project will also be administering the prize.\nThis grant is part of the Distill Prize’s total initial endowment of $125,000, which is also funded by Chris Olah, Greg Brockman, Jeff Dean, and DeepMind. We see this grant as an opportunity to increase the volume of work being done on a problem that we believe to be important but which is often not institutionally supported.\nWe see several possible benefits that could result from the prize:\n\nFostering a culture of deeply and fully understanding how machine learning systems work, which we expect could reduce the likelihood of undesirable side effects.\nHelping the Open Philanthropy Project to find and build relationships with machine learning researchers who can think and write clearly about how machine learning systems work. We expect that these researchers may also be more open to thinking and writing clearly about potential risks from advanced artificial intelligence (AI), one of our focus areas.\nWe also believe that explaining machine learning clearly could improve the interpretability and transparency of machine learning over time, which could help to mitigate risks from advanced AI, though this was not a major consideration in our decision to make this grant.\n\nWithout our funding, we estimate that there is a 60% chance that the prize would be administered at the same level of quality, a 30% chance that it would be administered at lower quality, and a 10% chance that it would not move forward at all. We believe that our assistance in administering the prize will also be of significant help to Distill.</td>
      <td><a href="https://www.openphilanthropy.org/grants/distill-prize-for-clarity-in-machine-learning-general-support/">https://www.openphilanthropy.org/grants/distill-prize-for-clarity-in-machine-learning-general-support/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Percy Liang Planning Grant</td>
      <td>Stanford University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$25,000</td>
      <td>March 2017</td>
      <td>Grant investigator: Daniel Dewey\n\nThis page was reviewed but not written by the grant investigator. Stanford University staff also reviewed this page prior to publication.\nThe Open Philanthropy Project recommended a planning grant of $25,000 to Professor Percy Liang at Stanford University. This grant was recommended to enable Professor Liang to spend significant time engaging in our process to determine whether to provide his research group with a much larger grant. We did end up recommending that larger grant, which we have written about in more detail here.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-percy-liang-planning-grant/">https://www.openphilanthropy.org/grants/stanford-university-percy-liang-planning-grant/</a></td>
    </tr>
    <tr>
      <td>Future of Humanity Institute — General Support</td>
      <td>Future of Humanity Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,995,425</td>
      <td>March 2017</td>
      <td>Published: March 2017\n\nFuture of Humanity Institute staff reviewed this page prior to publication.\n\nThe Open Philanthropy Project recommended a grant of £1,620,452 ($1,995,425 at the time of conversion) in general support to the Future of Humanity Institute (FHI). FHI plans to use this grant primarily to increase its reserves and to make a number of new hires; this grant also conceptually encompasses an earlier grant we recommended to support FHI in hiring Dr. Piers Millett for research on biosecurity and pandemic preparedness, and this page describes the case for both this grant and that earlier grant.\n1. Background\nThe Future of Humanity Institute (FHI) is a research center based at the University of Oxford that focuses on strategic analysis of existential risks, especially potential existential risks from advanced artificial intelligence (AI). This grant fits into our potential risks from advanced AI focus area and within the category of global catastrophic risks in general, and is also relevant to our interest in growing and empowering the effective altruism community.\n2. About the grant\n2.1 Budget and room for more funding\nFHI plans to use £1 million of this grant to increase its unrestricted reserves from about £327,000 to £1.327 million. It plans to use the remainder to support new junior staff members.\nFHI’s annual revenues and expenses are currently around £1 million per year, and the bulk of its funding is from academic grants (which are lumpy and hard to predict). It seems to us that having a little over a year of unrestricted reserves will help with FHI’s planning. For example, it might allow FHI to make potential staff members offers that are not conditional on whether FHI receives a grant it has applied for, or to promise staff currently funded by other grants that there will be funding available to support their work when one source of grant funding has ended and another has not yet begun. Much less of this would be possible at £327,000 in unrestricted funds.\nFHI’s other current funders include:\n\nThe Future of Life Institute\nThe European Research Council\nThe Leverhulme Trust\nSeveral individual donors\n\n2.2 Case for the grant\nNick Beckstead, who investigated this grant, (“Nick” throughout this page) sees FHI as having a number of strengths as an organization:\n\nNick believes that Professor Nick Bostrom, Director of FHI, is a particularly original and insightful thinker on the topics of AI, global catastrophic risks, and technology strategy in general. Nick views Professor Bostrom’s book, Superintelligence, as FHI’s most significant output so far and the best strategic analysis of potential risks from advanced AI to date. Nick’s impression is that Superintelligence has helped to raise awareness of potential risks from advanced AI. Nick considers the possibility that this grant will boost Professor Bostrom’s output to be a key potential benefit.\nNick finds FHI’s researchers in general to have impressive breadth, values aligned with effective altruism, and philosophical sophistication.\nFHI collaborates with Google DeepMind on technical work focused on addressing potential risks from advanced AI. We believe this is valuable because DeepMind is generally considered (including by us) to be one of the leading organizations in AI development.\nFHI constitutes a shovel-ready opportunity to support work on potential risks from advanced AI.\n\nWe believe that hiring Dr. Millett will address what we view as a key staffing need for FHI (expertise in biosecurity and pandemic preparedness). We have also been pleased to see that some of FHI’s more recent hires have backgrounds in machine learning.\nWe are not aware of other groups with a comparable focus on, and track record of, exploring strategic questions related to potential risks from advanced AI. We think FHI is one of the most generally impressive (in terms of staff and track record) organizations with a strong focus on effective altruism.\n2.3 Risks and reservations\nWe have some concerns about FHI as an organization:\n\nOur impression is that FHI’s junior researchers operate without substantial attention from a “principal investigator”-type figure. While giving researchers independence in this way may have benefits, we also believe that researchers might select better projects or execute them more effectively with additional guidance.\nIt seems to us that a substantial fraction of FHI’s most impactful work is due to Professor Nick Bostrom. Since Professor Bostrom’s own work is already funded and since he offers relatively limited guidance to junior research staff, the impact of additional funding may scale strongly sublinearly. (Our understanding is that Professor Bostrom’s allocation of attention is a deliberate choice, and does not necessarily seem unreasonable to us.)\nFHI has relatively limited experience with policy analysis and advocacy.\nOur impression is that FHI’s technical proficiency in machine learning and biotechnology is somewhat limited, which we believe may reduce its credibility when writing about these topics and/or cause it to overlook important points in these areas. We are optimistic that recent and forthcoming hires, discussed above, will be helpful on this front.\n\n3. Plans for learning and follow-up\n3.1 Key questions for follow-up\n\nWhat new staff has FHI hired, and what have they produced?\nWhat has been the most important research output from FHI’s staff?\nHave FHI’s additional reserves been useful, and if so, how?\nHow are FHI’s collaborations with industrial AI labs going?\nHas FHI had success applying for other grants?\nWhat has Dr. Millett produced during his time at FHI?</td>
      <td><a href="https://www.openphilanthropy.org/grants/future-of-humanity-institute-general-support/">https://www.openphilanthropy.org/grants/future-of-humanity-institute-general-support/</a></td>
    </tr>
    <tr>
      <td>AI Impacts — General Support (2016)</td>
      <td>AI Impacts</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$32,000</td>
      <td>December 2016</td>
      <td>AI Impacts researchers John Salvatier (left) and Katja Grace. (Photo courtesy of AI Impacts)\n\n\nPublished: February 2017\nThe Open Philanthropy Project recommended a grant of $32,000 to AI Impacts for general support. AI Impacts plans to use this grant to work on strategic questions related to potential risks from advanced artificial intelligence.</td>
      <td><a href="https://www.openphilanthropy.org/grants/ai-impacts-general-support-2016/">https://www.openphilanthropy.org/grants/ai-impacts-general-support-2016/</a></td>
    </tr>
    <tr>
      <td>Electronic Frontier Foundation — Artificial Intelligence Scenarios and Social Impacts</td>
      <td>Electronic Frontier Foundation</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$199,000</td>
      <td>November 2016</td>
      <td>Published: December 2016\nThe Open Philanthropy Project recommended a grant of $199,000 to the Electronic Frontier Foundation to enable it to carry out some preliminary work on policy-relevant questions related to progress in artificial intelligence, led by Peter Eckersley. We believe that Dr. Eckersley’s background, which includes policy and advocacy work on technology-related topics, makes him a good candidate to do this type of work.</td>
      <td><a href="https://www.openphilanthropy.org/grants/electronic-frontier-foundation-artificial-intelligence-scenarios-and-social-impacts/">https://www.openphilanthropy.org/grants/electronic-frontier-foundation-artificial-intelligence-scenarios-and-social-impacts/</a></td>
    </tr>
    <tr>
      <td>Machine Intelligence Research Institute — General Support (2016)</td>
      <td>Machine Intelligence Research Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$500,000</td>
      <td>August 2016</td>
      <td>Researchers at the Machine Intelligence Research Institute. (Photo courtesy of MIRI)\n\n\n\n\n\n\n\n\n\n\n\nWe decided to write about this grant at some length, because we and others have found it challenging to assess MIRI’s work, and we believe there will be substantial interest – from donors and potential donors – in a frank assessment of it. This page is a summary of the reasoning behind our decision to recommend the grant; it was reviewed but not written by the grant investigator(s).\nMachine Intelligence Research Institute staff reviewed this page prior to publication.\n\n\n\n\n\n\n\nThe Open Philanthropy Project recommended a grant of $500,000 to the Machine Intelligence Research Institute (MIRI), an organization doing technical research intended to reduce potential risks from advanced artificial intelligence. We made this grant despite our strong reservations about MIRI’s research, in light of other considerations detailed below.\nWe found MIRI’s work especially difficult to evaluate, so we set up a fairly extensive review process for five of MIRI’s best (according to MIRI) papers/results produced in 2015-2016. These papers/results were all concerned with MIRI’s “Agent Foundations” research agenda,1 which has been the primary focus of MIRI’s research so far. This process included reviews and extensive discussion by several of our technical advisors, who assessed both the research topics’ relevance to reducing potential risks and the pace of progress that had been made on the topics; we also commissioned reviews from eight academics to help inform the latter topic.\nBased on that review process, it seems to us that (i) MIRI has made relatively limited progress on the Agent Foundations research agenda so far, and (ii) this research agenda has little potential to decrease potential risks from advanced AI in comparison with other research directions that we would consider supporting. We view (ii) as particularly tentative, and some of our advisors thought that versions of MIRI’s research direction could have significant value if effectively pursued. In light of (i) and (ii), we elected not to recommend a grant of $1.5 million per year over the next two years, which would have closed much of MIRI’s funding gap and allowed it to hire 4-6 additional full-time researchers. This page does not contain the details of our evaluation of MIRI’s Agent Foundations research agenda, only high-level takeaways. We plan to write more about the details in the future and incorporate that content into this page.\nDespite our strong reservations about the technical research we reviewed, we felt that recommending $500,000 was appropriate for multiple reasons, including the following:\n\nWe see our evaluation of MIRI’s research direction as uncertain, in light of the fact that MIRI was working on technical research around potential risks from advanced AI for many years while few others were, and it is difficult to find people who are clearly qualified to assess its work. If MIRI’s research is higher-potential than it currently seems to us, there could be great value in supporting MIRI, especially since it is likely to draw less funding from traditional sources than most other kinds of research we could support. We think this argument is especially important in light of the fact that we consider potential risks from advanced AI to be an outstanding cause, and that there are few people or organizations working on it full-time.\nWe believe funding MIRI may increase the supply of technical people interested in potential risks from advanced AI and the diversity of problems and approaches considered by such researchers.\nWe see a possibility that MIRI’s research could improve in the near future, particularly because some research staff are now pursuing a more machine learning-focused research agenda.2\nWe believe that MIRI has had positive effects (independent of its technical research) in the past that would have been hard for us to predict, and has a good chance of doing so again in the future. For example, we believe MIRI was among the first to articulate the value alignment problem in great detail.\nMIRI constitutes a relatively “shovel-ready” opportunity to support work on potential risks from advanced AI because it is specifically focused on that set of issues and has room for more funding.\nThere are a number of other considerations. In particular, senior staff members at MIRI spent a considerable amount of time participating in our review process, and we feel that a “participation grant” is warranted in this context. (This reasoning is only part of our thinking, and would not justify the full amount of the grant; however, note that we believe MIRI spent several times as much capacity on our process as nonprofits typically do when they receive participation grants from GiveWell). Additionally, as we ramp up our involvement in the area of potential risks from advanced AI, we expect to ask for substantially more time from MIRI staff.\n\nThere is a strong chance we will renew this grant next year.\nThe judgments and decisions that use “we” language in this page primarily refer to the opinions of Nick Beckstead (Program Officer, Scientific Research), Daniel Dewey (Program Officer, Potential Risks from Advanced Artificial Intelligence), and Holden Karnofsky (Executive Director).\n1. Background and process\nThis grant falls within our work on potential risks from advanced artificial intelligence (AI), one of our focus areas within global catastrophic risks. We wrote more about this cause on our blog.\n1.1 The organization\nThe Machine Intelligence Research Institute (MIRI) is a nonprofit working on computer science and mathematics research intended to reduce potential risks from advanced AI.\nMIRI was founded in 2000 as the Singularity Institute for Artificial Intelligence (SIAI), with the mission to “help humanity prepare for the moment when machine intelligence exceeds human intelligence.” Our understanding is that for several years SIAI was primarily focused on articulating and communicating problems of AI safety, by writing public content, influencing intellectuals, and co-hosting the Singularity Summit.3 In 2013, SIAI changed its name to MIRI and shifted its primary focus to conducting technical research, pursuing a highly theoretical “Agent Foundations” research agenda.4 In May 2016, MIRI announced5 that it would be pursuing a machine learning research agenda6 alongside the original agenda.\nOpen Philanthropy Project staff have been engaging in informal conversations with MIRI for a number of years. These conversations contributed to our decision to investigate potential risks from advanced AI and eventually make it one of our focus areas. For more details on these early conversations, please refer to our shallow investigation.\nWe consider MIRI to be a part of the “effective altruism” community. It is not a part of mainstream academia. Because MIRI’s research priorities are unusual and its work does not always fall within any specific academic subfield, there are relatively few people who we feel clearly have the right context to evaluate its technical research. For this reason, we and others7 in the effective altruism community have found it challenging to assess MIRI’s impact.\n1.2 Our investigation process\nNick Beckstead, Program Officer for Scientific Research, was the primary investigator for this grant. Daniel Dewey, Program Officer for Potential Risks from Advanced Artificial Intelligence, also did a substantial amount of investigation for this grant, particularly in evaluating the quality of MIRI’s research agenda.\nWe attempted to assess MIRI’s research primarily through detailed reviews of individual technical papers. MIRI sent us five papers/results which it considered particularly noteworthy from the last 18 months:\n\nBenya Fallenstein and Ramana Kumar. 2015. “Proof-Producing Reflection for HOL: With an Application to Model Polymorphism.” In Interactive Theorem Proving: 6th International Conference, ITP 2015, Nanjing, China, August 24-27, 2015, Proceedings. Springer.8\nVadim Kosoy. 2015. “Optimal Predictors: A Bayesian Notion of Approximation Algorithms.” Unpublished draft.\nScott Garrabrant, Benya Fallenstein, Abram Demski, and Nate Soares. 2016. “Inductive Coherence.” arXiv:1604.05288 [cs.AI]. Previously published as “Uniform Coherence.”9\nScott Garrabrant, Nate Soares, and Jessica Taylor. 2016. “Asymptotic Convergence in Online Learning with Unbounded Delays.” arXiv:1604.05280 [cs.LG].10\n2016. Unpublished result on “logical induction.”\n\nPapers 1, 3, and 4 were completed works, Paper 2 was an unpublished work in progress, and Result 5 was an unpublished result that was presented in person. This selection was somewhat biased in favor of newer staff, at our request; we felt this would allow us to better assess whether a marginal new staff member would make valuable contributions. Additionally, older works may have included collaborations between MIRI and Paul Christiano, one of our technical advisors, and we wanted to minimize any confusion or conflicts of interest that may have caused.\nAll of the papers/results fell under a category MIRI calls “highly reliable agent design”. Four of them were concerned with “logical uncertainty” — the challenge of assigning “reasonable” subjective probabilities to logical statements that are too computationally expensive to formally verify. One was concerned with “reflective reasoning” — the challenge of designing a computer system that can reason “reliably” about computations similar or identical to its own computations.\nPapers 1-4 were each reviewed in detail by two of four technical advisors (Paul Christiano, Jacob Steinhardt, Christopher Olah, and Dario Amodei). We also commissioned seven computer science professors and one graduate student with relevant expertise as external reviewers. Papers 2, 3, and 4 were reviewed by two external reviewers, while Paper 1 was reviewed by one external reviewer, as it was particularly difficult to find someone with the right background to evaluate it. Result 5 did not receive an external review because the result had not been written up and, at the time we were commissioning external reviews, MIRI asked us to keep the result confidential. However, the result was presented to Daniel Dewey and Paul Christiano, and they wrote reviews of the result for us. MIRI is now discussing the result publicly, though it has yet to be released as a finished paper.\nWe have made all external reviews of the published work (Papers 1, 3, and 4) public, although the reviewers’ names were kept anonymous. Of the four technical advisors named above, three provided permission to publish anonymized versions of their reviews of the published work. A consolidated document containing all public reviews can be found here.\nIn addition to these technical reviews, Daniel Dewey independently spent approximately 100 hours attempting to understand MIRI’s research agenda, in particular its relevance to the goals of creating safer and more reliable advanced AI. He had many conversations with MIRI staff members as a part of this process.\nOnce all the reviews were conducted, Nick, Daniel, Holden, and our technical advisors held a day-long meeting to discuss their impressions of the quality and relevance of MIRI’s research.\nIn addition to this review of MIRI’s research, Nick Beckstead spoke with MIRI staff about MIRI’s management practices, staffing, and budget needs.\n2. Our impression of MIRI’s Agent Foundations research\nWhile we are not confident we fully understand MIRI’s research, we currently have the impression that (i) MIRI has made relatively limited progress on the Agent Foundations research agenda so far, and (ii) this research agenda has limited potential to decrease potential risks from advanced AI in comparison with other research directions that we would consider supporting. We view (ii) as particularly tentative, and some of our advisors thought that versions of MIRI’s research direction could have significant value if effectively pursued. This page does not summarize the details of our reasoning on points (i) and (ii) or the details of our reasoning on the two questions listed below. We plan to write more about that in the future and add it to this page.\nThrough technical reviews and the subsequent discussion, we attempted to answer two key questions about MIRI’s research:\n2.1 How relevant is MIRI’s Agent Foundations research agenda?\nOur technical advisors generally didn’t believe that solving the problems outlined in MIRI’s Agent Foundations research agenda11 would be crucial for reducing potential risks from advanced AI. Some felt that it could be beneficial to solve these problems, but it would be difficult to make progress on them. There was a strong consensus that this work is especially unlikely to be useful in the case that transformative AI is developed within the next 20 years through deep-learning methods. We did not thoroughly review MIRI’s second, machine learning-focused research agenda, because at the time of our investigation very little work had been done on it.\n2.2 How much progress has MIRI made on its Agent Foundations agenda?\nOverall, both internal and external reviewers felt that the reviewed work was technically nontrivial, but unimpressive. (Paper 1, Fallenstein and Kumar 2015, was a notable exception. The reviewer of that paper suggested it was technically impressive work that a relatively limited number of computer scientists would be in a position to do. Paper 2 received very mixed reviews.) Note that MIRI predicted in advance that internal and external reviewers were relatively unlikely to be impressed by their work without substantially more back-and-forth than the reviewers had time for, and that this would likely be the case whether or not MIRI had made substantial progress. We did not see a compelling alternative way to address the question in addition to other approaches we were already pursuing, such as speaking at length with MIRI staff members and other researchers concerned with AI safety. MIRI’s advance predictions about the review process can be found here,12 along with post-review comments by MIRI that were drafted after the completion of our grant decision process.\nWe asked our technical advisors to help us get a sense for the overall aggregate productivity represented by these papers. One way of summarizing our impression of this conversation is that the total reviewed output is comparable to the output that might be expected of an intelligent but unsupervised graduate student over the course of 1-3 years. Our technical advisors felt that the distinction between supervised and unsupervised work was particularly important in this context, and that a supervised graduate student would be substantially more productive over that time frame.\n3. About the grant\n3.1 Budget and room for more funding\nMIRI operates on a budget of approximately $2 million per year. At the time of our investigation, it had between $2.4 and $2.6 million in reserve. In 2015, MIRI’s expenses were $1.65 million, while its income was slightly lower, at $1.6 million. Its projected expenses for 2016 were $1.8-2 million. MIRI expected to receive $1.6-2 million in revenue for 2016, excluding our support.\nNate Soares, the Executive Director of MIRI, said that if MIRI were able to operate on a budget of $3-4 million per year and had two years of reserves, he would not spend additional time on fundraising. A budget of that size would pay for 9 core researchers, 4-8 supporting researchers, and staff for operations, fundraising, and security.\nAny additional money MIRI receives beyond that level of funding would be put into prizes for open technical questions in AI safety. MIRI has told us it would like to put $5 million into such prizes.\n3.2 Case for the grant\nIf we had decided to pursue maximal growth for MIRI, we would have recommended a grant of approximately $1.5 million per year, and would likely have committed to two years of support. We decided against this option primarily because of our strong reservations about MIRI’s Agent Foundations research (see above). Despite this, we felt that recommending $500,000 was more appropriate than recommending no grant, for the following reasons:\n3.2.1 Uncertainty about our technical assessment\nWe see our evaluation of MIRI’s research direction as uncertain, in light of the fact that MIRI was working on technical research around potential risks from advanced AI for many years while few others were, and it is difficult to find people who are clearly qualified to assess its work. We note that among our technical advisors, Paul Christiano has been thinking about relevant topics for the longest, and he tended to be the most optimistic about MIRI’s work. His relative optimism may come from having a deeper understanding of the value alignment problem and a better sense of how MIRI’s work may be able to address it.\nIf MIRI’s research is higher-potential than it currently seems to us, there could be great value in supporting MIRI, especially since it is likely to draw less funding from traditional sources than most other kinds of research we could support. We think this argument is especially important in light of the fact that we consider potential risks from advanced AI to be an outstanding cause, and that there are few people or organizations working on it full-time.\nWe expect that in the coming years, MIRI will have more opportunities to make the case for the value of its research, and general interest in relevant research will grow. We are unlikely to continue renewing support to MIRI 2-3 years from now if we do not see a stronger case that its research is valuable.\n3.2.2 Increasing research supply and diversity\nOverall, it seems to us that supporting MIRI will increase the total supply of technical researchers who are very thoughtful about safe implementation of AI, while generally not drawing those researchers away from institutions where (we would guess) safety researchers would be likely to have greater positive impact, such as top AI labs. (It remains to be seen what safety work will be produced at these labs. The previous comment is primarily based on our and our technical advisors’ intuitions that it seems most promising for safety research to be closely coupled with cutting-edge capabilities research.) We find it quite valuable to increase the total supply of researchers because we believe technical research on potential risks from advanced AI may be one of the most important and neglected causes we are aware of.\nWe believe MIRI’s approach and reasoning is highly unusual compared to that exhibited in other AI-safety-relevant work. Specifically, MIRI strikes us as assigning an unusually high probability to catastrophic accidents and as being pessimistic about the difficulty of implementing robust and general safety measures. We believe it is likely beneficial for some people in the field to be focused on understanding the ways standard approaches could go wrong, which may be something MIRI is especially well-suited to do. In general, it seems valuable to promote this kind of intellectual diversity in the field.\n3.2.3 Potential for improvement\nMIRI’s research program is still fairly new, since it only shifted to doing full-time technical research in 2013, and most of its research staff are somewhat recent hires. MIRI feels its most recent result (the unpublished Result 5) is more impressive than its older work, but we will have substantial uncertainty on this point until the work has been written up and we have had a chance to review it more thoroughly.\nIn May 2016, MIRI announced13 that it would be splitting its research program, with a significant fraction of its time spent on the design of safe systems descended from present-day approaches in machine learning. We are not sure what to expect on this front. MIRI may have less of a comparative advantage when doing work that overlaps more with standard machine learning research. However, there is a possibility that we would evaluate research following this new agenda more positively than research following its Agent Foundations agenda.\nWhile we do not see a strong argument for the relevance and technical impressiveness of MIRI’s existing research, it is possible that such an argument will emerge over the next few years. If that happens, we would likely increase our support for MIRI.\n3.2.4 Early articulation of the value alignment problem\nWe believe that MIRI played an important role in publicizing and sharpening the value alignment problem. This problem is described in the introduction to MIRI’s Agent Foundations technical agenda.14 We are aware of MIRI writing about this problem publicly and in-depth as early as 2001, at a time when we believe it received substantial attention from very few others. While MIRI was not the first to discuss potential risks from advanced artificial intelligence,15 we believe it was a relatively early and prominent promoter, and generally spoke at more length about specific issues such as the value alignment problem than more long-standing proponents.\nThere was a general consensus among our technical advisors and grant decision-makers that MIRI should receive some amount of support in recognition of these contributions.\n3.2.5 Other considerations\n\nMIRI has inspired, assisted, and/or incubated a number of other researchers and effective altruists that seem to us to be doing useful work. Additionally, two of our grantees, the Center for Applied Rationality and SPARC, received substantial initial support from MIRI. When considering these contributions, as well as MIRI’s early articulation of the value alignment problem, we think there is a case that MIRI will produce further positive impact in a way that is difficult to anticipate.\nMIRI seems particularly well-aligned with our values, particularly with respect to effective altruism.\nMIRI constitutes a relatively “shovel-ready” opportunity to support work on potential risks from advanced AI because it is specifically focused on that set of issues and has room for more funding.\nSenior staff members at MIRI spent a considerable amount of time participating in our review process, and we feel that a “participation grant” is warranted in this context. (This reasoning is only part of our thinking, and would not justify the full amount of the grant; however, note that we believe MIRI spent several times as much capacity on our process as nonprofits typically do when they receive participation grants from GiveWell).\nAs we ramp up our involvement in the area of potential risks from advanced AI, we expect to ask for substantially more time from MIRI staff, both to get advice on pitfalls we might not otherwise consider, and to share and address some of our concerns about its activities (see next section).\n\n3.3 Risks and reservations\n\nAlthough we list multiple points in favor of MIRI above, MIRI’s core work is its technical research. We remain unconvinced of the value of this work despite putting a great deal of effort into trying to understand the case for it. This is a major reservation for us.\nWe are not confident that MIRI will add value pursuing its new, machine learning-focused research direction. It may have less of a comparative advantage when doing work that overlaps more with standard machine learning research.\nWe believe that MIRI often communicates in relatively idiosyncratic and undiplomatic ways, which can cause problems. We believe MIRI has communicated about the value-alignment problem in a way that has often caused mainstream AI researchers to be more dismissive of it, or reluctant to work on technical research to reduce potential risks from advanced AI for fear of being associated with MIRI and its ideas. We also feel there are instances of unprofessional behavior by MIRI staff that have posed similar risks. We see the potential for more negative impact along these lines in the future.\nWe are concerned that MIRI has only limited engagement with mainstream academia, and in particular very little hands-on experience with machine learning, with the exception of one recently-hired staff member (Jessica Taylor). We find this problematic because our intuition is that research into potential risks from advanced AI is likely to be most effective when closely coupled with cutting-edge machine learning research.\n\n3.4 Size of the grant\nWe had difficulty deciding on a grant size. In light of the arguments both for and against MIRI’s contributions, we felt a case could be made for any figure between $0 and $1.5 million per year (the latter being enough that MIRI would no longer prioritize fundraising and would expand core staff as fast as possible, as discussed above). We ultimately settled on a figure that we feel will most accurately signal our attitude toward MIRI. We feel $500,000 per year is consistent with seeing substantial value in MIRI while not endorsing it to the point of meeting its full funding needs. This amount is similar to what we expect to recommend to higher-end (although not the highest-end) academic grantees in this space in the future. Note that this does not mean that we believe the value of MIRI’s research alone is equivalent to that of higher-end academics in the field; we think that MIRI’s other positive impact detailed above contributes substantially to its overall value.\n4. Plans for follow-up\nAs of now, there is a strong chance that we will renew this grant next year. We believe that most of our important open questions and concerns are best assessed on a longer time frame, and we believe that recurring support will help MIRI plan for the future.\nTwo years from now, we are likely to do a more in-depth reassessment. In order to renew the grant at that point, we will likely need to see a stronger and easier-to-evaluate case for the relevance of the research we discuss above, and/or impressive results from the newer, machine learning-focused agenda, and/or new positive impact along some other dimension.\n5. Sources\n\n\n\nDOCUMENT\nSOURCE\n\n\nCenter for Applied Rationality home page\nSource (archive)\n\n\nEffective Altruism Forum, “Let’s conduct a survey on the quality of MIRI’s implementation”\nSource (archive)\n\n\nFallenstein & Kumar 2015\nSource (archive)\n\n\nGarrabrant et al. 2016a\nSource (archive)\n\n\nGarrabrant et al. 2016b\nSource (archive)\n\n\nMIRI Technical Research Agenda\nSource (archive)\n\n\nMIRI, “A new MIRI research program with a machine learning focus”\nSource (archive)\n\n\nMIRI, “Assessing our past and potential impact”\nSource (archive)\n\n\nMIRI, Comments on Open Philanthropy Reviews\nSource (archive)\n\n\nMIRI, Singularity Summit\nSource (archive)\n\n\nOpen Philanthropy Project, Anonymized Reviews of Three Recent Papers from MIRI’s Agent Foundations Research Agenda\nSource\n\n\nSPARC home page\nSource (archive)\n\n\nTaylor et al. 2016\nSource (archive)</td>
      <td><a href="https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2016/">https://www.openphilanthropy.org/grants/machine-intelligence-research-institute-general-support-2016/</a></td>
    </tr>
    <tr>
      <td>UC Berkeley — Center for Human-Compatible AI (2016)</td>
      <td>University of California, Berkeley</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$5,555,550</td>
      <td>August 2016</td>
      <td>Photo courtesy of UC Berkeley\n \nUC Berkeley staff reviewed this page prior to publication.\n\n\nThe Open Philanthropy Project recommended a grant of $5,555,550 over five years to UC Berkeley to support the launch of a Center for Human-Compatible Artificial Intelligence (AI), led by Professor Stuart Russell.\nWe believe the creation of an academic center focused on AI safety has significant potential benefits in terms of establishing AI safety research as a field and making it easier for researchers to learn about and work on this topic.\n1. Background\nThis grant falls within our work on potential risks from advanced artificial intelligence, one of our focus areas within global catastrophic risks. We wrote more about this cause on our blog.\nStuart Russell is Professor of Computer Science and Smith-Zadeh Professor in Engineering at the University of California, Berkeley. He is the co-author of Artificial Intelligence: A Modern Approach, which we understand to be one of the most widely-used textbooks on AI.\n2. About the grant\nThis grant will support the establishment of the Center for Human-Compatible AI at UC Berkeley, led by Professor Russell with the following co-Principal Investigators and collaborators:\n\nPieter Abbeel, Associate Professor of Computer Science, UC Berkeley\nAnca Dragan, Assistant Professor of Computer Science, UC Berkeley\nTom Griffiths, Professor of Psychology and Cognitive Science, UC Berkeley\nBart Selman, Professor of Computer Science, Cornell University\nJoseph Halpern, Professor of Computer Science, Cornell University\nMichael Wellman, Professor of Computer Science, University of Michigan\nSatinder Singh Baveja, Professor of Computer Science, University of Michigan\n\nResearch topics that the Center may focus on include:\n\nValue alignment through, e.g., inverse reinforcement learning from multiple sources (such as text and video).\nValue functions defined by partially observable and partially defined terms (e.g. “health,” “death”).\nThe structure of human value systems, and the implications of computational limitations and human inconsistency.\nConceptual questions including the properties of ideal value systems, tradeoffs among humans and long-term stability of values.\n\nWe see the creation of this center as an important component of our efforts to help build the field of AI safety for several reasons:\n\nWe expect the existence of the Center to make it much easier for researchers interested in exploring AI safety to discuss and learn about the topic, and potentially consider focusing their careers on it. Ideally this will result in a larger number of researchers ending up working on topics related to AI safety than otherwise would have.\nThe Center may allow researchers already focused on AI safety to dedicate more of their time to the topic and produce higher-quality research.\nWe hope that the existence of a well-funded academic center at a major university will solidify the place of this work as part of the larger fields of machine learning and artificial intelligence.\n\nBased on our in-progress investigation of field-building, our impression is that funding the creation of new academic centers is a very common part of successful philanthropic efforts to build new fields.\nWe also believe that supporting Professor Russell’s work in general is likely to be beneficial. He appears to us to be more focused on reducing potential risks of advanced artificial intelligence (particularly the specific risks we are most focused on) than any comparably senior, mainstream academic of whom we are aware. We also see him as an effective communicator with a good reputation throughout the field.\n2.1 Budget and room for more funding\nProfessor Russell estimates that the Center could, if funded fully, spend between $1.5 million and $2 million in its first year and later increase its budget to roughly $7 million per year.\nProfessor Russell currently has a few other sources of funding to support his own research and that of his students (all amounts are approximate):\n\n$340,000 from the Future of Life Institute\n$280,000 from the Defense Advanced Research Projects Agency\n$1,500,000 from the Leverhulme Trust (spread over 10 years)\n\nOur understanding is that most of this funding is already or will soon be accounted for, and that Professor Russell would not plan to announce a new Center of this kind without substantial additional funding. Professor Russell has also applied for a National Science Foundation Expedition grant, which would be roughly $7 million to $10 million over ten years. However, because we do not expect that decision to be made until at least a few months after the final deadline for proposals in January 2017, and because we understand those grants to be very competitive, we decided to set our level of funding without waiting for that announcement.\nWe are not aware of other potential funders who would consider providing substantial funding to the Center in the near future, and we believe that having long-term support in place is likely to make it easier for Professor Russell to recruit for the Center.\n2.2 Internal forecasts\nWe’re experimenting with recording explicit numerical forecasts of events related to our decisionmaking (especially grantmaking). The idea behind this is to pull out the implicit predictions that are playing a role in our decisions, and make it possible for us to look back on how well-calibrated and accurate those are. For this grant, we are recording the following forecast:\n\n50% chance that, two years from now, the Center will be spending at least $2 million a year, and will be considered by one or more of our relevant technical advisors to have a reasonably good reputation in the field.\n\n3. Our process\nWe have discussed the possibility of a grant to support Professor Russell’s work several times with him in the past. Following our decision earlier this year to make this focus area a major priority for 2016, we began to discuss supporting a new academic center at UC Berkeley in more concrete terms.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2016/">https://www.openphilanthropy.org/grants/uc-berkeley-center-for-human-compatible-ai-2016/</a></td>
    </tr>
    <tr>
      <td>George Mason University — Research into Future Artificial Intelligence Scenarios</td>
      <td>George Mason University</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$277,435</td>
      <td>June 2016</td>
      <td>Professor Hanson reviewed this page prior to publication.\n\n\n\n\n\n\n\nThe Open Philanthropy Project recommended a grant of $277,435 over three years to Robin Hanson (Associate Professor of Economics, George Mason University) to analyze potential scenarios in the future development of artificial intelligence (AI). Professor Hanson plans to focus on scenarios in which AI is developed through the steady accumulation of individual pieces of software and leads to a “multipolar” outcome (i.e. a scenario in which the control of advanced AI is distributed among multiple actors, rather than controlled by a single group, firm, or state). Part of this grant will pay to hire a research assistant. Ideally, this research will culminate in a book by Professor Hanson on the topic.\nUpdate: In July of 2017, we added $12,910 to the original grant amount to cover an increase in George Mason University’s instructional release costs (“teaching buyouts”). The “grant amount” above has been updated to reflect this.\n1. Background\nThis grant falls within our work on potential risks from advanced artificial intelligence, one of our focus areas within global catastrophic risks.\n2. About the grant\nProfessor Hanson’s grant proposal describes the project as follows:1\nRobin Hanson proposes to take three years to conduct a broad positive analysis of the multipolar scenario wherein AI results from relatively steady accumulation of software tools. That is, he proposes to assume that human level AI will result mainly from the continued accumulation of software tools and packages, with distributions of cost and value correlations similar to those seen so far in software practice, in an environment where no one actor dominates the process of creating or fielding such software. He will attempt a mostly positive analysis of the social consequences of these assumptions, both during and after a transition to a world dominated by AI. While this is hardly the universe of all desired analyses, it does seem to cover a non-trivial fraction of interesting cases.\n2.1 Case for the grant\nWhile we do not believe that the class of scenarios that Professor Hanson will be analyzing is necessarily the most likely way for future AI development to play out, we expect his research to contribute a significant amount of useful data collection and analysis that might be valuable to our thinking about AI more generally, as well as provide a model for other people to follow when performing similar analyses of other AI scenarios of interest.\nProfessor Hanson appears to us to be particularly well suited for this project, for several reasons:\n\nHis recently published book on the potential future of whole brain emulations, The Age of Em,2 seems to us to be a thoughtful analysis of what might happen if brain emulations were developed (though we do not agree with all of the book’s claims and predictions). We believe Professor Hanson’s analysis of future AI scenarios could prove similarly thoughtful.\nHe had developed an outline and plan for this analysis before we expressed interest in supporting it, making this an unusually “shovel-ready” grant.\nHe appears to us to be knowledgeable about economics, AI, and futurism generally, and to be a particularly original thinker.\nHe is particularly interested in analyzing scenarios where advances in AI have a transformative impact on the world.\n\nIn general, we would like to see a larger amount of thoughtful analysis of how AI-related scenarios might play out.\n2.2 Room for more funding\nWe do not believe that Professor Hanson would undertake this work in the near future without this funding. He had planned to turn his attention to other research if he did not receive funding for this specific project, and we are fairly confident that no other funder was planning to support the project.\n2.3 Risks and reservations\nOur main concern is that, after further consideration, we might later conclude that the scenario analyzed was foreseeably very unlikely (e.g. because advanced AI systems turn out to be very different from other kinds of software). However, we see value in having many potential scenarios analyzed, and see this is as a risk worth taking.\n3. Plans for learning and follow-up\nKey questions for follow-up:\n\nHas Professor Hanson found a research assistant, and if so, how has working with him or her gone?\nWhat progress has Professor Hanson made on the book?\nDoes the progress that has been made appear useful and/or insightful to us?\n\n4. Sources\n\n\n\nDOCUMENT\nSOURCE\n\n\nGeorge Mason University Proposal\nSource\n\n\nThe Age of Em Homepage\nSource (archive)</td>
      <td><a href="https://www.openphilanthropy.org/grants/george-mason-university-research-into-future-artificial-intelligence-scenarios/">https://www.openphilanthropy.org/grants/george-mason-university-research-into-future-artificial-intelligence-scenarios/</a></td>
    </tr>
    <tr>
      <td>Future of Life Institute — Artificial Intelligence Risk Reduction</td>
      <td>Future of Life Institute</td>
      <td>Potential Risks from Advanced AI</td>
      <td>$1,186,000</td>
      <td>August 2015</td>
      <td>(Photo Courtesy of Twitter)\n\n\n\n\n\n\n\n\n\n\n\nFuture of Life Institute staff reviewed this page prior to publication.\nNote: This page was created using content published by Good Ventures and GiveWell, the organizations that created the Open Philanthropy Project, before this website was launched. Uses of “we” and “our” on this page may therefore refer to Good Ventures or GiveWell, but they still represent the work of the Open Philanthropy Project.\n\n\n\n\n\n\n\nThe Open Philanthropy Project recommended $1,186,000 to the Future of Life Institute (FLI) to support research proposals aimed at keeping artificial intelligence robust and beneficial. In the first half of 2015, FLI issued a Request for Proposals (RFP) to gather research proposals on artificial intelligence risk reduction. This RFP was the first wave of a $10 million program funded by Elon Musk; the RFP planned to make grants worth approximately $6 million.\nThe RFP solicited applications for research projects (by small teams or individuals) and centers (to be founded focusing on policy and forecasting). After working closely with FLI during the receipt and evaluation of proposals, we determined that the value of high-quality project proposals submitted was greater than the available funding. Consequently, we made a grant of $1,186,000 to FLI to enable additional project proposals to be funded. We consider the value of this grant to comprise both the output of the additional projects funded and the less tangible benefits of supporting the first formal RFP in this field.\nWe were very pleased with the overall quality of the applications, and with the decisions made by the selection panel. The proposals that were funded by the RFP span a wide range of approaches, including research on ensuring that advanced AI systems that may be developed in the future are aligned with human values, managing the economic impacts of AI, and controlling autonomous weapons systems.\n\nRationale for the grant\nThe cause\nIn a March 2015 update on the Open Philanthropy Project, we identified ‘risks from artificial intelligence’ as a priority cause within our global catastrophic risk program area.\nIn brief, “risks from artificial intelligence (AI)” refers to risks that could potentially emerge as the capabilities of AI systems increase. It seems plausible that sometime this century, systems will be developed that can match human-level performance at a wide range of cognitive tasks. These advances could have extremely positive effects, but may also pose risks from intentional misuse or catastrophic accidents.\nSee our writeup of this issue for more detail on our view.\nThe Future of Life Institute’s 2015 Request for Proposals\nIn January 2015, the Future of Life Institute (FLI) organized a conference in Puerto Rico, called ‘The Future of AI: Opportunities and Challenges’. Following the conference, Elon Musk announced a $10 million donation to FLI to support “a global research program aimed at keeping AI beneficial to humanity.”1 Soon thereafter, FLI issued a Request for Proposals (RFP) to solicit proposals aiming to make AI systems robust and beneficial,2 and published alongside it a document expanding on research priorities within this area.3 The goal of the RFP was to allocate $6 million of Musk’s donation to the most promising proposals submitted, in two categories: “project grants” and “center grants”.4\nWe see this RFP as an important step in the development of the nascent field of AI safety research. It represents the first set of grant opportunities explicitly seeking to fund mainstream academic work on the subject, which we feel makes it an unusual opportunity for a funder to engage in early-stage field-building. We felt that it was important that the process go well, in the sense that strong proposals be funded, and that the academics who took part feel that applying was a good use of their time.\nFor this reason, we have been working closely with FLI since the announcement of the RFP. We wanted to follow what proposals were submitted, with the intention of potentially contributing additional funding if we believed that high quality proposals would otherwise go unfunded.\nOur decision process\nOur Program Officer for Global Catastrophic Risks, Howie Lempel, reviewed first round applications with the assistance of Dario Amodei, one of our technical advisors; other Open Philanthropy staff looked over some applications and were involved in discussions. Following this review, we determined that at the available level of funding ($6 million), a number of promising proposals would have to be rejected.\nAt this point (around May), we told FLI that we would plan to recommend a grant of at least $1 million towards the RFP. Telling FLI about our planned recommendation at this stage was intended to assist FLI in planning the review of second round proposals, and as an expression of good faith while we played an active role in the RFP process.\nWe discussed the RFP with a number of people, including applicants, AI researchers from within academia, and researchers from within the existing AI safety field. These conversations included discussion of the value of different types of research, as well as asking for their perceptions of how the RFP was proceeding. We shared our impressions from these discussions with FLI staff, including making some small logistical suggestions to help the RFP run smoothly. Conversation notes from these conversations are not available, though there are some details on what we heard and what suggestions we made below.\nRepresentatives of the Open Philanthropy Project attended the review panel meeting in late June, where the final funding allocations were decided. There we focused on evaluating the proposals which would be affected by our decision on how much funding to allocate. We decided that contributing a total of $1,186,000 would enable the RFP to fund all the proposals that the panel had determined to be the strongest.\nThe proposals\nThe full list of proposals receiving funding, including summaries and technical abstracts, may be found here. That link includes all materials that can be shared publicly (we cannot share e.g. rejected proposals, though we give general comments on the process below).\nFLI’s announcement of the grants gives the following overview of the awardees:5\nThe winning teams, chosen from nearly 300 applicants worldwide, will research a host of questions in computer science, law, policy, economics, and other fields relevant to coming advances in AI.\nThe 37 projects being funded include:\n\nThree projects developing techniques for AI systems to learn what humans prefer from observing our behavior, including projects at UC Berkeley and Oxford University\nA project by Benja Fallenstein at the Machine Intelligence Research Institute on how to keep the interests of superintelligent systems aligned with human values\nA project lead by Manuela Veloso from Carnegie-Mellon University on making AI systems explain their decisions to humans\nA study by Michael Webb of Stanford University on how to keep the economic impacts of AI beneficial\nA project headed by Heather Roff studying how to keep AI-driven weapons under “meaningful human control”\nA new Oxford-Cambridge research center for studying AI-relevant policy\n\n\nWith the research field of AI safety at such an early stage, we feel that it would be premature to have confident expectations of which research directions will end up being relevant and important once AI systems become advanced enough to pose significant risks. As such, we were hoping that this RFP would provide an opportunity to support experts from a variety of backgrounds to tackle different aspects of the problem, and expand the range of approaches being taken. We were excited to see breadth across the research proposals submitted, which were diverse both relative to each other and to previous work on AI safety.\nThere are some specific research directions which we find particularly promising, though with low confidence. Some of these directions – which we were glad to see represented among the successful projects – include transparency and meta-reasoning, robustness to context changes, calibration of uncertainty, and general forecasting.6 We also believe that early attempts to explore what it looks like when AI systems attempt to learn what humans value may be useful in informing later work, if more advanced and general AI (which may need to have some internal representation of human values, in a way current systems do not) is eventually developed. A number of different approaches to this value-learning question were among the successful proposals, including projects led by Stuart Russell, Owain Evans, and Paul Christiano.\nOverall, we were very impressed by the quality of the applicants and the proposals submitted. We were also pleased with the decisions made by the review panel; we feel that recommendations were consistently based on the quality of the proposals, without unduly prioritizing either projects from within mainstream academia, or from within the existing AI safety field.\nRoom for more funding\nWe believe it is very unlikely that the proposals funded due to this grant would have received funding from this RFP in the absence of the grant.\nElon Musk’s donation was the only other source of funding for this RFP, and this contribution was capped at $6 million from the outset.7 We considered the possibility that this amount would be increased late in the process if the quality of submissions was higher than expected. We decided to make the grant once we had decided that despite the large number of excellent proposals, it was unlikely that the total amount of funding for this set of grants would be raised.\nIn addition, the funding provided by Musk was restricted such that it had to be disbursed evenly across three years, funding $2 million of proposals each year. Given that the proposals submitted could last one, two, or three years and generally requested an even amount of funding across their duration, this restriction meant that it would be essentially impossible for the RFP to allocate its entire budget if it funded any one- or two-year projects. Our funding meant that shorter projects could be funded without running into this issue. We also made an effort to determine whether some projects which had applied for one or two years of funding could be restructured to receive funds over a longer time horizon. A large number of the projects which we suggested be restructured in this way accepted the suggestion, and were able to receive funds which they might not have had access to without our involvement.\nWe find it relatively likely that some of the projects funded by this RFP would have received funding from other sources, if they had not been successful here. However, we do not consider this a major concern. One reason is that while some projects would have received funding, it’s far from clear that the majority would have. Secondly, it was important to us not only that the specific additional projects received funding, but also that the RFP be successful as a whole. This included ensuring that talented applicants not have their proposals rejected in a way that could cause them to feel demoralized or unmotivated to do research related to AI safety in future, and funding the set of proposals which would best ensure the productive development of the field as a whole.\nThe organization\nOverall, we have been impressed with what FLI has been able to achieve over the past six months, and are very happy with the outcome of the RFP.\nPoints we were particularly pleased with:\n\nThe conference in Puerto Rico was organized quickly, brought together an impressive group of participants, and led to a decisive result.8\nThe RFP received a large number of what we (and Dario, our advisor) perceived as high quality proposals, which we consider especially impressive considering the relatively short timeline and the unusual topic.\nWe felt that the review panel was strong, and were happy with the decisions it made.\n\nThere were some areas in which we found working with FLI challenging. We feel these areas were minor compared to FLI’s strengths listed above, but wish to explain them to give a sense of the considerations that went into this grant. We discuss them in the next section.\nRisks and reservations about this grant\nThe most significant risk to the success of this grant is that the research it funds may simply turn out not to contribute usefully to the field of AI safety. We see this risk as intrinsic to the field as a whole at present, and not as specific to this grant; see this section of our writeup on risks from artificial intelligence for more detail on our thoughts on this front.\nThere are two other notable reservations we had while making this grant:\n\nWe see some risk that any work in this field will increase the likelihood of counterproductive regulation being introduced in response to increased attention to the field. On balance, we expect that this grant and similar work will reduce the risk of this type of regulation, rather than increasing it; we have written more on our views on this question in this section of our cause writeup.\nWe find it fairly likely that some of the successful projects would have received funding from other sources if they had not been funded by this RFP. While this reduces this grant’s expected counterfactual impact on research output, we do not consider it a major concern; we also believe the indirect benefits on community and field-building are important.\n\nAs a more minor consideration, we share some of the challenges of working with FLI as an indication of reservations we had during the process of considering the grant, though we don’t consider these to be major issues for the grant going forward.\n\nThere is a built-in tension in our relationships with potential grantees; they are both organizations fundraising from us as well as partners in achieving our goals. The latter role incentivized FLI to share information with us about possible weaknesses in the competing projects, so that we could provide maximally helpful input, whereas the former role incentivized FLI to present the proposals and review process to us in a positive light. Although FLI did share all proposal details and written panel reports with us, there were cases in which we felt that certain issues weren’t sufficiently called to our attention, and/or that the tension between fundraising and achieving goals hampered clear communication.\nWe first observed this dynamic while evaluating the first round of RFP proposals. Prior to selecting proposals for the second round, FLI asked us to provide a rough initial estimate of our likely level of funding so they could decide how many second rounders to solicit. To aid our determination, we were given an estimate of the number of high quality first round proposals based on FLI’s polling of the reviewers (as well as access to the proposals themselves). We felt the proposals were strong overall but that there were significantly fewer very strong proposals than this information had implied.\n\nOver the course of the RFP we encountered additional communication issues, although these were smaller and/or more ambiguous. Communication may also have been smoother if the RFP involved fewer short timelines and more advance planning, issues discussed immediately below.\n\nWe believe that parts of the process might have gone more smoothly, particularly in terms of communications with applicants, by a combination of more thorough advance planning, a less compressed schedule, and more input on the process from external experts who have experience with RFPs within the AI field. For example:\n\nSome applicants commented that it was not clear what types of research were eligible for funding (although this is somewhat to be expected given the novelty of the overall topic).\nThe grants included a specific requirement that overhead costs not exceed 15%. However, some applicants were unsure when or whether they should negotiate this arrangement with their institution.\nBased on input from first-round AI expert reviewers, finalists were specifically advised that proposals for less than $100,000 were more likely to be funded,9 putting downward pressure on budgets. This could have been better explained and communicated, as our impression is that the length of the requested proposals, the modest amount of funding available, and this budget pressure negatively affected the cost-benefit tradeoff of applying. We spoke with several applicants who were considering withdrawing from the process and raised this concern, although the specific applicants we spoke with did not withdraw. This may have contributed to some (in our opinion) strong proposals from leading institutions withdrawing between rounds, and may have been part of the reason that some cutting-edge topics in current AI research (e.g. deep learning) were somewhat under-represented among the projects funded. We did not have the opportunity to discuss these applicants’ reasons for withdrawal.\n\n\nWe consider FLI’s model as an all-volunteer organization to be somewhat risky in general, although it has clear advantages in terms of maintaining relatively low expenses. An example of a specific concern is that FLI did not have a PR professional involved in the grant process, even though it generated a reasonable amount of press coverage. However, we should note that we have been pleased overall with how the PR around the grants has gone.\n\nLessons learned\nThere are two major lessons that we took away from the process of making this grant:\n\nWe communicated poorly with FLI about the public announcement of our grant, which caused a relatively significant misunderstanding when it came time to make the announcement. We did not understand FLI’s planned timeline for issuing a press release, and failed to communicate that we had expected more time to review the release before publishing. In the future, we will put more emphasis on establishing both parties’ expectations for public announcements more clearly and further in advance. We view this as our mistake.\nIn the future, when running a grants competition in a given academic field, we will place more emphasis on having people from within that field provide feedback on the application process and on communications to applicants before they are sent out. We believe this will be an effective way to stay on top of how communications with applicants will be perceived and interpreted, and how the program compares with expectations from various communities across the field.\n\nPlans for follow-up\nFollow-up expectations\nGrantees are required to update FLI on their progress once per year for the duration of their funding. We plan to discuss these updates with FLI, and intend to write publicly about our impressions of how the projects are progressing. Given that the full benefits of the research funded by this RFP relate to long-term field development, we do not expect to be able to say with confidence to what extent the projects are succeeding. However, we do plan to keep track of the projects’ research output, including, for example, publications and conference presentations, where appropriate.\nSources\n\n\n\nDOCUMENT\nSOURCE\n\n\nFuture of Life Institute announcement of grantees\nSource (archive)\n\n\nFuture of Life Institute grants competition 2015\nSource (archive)\n\n\nFuture of Life Institute open letter\nSource (archive)\n\n\nFuture of Life Institute press release, Jan 15 2015\nSource (archive)\n\n\nFuture of Life Institute research priorities 2015\nSource (archive)</td>
      <td><a href="https://www.openphilanthropy.org/grants/future-of-life-institute-artificial-intelligence-risk-reduction/">https://www.openphilanthropy.org/grants/future-of-life-institute-artificial-intelligence-risk-reduction/</a></td>
    </tr>
  </tbody>
</table>
    
    <h1>Innovation Policy</h1>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Grant Title</th>
      <th>Organization Name</th>
      <th>Focus Area</th>
      <th>Amount</th>
      <th>Date</th>
      <th>Abstract</th>
      <th>Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>University of Pennsylvania — High-Skilled Immigration Budget Modeling</td>
      <td>University of Pennsylvania</td>
      <td>Innovation Policy</td>
      <td>$1,100,000</td>
      <td>April 2024</td>
      <td>Image courtesy of the University of Pennsylvania\nOpen Philanthropy recommended a gift of $1,100,000 over two years to the University of Pennsylvania to support high-skilled immigration budget modeling at the Penn Wharton Budget Model, a research initiative at the university.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-pennsylvania-high-skilled-immigration-budget-modeling/">https://www.openphilanthropy.org/grants/university-of-pennsylvania-high-skilled-immigration-budget-modeling/</a></td>
    </tr>
    <tr>
      <td>Dartmouth — Workshop Travel Expenses (March 2024)</td>
      <td>Dartmouth</td>
      <td>Innovation Policy</td>
      <td>$20,000</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a gift of $20,000 to Heidi Williams at Dartmouth to support travel expenses for participants in a workshop on modeling the impact of public policies on pharmaceutical markets.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/dartmouth-workshop-travel-expenses-march-2024/">https://www.openphilanthropy.org/grants/dartmouth-workshop-travel-expenses-march-2024/</a></td>
    </tr>
    <tr>
      <td>UK Day One – Science and Technology Policy Crowdsourcing</td>
      <td>UK Day One</td>
      <td>Innovation Policy</td>
      <td>$150,000</td>
      <td>March 2024</td>
      <td>Open Philanthropy recommended a grant of $150,000 to UK Day One to support the development of crowdsourced science and technology policy ideas. UK Day One is a nonpartisan initiative fiscally sponsored by the Digital Harbor Foundation. It is dedicated to advancing the UK’s policy landscape on science, technology, and innovation.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/uk-day-one-science-and-technology-policy-crowdsourcing/">https://www.openphilanthropy.org/grants/uk-day-one-science-and-technology-policy-crowdsourcing/</a></td>
    </tr>
    <tr>
      <td>Monash University — Academia and Policymaking Living Literature Review</td>
      <td>Monash University</td>
      <td>Innovation Policy</td>
      <td>$69,324</td>
      <td>February 2024</td>
      <td>Open Philanthropy recommended a grant of AUD $106,189 (approximately $69,324 at the time of conversion) to Monash University to support Paul Kellner’s work on a living literature review* about the connection between academia and policymaking.\nThis falls within our focus area of innovation policy.\n*A term we use to refer to a collection of articles, written by one individual, that synthesizes academic research and is updated as the literature evolves.</td>
      <td><a href="https://www.openphilanthropy.org/grants/monash-university-academia-and-policymaking-living-literature-review/">https://www.openphilanthropy.org/grants/monash-university-academia-and-policymaking-living-literature-review/</a></td>
    </tr>
    <tr>
      <td>Leibniz Institute for Economic Research — Development Economics Paper Replication</td>
      <td>Leibniz Institute for Economic Research</td>
      <td>Innovation Policy</td>
      <td>$80,246</td>
      <td>February 2024</td>
      <td>Open Philanthropy recommended a grant of EUR 75,000 (approximately $80,246 at the time of conversion) to the Leibniz Institute for Economic Research to support the replication of 30 papers from top development economics journals.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/leibniz-institute-for-economic-research-development-economics-paper-replication/">https://www.openphilanthropy.org/grants/leibniz-institute-for-economic-research-development-economics-paper-replication/</a></td>
    </tr>
    <tr>
      <td>Dartmouth — Workshop Travel Expenses (January 2024)</td>
      <td>Dartmouth</td>
      <td>Innovation Policy</td>
      <td>$11,000</td>
      <td>January 2024</td>
      <td>Open Philanthropy recommended a gift of $11,000 to Heidi Williams at Dartmouth to support travel expenses for participants in a workshop on estimating the economic and budgetary impacts of immigration.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/dartmouth-workshop-travel-expenses/">https://www.openphilanthropy.org/grants/dartmouth-workshop-travel-expenses/</a></td>
    </tr>
    <tr>
      <td>Speculative Technologies — Research Program Training</td>
      <td>Speculative Technologies</td>
      <td>Innovation Policy</td>
      <td>$250,000</td>
      <td>November 2023</td>
      <td>Image courtesy of Speculative Technologies\nOpen Philanthropy recommended a grant of $250,000 to Speculative Technologies to support its Brains program, which trains individuals to lead ambitious coordinated research programs.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/speculative-technologies-research-program-training/">https://www.openphilanthropy.org/grants/speculative-technologies-research-program-training/</a></td>
    </tr>
    <tr>
      <td>Fund for Global Talent Mobility — High-Skilled Immigration Grants</td>
      <td>Fund for Global Talent Mobility</td>
      <td>Innovation Policy</td>
      <td>$900,000</td>
      <td>October 2023</td>
      <td>Open Philanthropy recommended a grant of $900,000 to the Fund for Global Talent Mobility to support a regranting initiative aimed at increasing the use of existing high-skilled immigration pathways to the US. Grant applications will be processed on an expedited timeline by immigration lawyer Amy Nice, who serves as project director for the Fund.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/fund-for-global-talent-mobility-high-skilled-immigration-grants/">https://www.openphilanthropy.org/grants/fund-for-global-talent-mobility-high-skilled-immigration-grants/</a></td>
    </tr>
    <tr>
      <td>National Bureau of Economic Research — Innovation Policy Working Group (2023)</td>
      <td>National Bureau of Economic Research</td>
      <td>Innovation Policy</td>
      <td>$214,775</td>
      <td>June 2023</td>
      <td>Open Philanthropy recommended a grant of $214,775 to the National Bureau of Economic Research to support its Innovation Policy Working Group, led by Professors Benjamin Jones and Heidi Williams. This funding will support a summer boot camp for PhD students on the economics of innovation, as well as an innovation policy workshop.\nThis follows our August 2021 support and falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/national-bureau-of-economic-research-innovation-policy-working-group-2023/">https://www.openphilanthropy.org/grants/national-bureau-of-economic-research-innovation-policy-working-group-2023/</a></td>
    </tr>
    <tr>
      <td>University of Ottawa — Institute for Replication</td>
      <td>University of Ottawa</td>
      <td>Innovation Policy</td>
      <td>$464,698</td>
      <td>May 2023</td>
      <td>Photo courtesy of Institute for Replication\nOpen Philanthropy recommended a grant of $464,698 to the University of Ottawa to support the Institute for Replication, which works to improve the credibility of scientific research by reproducing and replicating results in leading academic journals.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/university-of-ottawa-institute-for-replication/">https://www.openphilanthropy.org/grants/university-of-ottawa-institute-for-replication/</a></td>
    </tr>
    <tr>
      <td>Florian Jehn — Societal Collapse Living Literature Review</td>
      <td>NaN</td>
      <td>Innovation Policy</td>
      <td>$35,000</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $35,000 to Florian Jehn to support work on a living literature review* on societal collapse.\nThis falls within our focus area of innovation policy.\n*A term we use to refer to a collection of articles, written by one individual, that synthesizes academic research and is updated as the literature evolves.</td>
      <td><a href="https://www.openphilanthropy.org/grants/florian-jehn-societal-collapse-living-literature-review/">https://www.openphilanthropy.org/grants/florian-jehn-societal-collapse-living-literature-review/</a></td>
    </tr>
    <tr>
      <td>Economic Innovation Group — High-Skilled Immigration Research and Advocacy</td>
      <td>Economic Innovation Group</td>
      <td>Innovation Policy</td>
      <td>$1,223,935</td>
      <td>April 2023</td>
      <td>Open Philanthropy recommended a grant of $1,223,935 over two years to the Economic Innovation Group to support its research and advocacy for reforms that would increase high-skilled immigration to the United States.\nThis falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/economic-innovation-group-high-skilled-immigration-research-and-advocacy/">https://www.openphilanthropy.org/grants/economic-innovation-group-high-skilled-immigration-research-and-advocacy/</a></td>
    </tr>
    <tr>
      <td>Institute for Progress — General Support (2023)</td>
      <td>Institute for Progress</td>
      <td>Innovation Policy</td>
      <td>$9,341,660</td>
      <td>March 2023</td>
      <td>Open Philanthropy recommended three grants totaling $9,341,660 over three years to the Institute for Progress for general support. The Institute for Progress is a think tank that conducts policy research on topics related to scientific progress, including high-skill immigration, metascience, biotechnology, and other emerging technologies.\nThis follows our October 2021 support and falls within our focus area of innovation policy.</td>
      <td><a href="https://www.openphilanthropy.org/grants/institute-for-progress-general-support-2023/">https://www.openphilanthropy.org/grants/institute-for-progress-general-support-2023/</a></td>
    </tr>
    <tr>
      <td>Federation of American Scientists — High-Skilled Immigration Reform</td>
      <td>Federation of American Scientists</td>
      <td>Innovation Policy</td>
      <td>$1,011,000</td>
      <td>February 2023</td>
      <td>Open Philanthropy recommended a grant of $1,011,000 over two years to the Federation of American Scientists to support research and advocacy related to immigration reform for high-skilled workers.\nThis follows our July 2021 support and falls within our focus area of innovation policy.\nThe grant amount was updated in March 2024.</td>
      <td><a href="https://www.openphilanthropy.org/grants/federation-of-american-scientists-high-skilled-immigration-reform/">https://www.openphilanthropy.org/grants/federation-of-american-scientists-high-skilled-immigration-reform/</a></td>
    </tr>
    <tr>
      <td>Abdul Latif Jameel Poverty Action Lab — Innovation and Science Research</td>
      <td>Abdul Latif Jameel Poverty Action Lab</td>
      <td>Innovation Policy</td>
      <td>$649,176</td>
      <td>August 2022</td>
      <td>Open Philanthropy recommended a grant of $649,176 over two years to the Abdul Latif Jameel Poverty Action Lab (J-PAL) at the Massachusetts Institute of Technology to support their new Science for Progress initiative, which will conduct research and randomized evaluations related to innovation and science. This initiative will be co-led by Professor Paul Niehaus and Professor Heidi Williams.\nThis falls within our work on global health and wellbeing.</td>
      <td><a href="https://www.openphilanthropy.org/grants/abdul-latif-jameel-poverty-action-lab-innovation-and-science-research/">https://www.openphilanthropy.org/grants/abdul-latif-jameel-poverty-action-lab-innovation-and-science-research/</a></td>
    </tr>
    <tr>
      <td>Stanford University — Research on Science and Innovation (Heidi Williams)</td>
      <td>Stanford University</td>
      <td>Innovation Policy</td>
      <td>$599,190</td>
      <td>May 2022</td>
      <td>Open Philanthropy recommended a grant of $599,190 to Stanford University to support a one-year period of research leave for Professor Heidi Williams. During this time, Professor Williams will pursue research collaborations with both public and private science funders in order to investigate potential improvements to scientific funding and research, with the goal of improving both how society supports science and how science benefits society.\nThis falls within our work on scientific research.</td>
      <td><a href="https://www.openphilanthropy.org/grants/stanford-university-research-on-science-and-innovation-heidi-williams/">https://www.openphilanthropy.org/grants/stanford-university-research-on-science-and-innovation-heidi-williams/</a></td>
    </tr>
    <tr>
      <td>Institute for Progress — Policy Research</td>
      <td>Institute for Progress</td>
      <td>Innovation Policy</td>
      <td>$2,215,000</td>
      <td>October 2021</td>
      <td>Open Philanthropy recommended a grant of $2,215,000 over one and a half years to the Institute for Progress, a new think tank, to conduct research and advocacy on policies relevant to a number of our grantmaking areas, including immigration policy and scientific research.\nThe grant amount was updated on September 19, 2022.</td>
      <td><a href="https://www.openphilanthropy.org/grants/institute-for-progress-policy-research/">https://www.openphilanthropy.org/grants/institute-for-progress-policy-research/</a></td>
    </tr>
    <tr>
      <td>National Bureau of Economic Research — Innovation Policy Working Group</td>
      <td>National Bureau of Economic Research</td>
      <td>Innovation Policy</td>
      <td>$467,500</td>
      <td>August 2021</td>
      <td>Open Philanthropy recommended a grant of $467,500 over two years to the National Bureau of Economic Research (NBER) to support its Innovation Policy Working Group, led by Professors Benjamin Jones and Heidi Williams. This funding will support a summer boot camp for PhD students on the economics of innovation, as well as an innovation policy workshop. We believe that these activities could help build the field of innovation economics, which in turn could improve science and innovation policy in the U.S. and other countries.</td>
      <td><a href="https://www.openphilanthropy.org/grants/national-bureau-of-economic-research-innovation-policy-working-group/">https://www.openphilanthropy.org/grants/national-bureau-of-economic-research-innovation-policy-working-group/</a></td>
    </tr>
  </tbody>
</table>
</body>
</html>
