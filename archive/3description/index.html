<!DOCTYPE html>
<html lang="en">

<head>
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/addons/p5.sound.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4"
    crossorigin="anonymous"></script>
  <script async src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>
  </script>
  <link rel="stylesheet" type="text/css" href="../../style.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">
  <link rel="shortcut icon" href="#" />
  <title>Zhuodi Cai</title>
  <link rel="icon" type="image/png" href="../../images/favicon.png">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  <meta name="description" content="This is Zhuodi (Zoe) Cai. Welcome to my cyber cabin!">
  <meta name="author" content="Zhuodi Cai">
  <meta property="og:title" content="Zhuodi Cai">
  <meta property="og:description" content="This is Zhuodi (Zoe) Cai. Welcome to my cyber cabin!">
  <meta property="og:image" content="https://github.com/zhuodicai/zhuodicai.github.io/blob/6a90ad4fbce563889faac3f2b38c9f292c508b63/images/favicon.png">
  <meta property="og:url" content="https://zhuodicai.github.io/index.html">
  <meta name="google-site-verification" content="-DJgxNC8digfD6D_1l7Via3aPbSGBaQsJ_u1-Y0uHKM" />

  <style>
    .speechcontent table,
    .speechcontent th,
    .speechcontent td {
      border: 1px solid black;
      border-collapse: collapse;
    }

    .google-slides-container {
      position: relative;
      width: 100%;
      padding-top: 60%;
      overflow: hidden;
    }

    .google-slides-container iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
  </style>
</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-light">
    <div class="container-fluid">
      <a class="navbar-brand" href="../../index.html">ZHUODI CAI</a>
      <div class="custom-navbar">
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
      <div class="collapse navbar-collapse text-center" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="../../index.html">Home</a>
          </li>
          <li class="nav-item active">
            <a class="nav-link" href="../index.html" style="text-decoration: underline;">Archive<span
                class="sr-only"></span></a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../../journal/index.html">Journal</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../../gallery/index.html">3D Gallery</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <div class="container text-align-top text-start" style="margin-top: 10vh;">

    <h3 class="text-center">3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach</h3>
    <br>
    
    <div class="text-center">
      <a href="https://dl.acm.org/doi/10.1145/3632776.3632785"><button type="button"
          class="mybutton">Paper</button></a>
    </div>

    <br>
    <table class="table table-borderless text-start">
      <thead>
        <tr style="vertical-align: top;">
          <th style="padding: 0;" scope="col">Category&nbsp&nbsp&nbsp&nbsp</th>
          <th style="padding: 0;" scope="col">Duration&nbsp&nbsp&nbsp&nbsp</th>
          <th style="padding: 0;" scope="col">Tool / Material&nbsp&nbsp&nbsp&nbsp</th>
        </tr>
      </thead>
      <tbody>
        <tr style="vertical-align: top;">
          <td style="padding: 0;">ML/AI Application; 3D; Real-Time Communications; Design; Development</td>
          <td style="padding: 0;">January 2023 - May 2023 @ New York University</td>
          <td style="padding: 0;">JavaScript, Arduino(C++), Python, Figma</td>
        </tr>
      </tbody>
    </table>
    <br>
    <p>
      This project is my graduate thesis in the Interactive Telecommunications Program (ITP) at New
      York University Tisch School of the Arts. A part of the whole experimental project has been composed into paper.
      
      Special thanks to Sharon De La Cruz as thesis advisor, the ITP
      community, and Yuan Li for technical discussions.
    </p>
    <br><br>

    <!-- <p><b>Paper</b></p>
    <div class="google-slides-container" style="box-shadow: 0 0 10px 5px #22252910; margin-bottom:30px;">
      <iframe src="https://drive.google.com/file/d/1zPcbac6Krd8L40MwaGfyTUZBas0rr4tB/preview" width="640" height="480" allow="autoplay"></iframe>
    </div> -->

    <p><b>Thesis Presentation Slides</b></p>
    <div class="google-slides-container" style="box-shadow: 0 0 10px 5px #22252910; margin-bottom:30px;">
      <iframe
        src="https://docs.google.com/presentation/d/e/2PACX-1vRKD9crWZfnUoijXUcOMYe6KHhfbOtmNpuls4lX8UndOJF-w-10i0oLcGFmkZwl4_scf33pTNZaavUt/embed?start=false&amp;loop=true&amp;delayms=3000"
        frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true"
        webkitallowfullscreen="true" class=""></iframe>
    </div>
    <br>
    <p><b>Thesis Presentation Speech Content</b></p>
    <table class="speechcontent" style="width:100%; box-shadow: 0 0 10px 5px #22252910; margin-bottom:30px;">
      <tbody>
        <tr style="color:#f7f7f3; background-color: black;vertical-align: top;">
          <th style="width:20%">Page</th>
          <th>Speech Content</th>
        </tr>
        <tr>
          <td>1</td>
          <td>Hi everyone! My name is Zoe and my thesis is called 3Description.</td>
        </tr>
        <tr>
          <td>2</td>
          <td><b>Background</b></td>
        </tr>
        <tr>
          <td>3</td>
          <td>Nowadays the 3D is playing a more and more important role in our daily life. And I see a lot of you are
            interested in VR! Cool! So shall we create 3D assets that would affect our tomorrow?</td>
        </tr>
        <tr>
          <td>4</td>
          <td>Unfortunately, not everyone is ready to jump on board! I am not. Because of wrist tendonitis, it's really
            painful for me to build 3D models in a traditional way, like using a mouse. I'm not alone. There are 800
            million people are suffering from hand and wrist pain. So, what about us? My friends are not ready. Without
            art or design background, it's even hard for them to draw a decent 2D sketch, let alone the 3D modeling. And
            for the mainstream 3D modeling software, there are only around 10 million times download. So, what about the
            others? Should everyone learn 3D modeling all of sudden? Or, should we keep on buying models created by
            others? Or, should we rely entirely on the AIGC and give up our own creativity? </td>
        </tr>
        <tr>
          <td>5</td>
          <td>No, thank you!</td>
        </tr>
        <tr>
          <td>6</td>
          <td><b>Statement</b></td>
        </tr>
        <tr>
          <td>7</td>
          <td>So the statement is... From imagination to 3D, is there any accessible and painless way? </td>
        </tr>
        <tr>
          <td>8</td>
          <td><b>Research</b></td>
        </tr>
        <tr>
          <td>9</td>
          <td>To this end, I conducted qualitative research with behavior observation. The subjects are from non-art or
            design background. The task is to imagine and describe 3D appearance to me. To scale down the scope, let's
            say it's a flower.</td>
        </tr>
        <tr>
          <td>10</td>
          <td><b> Experiments</b> <br> The following experiments are based on the research.</td>
        </tr>
        <tr>
          <td>11</td>
          <td>If we feed the verbal description directly to AI models, we will get results like these. They are abstract
            and unadjustable.</td>
        </tr>
        <tr>
          <td>12</td>
          <td>During the interview, I found there were several things the subjects would love to describe. Shape,
            position, and color. For shape descriptions, there are two main ways, verbal and gestural. For verbal
            description, analogy is the most frequently used approach, it's very vague. Therefore, in the 1st round
            experiment, I failed, because I used fixed descriptions to change geometries and soon found it's not
            suitable for the analogy. Then I turned to do speech recognition with Whisper API, send the result to GPT3.5
            API, get the response from GPT3.5, write regex to extract the code part that we need, replace the relevant
            code in our JavaScript file, and re-render the WebGL canvas. Another way is using the gesture, here I use
            the hand pose recognition to change shape. Because I found the subjects would use both of their hands to
            tell the shape like this.</td>
        </tr>
        <tr>
          <td>13</td>
          <td>The position is very like the shape, there are also two ways of description. For the gesture part, I
            firstly tried the IoT accelerometer to change the openness. Then I replaced this solution, because I found
            it took some time for the subjects to understand how to use the device. So I turned to use the hand pose
            recognition as well. Now we can change distance, for example, between two vertices. Or we can change angles,
            for example, between two model parts.</td>
        </tr>
        <tr>
          <td>14</td>
          <td>The color is similar to the shape and position, so let's watch the recording directly!</td>
        </tr>
        <tr>
          <td>15</td>
          <td><b>More</b> <br> So, what could be more?</td>
        </tr>
        <tr>
          <td>16</td>
          <td>More models! Here are some point clouds I got from Point-E. But if we want to apply the same logic as we
            mentioned before, we might want to convert the point cloud to mesh. If we convert directly, we would get one
            entity and not able to change, for example, the position relationship between two model parts. </td>
        </tr>
        <tr>
          <td>17</td>
          <td>So before converting, we need to separate each part by recognizing the color channels like this.</td>
        </tr>
        <tr>
          <td>18</td>
          <td>Here's the result. The grey one is before the segmentation and the other two are after.</td>
        </tr>
        <tr>
          <td>19</td>
          <td>What's more? More collaborations! If we use this in a livestream experience, like a 3D collaboration
            meeting, we can discuss based on a concrete result with real-time visual feedback. This would help us
            improve the meeting effeciency, because now or ideas are not abstract, they are tangible.</td>
        </tr>
        <tr>
          <td>20</td>
          <td>What's more? More ways of communications! I got this behavioral data in the user test with accelerometer.
            I found when the subjects sent me the signals by saying "that's it", "here", or something else, they meant
            to confirm the current angle. This could be a possible way for us to communicate with the interface. Maybe
            in future, just with a frown, the solution would be change automatically. </td>
        </tr>
        <tr>
          <td>21</td>
          <td><b>Conclusion</b> <br>We see there are a lot things ahead that we can do. And I would really love to
            discuss more about them. But now let's draw a conclusion. So the whole point is, we describe with words and
            gestures, and "BOOM" we got this! <br>
            <i style="color:#DFDFDC">Action: Here, like a magic show, I suddenly took out a red physical 3D
              printed flower model made through human-machine collaboration, like the one shown in the previous
              recording video. What I didn’t mention is that the prompt for the petal is “my heart” and for the sepal is
              “my soul”!</i>
          </td>
        </tr>
        <tr>
          <td>22</td>
          <td>Thank you all who inspired me, directed me, and supported me! This flower is for you!<br>
            <i style="color:#DFDFDC">Action: Here I sent the flowers to the audiences!</i>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    </p>
    <br><br><br><br>
  </div>



  <footer class="text-center text-lg-start fixed-bottom">
    <div class="container-fluid">
      <div class="row">
        <div class="col-4 text-start">
          <i class="bi bi-github" style="color: black;"></i>&nbsp&nbsp <a href="https://github.com/zhuodicai"
            target="_blank"></a>
          <i class="bi bi-linkedin" style="color: black;"></i>&nbsp&nbsp <a href="https://www.linkedin.com/in/zoe-cai"
            target="_blank"></a>
          <i class="bi bi-instagram" style="color: black;"></i>&nbsp&nbsp <a href="https://www.instagram.com/20e.emmm/"
            target="_blank"></a>
          <i class="bi bi-envelope-fill" style="color: black;"></i>&nbsp&nbsp<a href="mailto: zc2525@nyu.edu"
            target="_blank"></a>
        </div>
        <div class="col-8">
          <div class="text-end" style="color: black;">
            <p>© <span id="current-year"></span> Zhuodi Cai &nbsp</p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script>document.getElementById("current-year").innerHTML = new Date().getFullYear();</script>

</body>

</html>